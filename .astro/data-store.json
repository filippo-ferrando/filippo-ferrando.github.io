[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.16.9","content-config-digest","75c3fc806c9ee8bf","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://filippo-ferrando.github.io\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"monokai\",\"themes\":{},\"wrap\":true,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[[null,{\"domain\":\"localhost:4321\"}]],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":true,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false,\"svgo\":false},\"legacy\":{\"collections\":false}}","blog",["Map",11,12,36,37,104,105,144,145,196,197,225,226,265,266,314,315,353,354,393,394,431,432,477,478,517,518,553,554,603,604,632,633,661,662,698,699,737,738,780,781,814,815,849,850,902,903,953,954,990,991,1041,1042,1083,1084,1128,1129,1195,1196,1229,1230,1263,1264,1300,1301,1340,1341,1370,1371,1404,1405,1469,1470,1501,1502,1547,1548,1619,1620],"hello-world",{"id":11,"data":13,"body":20,"filePath":21,"assetImports":22,"digest":24,"rendered":25,"legacyId":35},{"title":14,"description":15,"pubDate":16,"tags":17,"coverImage":19},"Hello World!","First post",["Date","2023-01-01T00:00:00.000Z"],[18],"first post","__ASTRO_IMAGE_./post_img.webp","**Hi everyone!** this is my personal portfolio in version **3.0** with also a blog attached!\nI'll be using this space to talk about what I study/am passionate about, so mainly tweaks, distros, and other things related to the linux world, but also networking, maybe programming, and anything else I can think of!\u003Cbr>\u003Cbr>\nFrom this portfolio you can contact me via **telegram** and **email**, feel free to do so! any kind of suggestion, help or even just a hello is welcome!","src/content/blog/1/index.md",[23],"./post_img.webp","2ec451632a971819",{"html":26,"metadata":27},"\u003Cp>\u003Cstrong>Hi everyone!\u003C/strong> this is my personal portfolio in version \u003Cstrong>3.0\u003C/strong> with also a blog attached!\nI’ll be using this space to talk about what I study/am passionate about, so mainly tweaks, distros, and other things related to the linux world, but also networking, maybe programming, and anything else I can think of!\u003Cbr>\u003Cbr>\nFrom this portfolio you can contact me via \u003Cstrong>telegram\u003C/strong> and \u003Cstrong>email\u003C/strong>, feel free to do so! any kind of suggestion, help or even just a hello is welcome!\u003C/p>",{"headings":28,"localImagePaths":29,"remoteImagePaths":30,"frontmatter":31,"imagePaths":34},[],[],[],{"title":14,"slug":11,"description":15,"tags":32,"pubDate":33,"coverImage":23},[18],"Jan 1 2023",[],"1/index.md","btrfs-overview",{"id":36,"data":38,"body":45,"filePath":46,"assetImports":47,"digest":49,"rendered":50,"legacyId":103},{"title":39,"description":40,"pubDate":41,"tags":42,"coverImage":44},"BTRFS","The file system for the next-generation",["Date","2023-02-28T00:00:00.000Z"],[43],"Linux","__ASTRO_IMAGE_./post_img10.jpg","# BTRFS: The Next-generation File System\n\nBTRFS (B-tree file system) is a modern file system that was designed to address the limitations of traditional file systems. It was developed by Oracle Corporation and released in 2009. Since then, it has been constantly evolving and improving with the help of the open-source community. BTRFS is a copy-on-write file system that supports features such as snapshots, checksums, and compression. It is also known for its scalability, performance, and reliability.\n\n## Features\n\n### Copy-on-write\n\nBTRFS uses a copy-on-write mechanism that ensures data consistency and integrity. When data is modified, BTRFS creates a new copy of the data and writes the changes to the new copy. The original data is left unchanged until the new copy is written successfully. This mechanism ensures that there are no data inconsistencies or corruption.\n\n### Snapshots\n\nBTRFS supports snapshots, which are read-only copies of a file system at a specific point in time. Snapshots are useful for creating backups, restoring data, and testing changes without affecting the original file system. Snapshots can be created manually or automatically, and they consume minimal disk space as they only store the differences between the original and the snapshot.\n\n### Checksums\n\nBTRFS uses checksums to ensure data integrity. When data is written to the file system, BTRFS calculates a checksum for the data and stores it alongside the data. When the data is read, BTRFS recalculates the checksum and compares it to the stored checksum. If the checksums do not match, BTRFS knows that the data has been corrupted and can take corrective action.\n\n### Compression\n\nBTRFS supports compression, which reduces the size of data on disk. Compression is useful for saving disk space, reducing network bandwidth usage, and improving performance. BTRFS supports several compression algorithms, including zlib, LZO, and ZSTD.\n\n## Scalability\n\nBTRFS is designed to be scalable, which means it can handle large amounts of data and file systems. BTRFS can support file systems up to 16 exabytes in size, which is much larger than traditional file systems. BTRFS can also handle large files, directories, and metadata with ease.\n\n## Performance\n\nBTRFS is known for its performance, especially when it comes to reading and writing large files. BTRFS uses techniques such as caching, pre-fetching, and parallelism to optimize performance. BTRFS also supports TRIM, which improves performance by notifying the file system of unused blocks.\n\n## Reliability\n\nBTRFS is designed to be reliable, which means it can detect and correct errors automatically. BTRFS uses checksums, redundant data, and error correction codes to ensure data integrity. BTRFS can also detect and correct errors on disk, such as bad sectors and data corruption.\n\n## Use Cases\n\nBTRFS is widely used in enterprise environments due to its scalability, reliability, and performance. It is a popular choice for storing large amounts of data, such as media files and databases. BTRFS is also used for virtualization, as it can handle large virtual machine images and snapshots. BTRFS is gaining popularity in the Linux community, and is being adopted by several Linux distributions, including SUSE Linux Enterprise, Fedora, and OpenSUSE.\n\n## Downsides\n\nWhile BTRFS has many advantages, it also has some downsides. BTRFS is relatively new compared to other file systems, which means it may have some stability issues. However, the BTRFS development team has been working on addressing these issues, and the file system has become increasingly stable over time. BTRFS also requires more resources than traditional file systems, which may impact performance on older hardware. Finally, not all Linux distributions support BTRFS out of the box, which may require some additional configuration.\n\n## Conclusion\n\nBTRFS is a modern file system that addresses the limitations of traditional file systems. It supports features such as snapshots, checksums, and compression, and is known for its scalability, performance, and reliability. BTRFS is widely used in enterprise environments, and is gaining popularity in the Linux community. If you're looking for a next-generation file system, BTRFS is definitely worth considering. Just make sure to check that your distribution of choice supports it and that your hardware can handle it. BTRFS is a great choice for anyone who needs a reliable, scalable, and performant file system for storing large amounts of data.\n\n#### Authors\n\n- [@filippo-ferrando](https://www.github.com/filippo-ferrando)","src/content/blog/10/index.md",[48],"./post_img10.jpg","96713145f3755c2b",{"html":51,"metadata":52},"\u003Ch1 id=\"btrfs-the-next-generation-file-system\">BTRFS: The Next-generation File System\u003C/h1>\n\u003Cp>BTRFS (B-tree file system) is a modern file system that was designed to address the limitations of traditional file systems. It was developed by Oracle Corporation and released in 2009. Since then, it has been constantly evolving and improving with the help of the open-source community. BTRFS is a copy-on-write file system that supports features such as snapshots, checksums, and compression. It is also known for its scalability, performance, and reliability.\u003C/p>\n\u003Ch2 id=\"features\">Features\u003C/h2>\n\u003Ch3 id=\"copy-on-write\">Copy-on-write\u003C/h3>\n\u003Cp>BTRFS uses a copy-on-write mechanism that ensures data consistency and integrity. When data is modified, BTRFS creates a new copy of the data and writes the changes to the new copy. The original data is left unchanged until the new copy is written successfully. This mechanism ensures that there are no data inconsistencies or corruption.\u003C/p>\n\u003Ch3 id=\"snapshots\">Snapshots\u003C/h3>\n\u003Cp>BTRFS supports snapshots, which are read-only copies of a file system at a specific point in time. Snapshots are useful for creating backups, restoring data, and testing changes without affecting the original file system. Snapshots can be created manually or automatically, and they consume minimal disk space as they only store the differences between the original and the snapshot.\u003C/p>\n\u003Ch3 id=\"checksums\">Checksums\u003C/h3>\n\u003Cp>BTRFS uses checksums to ensure data integrity. When data is written to the file system, BTRFS calculates a checksum for the data and stores it alongside the data. When the data is read, BTRFS recalculates the checksum and compares it to the stored checksum. If the checksums do not match, BTRFS knows that the data has been corrupted and can take corrective action.\u003C/p>\n\u003Ch3 id=\"compression\">Compression\u003C/h3>\n\u003Cp>BTRFS supports compression, which reduces the size of data on disk. Compression is useful for saving disk space, reducing network bandwidth usage, and improving performance. BTRFS supports several compression algorithms, including zlib, LZO, and ZSTD.\u003C/p>\n\u003Ch2 id=\"scalability\">Scalability\u003C/h2>\n\u003Cp>BTRFS is designed to be scalable, which means it can handle large amounts of data and file systems. BTRFS can support file systems up to 16 exabytes in size, which is much larger than traditional file systems. BTRFS can also handle large files, directories, and metadata with ease.\u003C/p>\n\u003Ch2 id=\"performance\">Performance\u003C/h2>\n\u003Cp>BTRFS is known for its performance, especially when it comes to reading and writing large files. BTRFS uses techniques such as caching, pre-fetching, and parallelism to optimize performance. BTRFS also supports TRIM, which improves performance by notifying the file system of unused blocks.\u003C/p>\n\u003Ch2 id=\"reliability\">Reliability\u003C/h2>\n\u003Cp>BTRFS is designed to be reliable, which means it can detect and correct errors automatically. BTRFS uses checksums, redundant data, and error correction codes to ensure data integrity. BTRFS can also detect and correct errors on disk, such as bad sectors and data corruption.\u003C/p>\n\u003Ch2 id=\"use-cases\">Use Cases\u003C/h2>\n\u003Cp>BTRFS is widely used in enterprise environments due to its scalability, reliability, and performance. It is a popular choice for storing large amounts of data, such as media files and databases. BTRFS is also used for virtualization, as it can handle large virtual machine images and snapshots. BTRFS is gaining popularity in the Linux community, and is being adopted by several Linux distributions, including SUSE Linux Enterprise, Fedora, and OpenSUSE.\u003C/p>\n\u003Ch2 id=\"downsides\">Downsides\u003C/h2>\n\u003Cp>While BTRFS has many advantages, it also has some downsides. BTRFS is relatively new compared to other file systems, which means it may have some stability issues. However, the BTRFS development team has been working on addressing these issues, and the file system has become increasingly stable over time. BTRFS also requires more resources than traditional file systems, which may impact performance on older hardware. Finally, not all Linux distributions support BTRFS out of the box, which may require some additional configuration.\u003C/p>\n\u003Ch2 id=\"conclusion\">Conclusion\u003C/h2>\n\u003Cp>BTRFS is a modern file system that addresses the limitations of traditional file systems. It supports features such as snapshots, checksums, and compression, and is known for its scalability, performance, and reliability. BTRFS is widely used in enterprise environments, and is gaining popularity in the Linux community. If you’re looking for a next-generation file system, BTRFS is definitely worth considering. Just make sure to check that your distribution of choice supports it and that your hardware can handle it. BTRFS is a great choice for anyone who needs a reliable, scalable, and performant file system for storing large amounts of data.\u003C/p>\n\u003Ch4 id=\"authors\">Authors\u003C/h4>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://www.github.com/filippo-ferrando\" target=\"_blank\" rel=\"noopener\">@filippo-ferrando\u003C/a>\u003C/li>\n\u003C/ul>",{"headings":53,"localImagePaths":97,"remoteImagePaths":98,"frontmatter":99,"imagePaths":102},[54,58,62,66,69,72,75,78,81,84,87,90,93],{"depth":55,"slug":56,"text":57},1,"btrfs-the-next-generation-file-system","BTRFS: The Next-generation File System",{"depth":59,"slug":60,"text":61},2,"features","Features",{"depth":63,"slug":64,"text":65},3,"copy-on-write","Copy-on-write",{"depth":63,"slug":67,"text":68},"snapshots","Snapshots",{"depth":63,"slug":70,"text":71},"checksums","Checksums",{"depth":63,"slug":73,"text":74},"compression","Compression",{"depth":59,"slug":76,"text":77},"scalability","Scalability",{"depth":59,"slug":79,"text":80},"performance","Performance",{"depth":59,"slug":82,"text":83},"reliability","Reliability",{"depth":59,"slug":85,"text":86},"use-cases","Use Cases",{"depth":59,"slug":88,"text":89},"downsides","Downsides",{"depth":59,"slug":91,"text":92},"conclusion","Conclusion",{"depth":94,"slug":95,"text":96},4,"authors","Authors",[],[],{"title":39,"slug":36,"description":40,"pubDate":100,"tags":101,"coverImage":48},"Feb 28 2023",[43],[],"10/index.md","different-types-of-kernel",{"id":104,"data":106,"body":112,"filePath":113,"assetImports":114,"digest":116,"rendered":117,"legacyId":143},{"title":107,"description":108,"pubDate":109,"tags":110,"coverImage":111},"Different types of kernel","Main kernel options comparison",["Date","2023-03-05T00:00:00.000Z"],[43],"__ASTRO_IMAGE_./post_img11.png","# Mainline vs Zen Kernel: Choosing the Right Kernel for Your Linux System\n\nWhen it comes to choosing a kernel for your Linux distribution, there are many options available. Two of the most popular kernels are the mainline and Zen kernel. Both have their pros and cons, and it can be tough to choose which one is best for your system.\n\n## Mainline Kernel\n\nThe mainline kernel is the official kernel released by Linus Torvalds and the Linux kernel development team. It's the most stable and widely used kernel for Linux distributions. It contains all of the latest features and security patches, and is regularly updated with new releases. It's also very configurable, which means you can customize it to suit your needs.\n\nOne of the main advantages of the mainline kernel is its stability. Since it's the official kernel, it has undergone extensive testing and bug fixing to ensure that it's reliable and secure. This makes it a good choice for servers and other mission-critical systems.\n\nIn addition to its stability, the mainline kernel is also highly configurable. This means that you can customize it to suit your specific needs, whether you're looking to optimize performance, reduce memory usage, or improve security.\n\nHowever, one of the downsides of the mainline kernel is that it can be slower than other kernels, due to its large size and extensive feature set. It can also be more difficult to install and configure, especially for new Linux users.\n\n## Zen Kernel\n\nThe Zen kernel is a modified version of the mainline kernel that's optimized for desktop and gaming performance. It's designed to be faster and more responsive than the mainline kernel, with lower latencies and better support for modern hardware.\n\nThe Zen kernel achieves this by removing unnecessary code and features from the mainline kernel, and optimizing the remaining code for performance. It also includes a number of patches and optimizations that are not available in the mainline kernel.\n\nWhile the Zen kernel is faster and more responsive than the mainline kernel, it may not be as stable or reliable. This is because it's not as widely tested or supported as the mainline kernel. It's also less configurable, which means you may not be able to customize it as much as the mainline kernel.\n\n## Other Kernel Options\n\nBesides the mainline and Zen kernels, there are other kernel options available that may be worth considering depending on your specific needs. For example:\n\n- The PREEMPT_RT kernel is designed for real-time applications and provides low latency and high determinism. It's often used in industrial control systems and robotics.\n- The TuxOnIce kernel is optimized for hibernation and suspend-to-disk. It allows you to quickly and easily put your system into a low-power state, without losing any data or open applications.\n- The Liquorix kernel is a high-performance kernel optimized for desktop and multimedia use. It includes a number of patches and optimizations that are not available in the mainline kernel or Zen kernel, such as the BFS scheduler and the MuQSS CPU scheduler.\n\n## Choosing the Right Kernel for Your System\n\nIn the end, the choice between the mainline and Zen kernel comes down to your specific needs and preferences. If you prioritize stability and configurability, the mainline kernel is probably your best bet. If you're looking for better desktop and gaming performance, and don't mind sacrificing some stability, the Zen kernel may be the way to go.\n\nHowever, it's important to remember that the kernel is just one part of your Linux system. Other factors, such as your desktop environment, applications, and hardware, can also have a significant impact on your system's performance and stability. Therefore, it's important to consider your entire system when making decisions about which kernel to use.\n\nFor example, if you're running a gaming system, you may want to consider using the Zen kernel along with a lightweight desktop environment and optimized graphics drivers. On the other hand, if you're running a server, you may want to stick with the mainline kernel and focus on optimizing your network and storage configurations.\n\nRegardless of which kernel you choose, make sure to keep it up-to-date with the latest security patches and updates. This will help keep your system running smoothly and avoid any potential security vulnerabilities.\n\nIn conclusion, choosing the right kernel for your Linux system requires careful consideration of your specific needs and preferences. By weighing the pros and cons of each option and taking into account your entire system, you can make an informed decision that will help you get the most out of your Linux experience.\n\n#### Authors\n\n- [@filippo-ferrando](https://www.github.com/filippo-ferrando)","src/content/blog/11/index.md",[115],"./post_img11.png","723ab36e54e2a966",{"html":118,"metadata":119},"\u003Ch1 id=\"mainline-vs-zen-kernel-choosing-the-right-kernel-for-your-linux-system\">Mainline vs Zen Kernel: Choosing the Right Kernel for Your Linux System\u003C/h1>\n\u003Cp>When it comes to choosing a kernel for your Linux distribution, there are many options available. Two of the most popular kernels are the mainline and Zen kernel. Both have their pros and cons, and it can be tough to choose which one is best for your system.\u003C/p>\n\u003Ch2 id=\"mainline-kernel\">Mainline Kernel\u003C/h2>\n\u003Cp>The mainline kernel is the official kernel released by Linus Torvalds and the Linux kernel development team. It’s the most stable and widely used kernel for Linux distributions. It contains all of the latest features and security patches, and is regularly updated with new releases. It’s also very configurable, which means you can customize it to suit your needs.\u003C/p>\n\u003Cp>One of the main advantages of the mainline kernel is its stability. Since it’s the official kernel, it has undergone extensive testing and bug fixing to ensure that it’s reliable and secure. This makes it a good choice for servers and other mission-critical systems.\u003C/p>\n\u003Cp>In addition to its stability, the mainline kernel is also highly configurable. This means that you can customize it to suit your specific needs, whether you’re looking to optimize performance, reduce memory usage, or improve security.\u003C/p>\n\u003Cp>However, one of the downsides of the mainline kernel is that it can be slower than other kernels, due to its large size and extensive feature set. It can also be more difficult to install and configure, especially for new Linux users.\u003C/p>\n\u003Ch2 id=\"zen-kernel\">Zen Kernel\u003C/h2>\n\u003Cp>The Zen kernel is a modified version of the mainline kernel that’s optimized for desktop and gaming performance. It’s designed to be faster and more responsive than the mainline kernel, with lower latencies and better support for modern hardware.\u003C/p>\n\u003Cp>The Zen kernel achieves this by removing unnecessary code and features from the mainline kernel, and optimizing the remaining code for performance. It also includes a number of patches and optimizations that are not available in the mainline kernel.\u003C/p>\n\u003Cp>While the Zen kernel is faster and more responsive than the mainline kernel, it may not be as stable or reliable. This is because it’s not as widely tested or supported as the mainline kernel. It’s also less configurable, which means you may not be able to customize it as much as the mainline kernel.\u003C/p>\n\u003Ch2 id=\"other-kernel-options\">Other Kernel Options\u003C/h2>\n\u003Cp>Besides the mainline and Zen kernels, there are other kernel options available that may be worth considering depending on your specific needs. For example:\u003C/p>\n\u003Cul>\n\u003Cli>The PREEMPT_RT kernel is designed for real-time applications and provides low latency and high determinism. It’s often used in industrial control systems and robotics.\u003C/li>\n\u003Cli>The TuxOnIce kernel is optimized for hibernation and suspend-to-disk. It allows you to quickly and easily put your system into a low-power state, without losing any data or open applications.\u003C/li>\n\u003Cli>The Liquorix kernel is a high-performance kernel optimized for desktop and multimedia use. It includes a number of patches and optimizations that are not available in the mainline kernel or Zen kernel, such as the BFS scheduler and the MuQSS CPU scheduler.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"choosing-the-right-kernel-for-your-system\">Choosing the Right Kernel for Your System\u003C/h2>\n\u003Cp>In the end, the choice between the mainline and Zen kernel comes down to your specific needs and preferences. If you prioritize stability and configurability, the mainline kernel is probably your best bet. If you’re looking for better desktop and gaming performance, and don’t mind sacrificing some stability, the Zen kernel may be the way to go.\u003C/p>\n\u003Cp>However, it’s important to remember that the kernel is just one part of your Linux system. Other factors, such as your desktop environment, applications, and hardware, can also have a significant impact on your system’s performance and stability. Therefore, it’s important to consider your entire system when making decisions about which kernel to use.\u003C/p>\n\u003Cp>For example, if you’re running a gaming system, you may want to consider using the Zen kernel along with a lightweight desktop environment and optimized graphics drivers. On the other hand, if you’re running a server, you may want to stick with the mainline kernel and focus on optimizing your network and storage configurations.\u003C/p>\n\u003Cp>Regardless of which kernel you choose, make sure to keep it up-to-date with the latest security patches and updates. This will help keep your system running smoothly and avoid any potential security vulnerabilities.\u003C/p>\n\u003Cp>In conclusion, choosing the right kernel for your Linux system requires careful consideration of your specific needs and preferences. By weighing the pros and cons of each option and taking into account your entire system, you can make an informed decision that will help you get the most out of your Linux experience.\u003C/p>\n\u003Ch4 id=\"authors\">Authors\u003C/h4>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://www.github.com/filippo-ferrando\" target=\"_blank\" rel=\"noopener\">@filippo-ferrando\u003C/a>\u003C/li>\n\u003C/ul>",{"headings":120,"localImagePaths":137,"remoteImagePaths":138,"frontmatter":139,"imagePaths":142},[121,124,127,130,133,136],{"depth":55,"slug":122,"text":123},"mainline-vs-zen-kernel-choosing-the-right-kernel-for-your-linux-system","Mainline vs Zen Kernel: Choosing the Right Kernel for Your Linux System",{"depth":59,"slug":125,"text":126},"mainline-kernel","Mainline Kernel",{"depth":59,"slug":128,"text":129},"zen-kernel","Zen Kernel",{"depth":59,"slug":131,"text":132},"other-kernel-options","Other Kernel Options",{"depth":59,"slug":134,"text":135},"choosing-the-right-kernel-for-your-system","Choosing the Right Kernel for Your System",{"depth":94,"slug":95,"text":96},[],[],{"title":107,"slug":104,"description":108,"tags":140,"pubDate":141,"coverImage":115},[43],"Mar 05 2023",[],"11/index.md","fish-shell",{"id":144,"data":146,"body":152,"filePath":153,"assetImports":154,"digest":156,"rendered":157,"legacyId":195},{"title":147,"description":148,"pubDate":149,"tags":150,"coverImage":151},"FISH","Brief introduction to FISH shell",["Date","2023-03-10T00:00:00.000Z"],[43],"__ASTRO_IMAGE_./post_img12.webp","# An Introduction to Fish Shell\n\nFish Shell, also known as Friendly Interactive Shell, is a user-friendly and interactive shell for Unix-based systems. It is a command-line interface that provides a modern and easy-to-use environment for users.\n\nFish Shell is written in C programming language and offers many features that make it an appealing alternative to other shells such as Bash and Zsh. Some of the features that make Fish Shell stand out include:\n\n## Auto-suggestions\n\nFish Shell offers an auto-suggestion feature that suggests commands based on the user's history and current input. This feature makes it easier for users to navigate and execute commands, saving time and reducing errors. For instance, if a user types \"cd do\", the shell will suggest \"cd Documents/\", which the user can accept by pressing the right arrow key.\n\n## Syntax Highlighting\n\nFish Shell provides syntax highlighting for commands and their arguments, making it easier for users to identify and correct errors in their commands. This feature is especially useful for users who are new to the command line interface. Fish Shell uses a color scheme to highlight different parts of the command, such as the command itself, the arguments, and the options.\n\n## Tab Completion\n\nFish Shell offers a powerful tab completion feature that suggests commands and file paths as the user types. This feature saves time and reduces errors by minimizing the need for users to manually type commands and file paths. For example, if a user types \"cd D\", the shell will suggest \"cd Documents/\", and the user can accept this suggestion by pressing the tab key.\n\n## User-friendly Interface\n\nFish Shell's interface is designed to be user-friendly and intuitive. Its syntax and commands are easy to learn and remember, making it an ideal shell for beginners. The shell also has a built-in help system that provides detailed information on how to use specific commands. Fish Shell's interface is customizable, allowing users to change the color scheme, font, and other settings to suit their preferences.\n\n## Plugins\n\nFish Shell supports plugins, which allow users to extend the functionality of the shell. There are many plugins available for Fish Shell, including plugins for Git, Docker, and virtual environments. These plugins can help users automate tasks and perform complex operations with ease. For example, the Git plugin provides shortcuts for common Git commands, making it easier for users to work with Git repositories.\n\n## Compatibility\n\nFish Shell is compatible with most Unix-based systems, including macOS, Linux, and BSD. It also supports Unicode and emoji, which makes it a great choice for users who work with languages other than English. Fish Shell is lightweight and fast, making it a great choice for users who want to maximize performance without sacrificing functionality. Fish Shell's compatibility and performance make it an ideal choice for system administrators and developers who work with multiple operating systems.\n\n## Configuration\n\nFish Shell's configuration system is easy to use and flexible. Users can customize the shell's behavior and appearance by editing the configuration files. Fish Shell's configuration system is designed to be easy to understand and use, making it accessible to users with varying levels of technical expertise. The configuration files are written in a simple and easy-to-read syntax, allowing users to quickly make changes to the shell's settings.\n\n## Community\n\nFish Shell has an active and supportive community of users and developers. The community provides support, documentation, and plugins to help users get the most out of Fish Shell. The community also contributes to the development of Fish Shell, ensuring that it remains up-to-date and relevant. Fish Shell's community is friendly and welcoming, making it easy for new users to get started with the shell.\n\nOverall, Fish Shell is a powerful and user-friendly shell that offers many features that make it a great alternative to other shells. Its auto-suggestion, syntax highlighting, and tab completion features make it easier for users to navigate and execute commands, while its user-friendly interface makes it a great choice for beginners. With plugins, compatibility with most Unix-based systems, and a supportive community, Fish Shell is a versatile and powerful tool for anyone who needs to use the command line interface. Whether you are a system administrator, developer, or casual user, Fish Shell is an excellent choice for your command line needs.\n\n#### Authors\n\n- [@filippo-ferrando](https://www.github.com/filippo-ferrando)","src/content/blog/12/index.md",[155],"./post_img12.webp","c21e5a03f63c7121",{"html":158,"metadata":159},"\u003Ch1 id=\"an-introduction-to-fish-shell\">An Introduction to Fish Shell\u003C/h1>\n\u003Cp>Fish Shell, also known as Friendly Interactive Shell, is a user-friendly and interactive shell for Unix-based systems. It is a command-line interface that provides a modern and easy-to-use environment for users.\u003C/p>\n\u003Cp>Fish Shell is written in C programming language and offers many features that make it an appealing alternative to other shells such as Bash and Zsh. Some of the features that make Fish Shell stand out include:\u003C/p>\n\u003Ch2 id=\"auto-suggestions\">Auto-suggestions\u003C/h2>\n\u003Cp>Fish Shell offers an auto-suggestion feature that suggests commands based on the user’s history and current input. This feature makes it easier for users to navigate and execute commands, saving time and reducing errors. For instance, if a user types “cd do”, the shell will suggest “cd Documents/”, which the user can accept by pressing the right arrow key.\u003C/p>\n\u003Ch2 id=\"syntax-highlighting\">Syntax Highlighting\u003C/h2>\n\u003Cp>Fish Shell provides syntax highlighting for commands and their arguments, making it easier for users to identify and correct errors in their commands. This feature is especially useful for users who are new to the command line interface. Fish Shell uses a color scheme to highlight different parts of the command, such as the command itself, the arguments, and the options.\u003C/p>\n\u003Ch2 id=\"tab-completion\">Tab Completion\u003C/h2>\n\u003Cp>Fish Shell offers a powerful tab completion feature that suggests commands and file paths as the user types. This feature saves time and reduces errors by minimizing the need for users to manually type commands and file paths. For example, if a user types “cd D”, the shell will suggest “cd Documents/”, and the user can accept this suggestion by pressing the tab key.\u003C/p>\n\u003Ch2 id=\"user-friendly-interface\">User-friendly Interface\u003C/h2>\n\u003Cp>Fish Shell’s interface is designed to be user-friendly and intuitive. Its syntax and commands are easy to learn and remember, making it an ideal shell for beginners. The shell also has a built-in help system that provides detailed information on how to use specific commands. Fish Shell’s interface is customizable, allowing users to change the color scheme, font, and other settings to suit their preferences.\u003C/p>\n\u003Ch2 id=\"plugins\">Plugins\u003C/h2>\n\u003Cp>Fish Shell supports plugins, which allow users to extend the functionality of the shell. There are many plugins available for Fish Shell, including plugins for Git, Docker, and virtual environments. These plugins can help users automate tasks and perform complex operations with ease. For example, the Git plugin provides shortcuts for common Git commands, making it easier for users to work with Git repositories.\u003C/p>\n\u003Ch2 id=\"compatibility\">Compatibility\u003C/h2>\n\u003Cp>Fish Shell is compatible with most Unix-based systems, including macOS, Linux, and BSD. It also supports Unicode and emoji, which makes it a great choice for users who work with languages other than English. Fish Shell is lightweight and fast, making it a great choice for users who want to maximize performance without sacrificing functionality. Fish Shell’s compatibility and performance make it an ideal choice for system administrators and developers who work with multiple operating systems.\u003C/p>\n\u003Ch2 id=\"configuration\">Configuration\u003C/h2>\n\u003Cp>Fish Shell’s configuration system is easy to use and flexible. Users can customize the shell’s behavior and appearance by editing the configuration files. Fish Shell’s configuration system is designed to be easy to understand and use, making it accessible to users with varying levels of technical expertise. The configuration files are written in a simple and easy-to-read syntax, allowing users to quickly make changes to the shell’s settings.\u003C/p>\n\u003Ch2 id=\"community\">Community\u003C/h2>\n\u003Cp>Fish Shell has an active and supportive community of users and developers. The community provides support, documentation, and plugins to help users get the most out of Fish Shell. The community also contributes to the development of Fish Shell, ensuring that it remains up-to-date and relevant. Fish Shell’s community is friendly and welcoming, making it easy for new users to get started with the shell.\u003C/p>\n\u003Cp>Overall, Fish Shell is a powerful and user-friendly shell that offers many features that make it a great alternative to other shells. Its auto-suggestion, syntax highlighting, and tab completion features make it easier for users to navigate and execute commands, while its user-friendly interface makes it a great choice for beginners. With plugins, compatibility with most Unix-based systems, and a supportive community, Fish Shell is a versatile and powerful tool for anyone who needs to use the command line interface. Whether you are a system administrator, developer, or casual user, Fish Shell is an excellent choice for your command line needs.\u003C/p>\n\u003Ch4 id=\"authors\">Authors\u003C/h4>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://www.github.com/filippo-ferrando\" target=\"_blank\" rel=\"noopener\">@filippo-ferrando\u003C/a>\u003C/li>\n\u003C/ul>",{"headings":160,"localImagePaths":189,"remoteImagePaths":190,"frontmatter":191,"imagePaths":194},[161,164,167,170,173,176,179,182,185,188],{"depth":55,"slug":162,"text":163},"an-introduction-to-fish-shell","An Introduction to Fish Shell",{"depth":59,"slug":165,"text":166},"auto-suggestions","Auto-suggestions",{"depth":59,"slug":168,"text":169},"syntax-highlighting","Syntax Highlighting",{"depth":59,"slug":171,"text":172},"tab-completion","Tab Completion",{"depth":59,"slug":174,"text":175},"user-friendly-interface","User-friendly Interface",{"depth":59,"slug":177,"text":178},"plugins","Plugins",{"depth":59,"slug":180,"text":181},"compatibility","Compatibility",{"depth":59,"slug":183,"text":184},"configuration","Configuration",{"depth":59,"slug":186,"text":187},"community","Community",{"depth":94,"slug":95,"text":96},[],[],{"title":147,"slug":144,"description":148,"pubDate":192,"tags":193,"coverImage":155},"Mar 10 2023",[43],[],"12/index.md","the-heart-of-android-phone",{"id":196,"data":198,"body":205,"filePath":206,"assetImports":207,"digest":209,"rendered":210,"legacyId":224},{"title":199,"description":200,"pubDate":201,"tags":202,"coverImage":204},"The heart of your android phone","Brief talk about the android kernel",["Date","2023-03-21T00:00:00.000Z"],[43,203],"Android","__ASTRO_IMAGE_./post_img13.webp","# The Ubiquitous Nature of Linux in Your Phone and Beyond\n\nWhen we think of mobile phones, we often think of the apps, the sleek design, and the endless hours of scrolling. However, what we often overlook is the underlying technology that powers our phones. One of the most important components of the mobile phone is the operating system, which is responsible for managing the hardware resources and providing a platform for application development. And as it turns out, Linux is at the heart of many mobile operating systems, including Android.\n\nThe Linux kernel is an open-source software project that serves as the foundation for many operating systems, including Android. The kernel is responsible for managing the hardware resources, including the CPU, memory, and input/output devices, and providing a layer of abstraction between the hardware and the software running on it. This abstraction layer allows developers to write applications that can run on a wide range of devices, without having to worry about the underlying hardware.\n\nOne of the key benefits of open-source software is the ability to customize it to meet specific needs. Android, for example, is powered by a modified version of the Linux kernel, which has been optimized for mobile devices. This customization allows for better performance, longer battery life, and improved security.\n\nAnother way that Linux is used in mobile phones is through virtualization. Virtualization is the process of running multiple operating systems on a single piece of hardware. This can be useful for developers who want to test their apps on different operating systems without having to use multiple physical devices. By running virtual machines on a phone that uses Linux, developers can test their apps on a wide range of operating systems.\n\nFurthermore, Linux is also used in the development of mobile apps. Many popular programming languages, such as Java and Python, are supported by Linux. This means that developers can use Linux to write, test, and debug their apps, even if the app is intended for a different operating system, such as iOS.\n\nIn addition to Android, other mobile operating systems also use Linux. The Tizen operating system, developed by Samsung and Intel, uses the Linux kernel as its foundation. Additionally, many embedded systems, such as those found in cars, use Linux as well.\n\nLinux has been around for more than 20 years, and it has become one of the most popular operating systems in the world. It is used in everything from servers to supercomputers, and it powers many of the world's largest websites and services. Moreover, it is a free and open-source operating system, which means that anyone can access and modify its source code. This has led to a large and active community of developers who are constantly working to improve and enhance the operating system.\n\nLooking forward, the role of Linux in mobile computing will only continue to grow. With the rise of the Internet of Things (IoT), Linux is becoming increasingly important as a platform for connecting and managing a wide range of devices. This includes everything from smart home devices to industrial machinery. As the mobile industry continues to evolve and innovate, Linux will remain a key player in shaping the future of mobile computing.\n\nSo the next time you use your phone, remember that it's powered by the same technology that runs many of the world's servers and supercomputers. Linux truly is everywhere! It's in your phone, your car, and even in the applications that you use every day. And with the continued growth of the mobile industry, Linux will continue to play a vital role in shaping the future of mobile computing and beyond.\n\n#### Authors\n\n- [@filippo-ferrando](https://www.github.com/filippo-ferrando)","src/content/blog/13/index.md",[208],"./post_img13.webp","72eeecbb401b82a8",{"html":211,"metadata":212},"\u003Ch1 id=\"the-ubiquitous-nature-of-linux-in-your-phone-and-beyond\">The Ubiquitous Nature of Linux in Your Phone and Beyond\u003C/h1>\n\u003Cp>When we think of mobile phones, we often think of the apps, the sleek design, and the endless hours of scrolling. However, what we often overlook is the underlying technology that powers our phones. One of the most important components of the mobile phone is the operating system, which is responsible for managing the hardware resources and providing a platform for application development. And as it turns out, Linux is at the heart of many mobile operating systems, including Android.\u003C/p>\n\u003Cp>The Linux kernel is an open-source software project that serves as the foundation for many operating systems, including Android. The kernel is responsible for managing the hardware resources, including the CPU, memory, and input/output devices, and providing a layer of abstraction between the hardware and the software running on it. This abstraction layer allows developers to write applications that can run on a wide range of devices, without having to worry about the underlying hardware.\u003C/p>\n\u003Cp>One of the key benefits of open-source software is the ability to customize it to meet specific needs. Android, for example, is powered by a modified version of the Linux kernel, which has been optimized for mobile devices. This customization allows for better performance, longer battery life, and improved security.\u003C/p>\n\u003Cp>Another way that Linux is used in mobile phones is through virtualization. Virtualization is the process of running multiple operating systems on a single piece of hardware. This can be useful for developers who want to test their apps on different operating systems without having to use multiple physical devices. By running virtual machines on a phone that uses Linux, developers can test their apps on a wide range of operating systems.\u003C/p>\n\u003Cp>Furthermore, Linux is also used in the development of mobile apps. Many popular programming languages, such as Java and Python, are supported by Linux. This means that developers can use Linux to write, test, and debug their apps, even if the app is intended for a different operating system, such as iOS.\u003C/p>\n\u003Cp>In addition to Android, other mobile operating systems also use Linux. The Tizen operating system, developed by Samsung and Intel, uses the Linux kernel as its foundation. Additionally, many embedded systems, such as those found in cars, use Linux as well.\u003C/p>\n\u003Cp>Linux has been around for more than 20 years, and it has become one of the most popular operating systems in the world. It is used in everything from servers to supercomputers, and it powers many of the world’s largest websites and services. Moreover, it is a free and open-source operating system, which means that anyone can access and modify its source code. This has led to a large and active community of developers who are constantly working to improve and enhance the operating system.\u003C/p>\n\u003Cp>Looking forward, the role of Linux in mobile computing will only continue to grow. With the rise of the Internet of Things (IoT), Linux is becoming increasingly important as a platform for connecting and managing a wide range of devices. This includes everything from smart home devices to industrial machinery. As the mobile industry continues to evolve and innovate, Linux will remain a key player in shaping the future of mobile computing.\u003C/p>\n\u003Cp>So the next time you use your phone, remember that it’s powered by the same technology that runs many of the world’s servers and supercomputers. Linux truly is everywhere! It’s in your phone, your car, and even in the applications that you use every day. And with the continued growth of the mobile industry, Linux will continue to play a vital role in shaping the future of mobile computing and beyond.\u003C/p>\n\u003Ch4 id=\"authors\">Authors\u003C/h4>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://www.github.com/filippo-ferrando\" target=\"_blank\" rel=\"noopener\">@filippo-ferrando\u003C/a>\u003C/li>\n\u003C/ul>",{"headings":213,"localImagePaths":218,"remoteImagePaths":219,"frontmatter":220,"imagePaths":223},[214,217],{"depth":55,"slug":215,"text":216},"the-ubiquitous-nature-of-linux-in-your-phone-and-beyond","The Ubiquitous Nature of Linux in Your Phone and Beyond",{"depth":94,"slug":95,"text":96},[],[],{"title":199,"slug":196,"description":200,"pubDate":221,"tags":222,"coverImage":208},"Mar 21 2023",[43,203],[],"13/index.md","grub-bootloader",{"id":225,"data":227,"body":233,"filePath":234,"assetImports":235,"digest":237,"rendered":238,"legacyId":264},{"title":228,"description":229,"pubDate":230,"tags":231,"coverImage":232},"The GRUB Bootloader","Introduction to Grub bootloader",["Date","2023-03-27T00:00:00.000Z"],[43],"__ASTRO_IMAGE_./post_img14.png","# GRUB Bootloader\n\nGRUB (GRand Unified Bootloader) is a powerful bootloader software used in most Linux distributions. A bootloader is a program that manages the boot process of an operating system, allowing the user to select the OS to boot from a list of available ones.\n\n## How GRUB works\n\nGRUB is installed on the computer's hard drive or an external drive, where it stores configuration files and a list of available operating systems. When the computer is turned on, the BIOS (Basic Input/Output System) or the UEFI (Unified Extensible Firmware Interface) firmware locates the bootloader on the hard drive and loads it into the memory.\n\nGRUB then displays a menu that allows the user to select the operating system to boot or enter additional boot parameters. The menu can be customized to include various options, such as recovery mode or different kernel versions.\n\nThe default menu timeout is usually set to 5 seconds, but this can be changed by the user. If the user does not select an operating system within the timeout period, GRUB will boot the default operating system.\n\n## Features of GRUB\n\nGRUB is a flexible and customizable bootloader with numerous features, including:\n\n- Support for multiple filesystems, such as ext4, NTFS, and FAT.\n- Multiboot support, allowing the user to boot from different operating systems and kernels.\n- Customizable menu with different themes and options.\n- Password protection for the bootloader and menu entries.\n- Boot options, such as kernel parameters and recovery mode.\n- Support for booting from network devices and USB drives.\n- Ability to load other bootloader software, such as LILO or Windows Boot Manager.\n\n## GRUB configuration\n\nGRUB configuration is stored in the /boot/grub/grub.cfg file, which is generated automatically by the ```sudo grub-mkconfig -o /boot/grub/grub.cfg``` command or manually edited by the user. The file contains the list of available operating systems and the menu options.\n\nUsers can customize the GRUB menu by editing the /etc/default/grub file, which contains various options, such as timeout, default menu entry, and kernel parameters. The changes can be applied by running the update-grub command.\n\nIt is important to note that the /boot/grub/grub.cfg file should not be edited manually, as it is generated by the update-grub command and any manual changes may be overwritten. Instead, users should edit the /etc/default/grub file and then run the update-grub command to generate a new /boot/grub/grub.cfg file.\n\n## Some additional information\n\nGRUB is not only used in Linux distributions but also in some other operating systems. For example, GRUB can be used to boot Mac OS X, Windows, and BSD-based operating systems.\n\nGRUB can be used to load and boot other operating systems from the hard drive or a network device. This feature is called chainloading, and it is commonly used to boot Windows or other Linux distributions.\n\nGRUB can also be used to boot different kernels or configurations of the same operating system. For example, if the user has multiple Linux distributions installed, GRUB can be used to select a specific kernel or configuration to boot.\n\nIn addition to the features mentioned above, GRUB also supports advanced features, such as booting from a RAID array, LVM (Logical Volume Manager), or encrypted filesystems.\n\n## Conclusion\n\nGRUB is a powerful and versatile bootloader that provides users with various options and features for managing the boot process of their operating systems. Its flexibility and customization options make it a popular choice for Linux users, allowing them to boot from different configurations and kernels. Whether you need to boot a different operating system, select a specific kernel configuration, or use advanced features such as LVM or RAID, GRUB is the go-to bootloader for most Linux distributions. And with its support for other operating systems and advanced features, it is a great choice for anyone looking for a reliable and customizable bootloader.\n\n#### Authors\n\n- [@filippo-ferrando](https://www.github.com/filippo-ferrando)","src/content/blog/14/index.md",[236],"./post_img14.png","673c94bed55261ee",{"html":239,"metadata":240},"\u003Ch1 id=\"grub-bootloader\">GRUB Bootloader\u003C/h1>\n\u003Cp>GRUB (GRand Unified Bootloader) is a powerful bootloader software used in most Linux distributions. A bootloader is a program that manages the boot process of an operating system, allowing the user to select the OS to boot from a list of available ones.\u003C/p>\n\u003Ch2 id=\"how-grub-works\">How GRUB works\u003C/h2>\n\u003Cp>GRUB is installed on the computer’s hard drive or an external drive, where it stores configuration files and a list of available operating systems. When the computer is turned on, the BIOS (Basic Input/Output System) or the UEFI (Unified Extensible Firmware Interface) firmware locates the bootloader on the hard drive and loads it into the memory.\u003C/p>\n\u003Cp>GRUB then displays a menu that allows the user to select the operating system to boot or enter additional boot parameters. The menu can be customized to include various options, such as recovery mode or different kernel versions.\u003C/p>\n\u003Cp>The default menu timeout is usually set to 5 seconds, but this can be changed by the user. If the user does not select an operating system within the timeout period, GRUB will boot the default operating system.\u003C/p>\n\u003Ch2 id=\"features-of-grub\">Features of GRUB\u003C/h2>\n\u003Cp>GRUB is a flexible and customizable bootloader with numerous features, including:\u003C/p>\n\u003Cul>\n\u003Cli>Support for multiple filesystems, such as ext4, NTFS, and FAT.\u003C/li>\n\u003Cli>Multiboot support, allowing the user to boot from different operating systems and kernels.\u003C/li>\n\u003Cli>Customizable menu with different themes and options.\u003C/li>\n\u003Cli>Password protection for the bootloader and menu entries.\u003C/li>\n\u003Cli>Boot options, such as kernel parameters and recovery mode.\u003C/li>\n\u003Cli>Support for booting from network devices and USB drives.\u003C/li>\n\u003Cli>Ability to load other bootloader software, such as LILO or Windows Boot Manager.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"grub-configuration\">GRUB configuration\u003C/h2>\n\u003Cp>GRUB configuration is stored in the /boot/grub/grub.cfg file, which is generated automatically by the \u003Ccode>sudo grub-mkconfig -o /boot/grub/grub.cfg\u003C/code> command or manually edited by the user. The file contains the list of available operating systems and the menu options.\u003C/p>\n\u003Cp>Users can customize the GRUB menu by editing the /etc/default/grub file, which contains various options, such as timeout, default menu entry, and kernel parameters. The changes can be applied by running the update-grub command.\u003C/p>\n\u003Cp>It is important to note that the /boot/grub/grub.cfg file should not be edited manually, as it is generated by the update-grub command and any manual changes may be overwritten. Instead, users should edit the /etc/default/grub file and then run the update-grub command to generate a new /boot/grub/grub.cfg file.\u003C/p>\n\u003Ch2 id=\"some-additional-information\">Some additional information\u003C/h2>\n\u003Cp>GRUB is not only used in Linux distributions but also in some other operating systems. For example, GRUB can be used to boot Mac OS X, Windows, and BSD-based operating systems.\u003C/p>\n\u003Cp>GRUB can be used to load and boot other operating systems from the hard drive or a network device. This feature is called chainloading, and it is commonly used to boot Windows or other Linux distributions.\u003C/p>\n\u003Cp>GRUB can also be used to boot different kernels or configurations of the same operating system. For example, if the user has multiple Linux distributions installed, GRUB can be used to select a specific kernel or configuration to boot.\u003C/p>\n\u003Cp>In addition to the features mentioned above, GRUB also supports advanced features, such as booting from a RAID array, LVM (Logical Volume Manager), or encrypted filesystems.\u003C/p>\n\u003Ch2 id=\"conclusion\">Conclusion\u003C/h2>\n\u003Cp>GRUB is a powerful and versatile bootloader that provides users with various options and features for managing the boot process of their operating systems. Its flexibility and customization options make it a popular choice for Linux users, allowing them to boot from different configurations and kernels. Whether you need to boot a different operating system, select a specific kernel configuration, or use advanced features such as LVM or RAID, GRUB is the go-to bootloader for most Linux distributions. And with its support for other operating systems and advanced features, it is a great choice for anyone looking for a reliable and customizable bootloader.\u003C/p>\n\u003Ch4 id=\"authors\">Authors\u003C/h4>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://www.github.com/filippo-ferrando\" target=\"_blank\" rel=\"noopener\">@filippo-ferrando\u003C/a>\u003C/li>\n\u003C/ul>",{"headings":241,"localImagePaths":258,"remoteImagePaths":259,"frontmatter":260,"imagePaths":263},[242,244,247,250,253,256,257],{"depth":55,"slug":225,"text":243},"GRUB Bootloader",{"depth":59,"slug":245,"text":246},"how-grub-works","How GRUB works",{"depth":59,"slug":248,"text":249},"features-of-grub","Features of GRUB",{"depth":59,"slug":251,"text":252},"grub-configuration","GRUB configuration",{"depth":59,"slug":254,"text":255},"some-additional-information","Some additional information",{"depth":59,"slug":91,"text":92},{"depth":94,"slug":95,"text":96},[],[],{"title":228,"slug":225,"description":229,"pubDate":261,"tags":262,"coverImage":236},"Mar 27 2023",[43],[],"14/index.md","5g-connection",{"id":265,"data":267,"body":275,"filePath":276,"assetImports":277,"digest":279,"rendered":280,"legacyId":313},{"title":268,"description":269,"pubDate":270,"tags":271,"coverImage":274},"5G Connection","Overview about the new 5G technology",["Date","2023-04-05T00:00:00.000Z"],[272,273],"Networking","Broadband","__ASTRO_IMAGE_./post_img15.jpg","# 5G connection\n\n5G is the fifth generation of wireless technology that promises to revolutionize the way we communicate, work, and live. It is the latest standard of mobile networks that offers lightning-fast data speeds, greater bandwidth, and low latency. In this blog post, we will discuss the new standard of 5G and its impact on the world.\n\n## What is 5G?\n\n5G is the latest standard of mobile networks that promises to offer faster data speeds, lower latency, and greater bandwidth. It is the successor to 4G and 3G networks, and it uses advanced technologies such as millimeter-wave spectrum, massive MIMO, and beamforming to provide faster speeds and greater capacity.\n\n## 5G New Standard:\n\nThe new standard of 5G is designed to offer even faster data speeds and greater capacity. The new standard includes three different types of 5G networks: low-band, mid-band, and high-band. Each type of network offers different speeds and capacity, depending on the frequency spectrum used.\n\n### Low-band 5G:\n\nLow-band 5G operates on frequencies below 1GHz and offers the widest coverage area of the three types of networks. It is designed to provide better indoor coverage and reliable connectivity in rural areas. The data speeds offered by low-band 5G are slightly faster than 4G.\n\n### Mid-band 5G:\n\nMid-band 5G operates on frequencies between 1GHz and 6GHz and offers faster data speeds and greater capacity than low-band 5G. It is designed to provide better outdoor coverage and reliable connectivity in urban areas. The data speeds offered by mid-band 5G are several times faster than 4G.\n\n### High-band 5G:\n\nHigh-band 5G operates on frequencies above 24GHz and offers the fastest data speeds and the greatest capacity of the three types of networks. It is designed to provide ultra-fast connectivity for high-bandwidth applications such as video streaming and virtual reality. The data speeds offered by high-band 5G are up to 100 times faster than 4G.\n\n### Impact of 5G:\n\nThe impact of 5G on the world is expected to be significant. It will enable the development of new applications and services that were not possible with previous generations of wireless technology. Here are some of the ways 5G is expected to impact the world:\n\n1. Faster Internet Speeds:\n    \n    With 5G, users can expect much faster internet speeds than before. This means that downloading and uploading large files will take much less time, and streaming high-quality video content will be smoother.\n    \n2. Improved Mobile Connectivity:\n    \n    5G will provide better connectivity for mobile devices in crowded urban areas. This means that users will be able to access the internet and make phone calls with fewer dropped connections.\n    \n3. Enhanced Virtual Reality and Augmented Reality:\n    \n    The high-band 5G network will enable the development of advanced virtual and augmented reality applications. This technology will revolutionize the way we interact with the world and enable new possibilities in gaming, education, and healthcare.\n    \n4. Increased Automation and Efficiency:\n    \n    5G will enable the development of advanced automation technologies that can improve efficiency in manufacturing and logistics. The low latency and high-speed connectivity of 5G will allow machines to communicate with each other in real-time, reducing errors and improving productivity.\n    \n\n## Conclusion:\n\nThe new standard of 5G is set to revolutionize the way we communicate, work, and live. It offers faster data speeds, greater bandwidth, and lower latency than previous generations of wireless technology. With the development of new applications and services, 5G is expected to have a significant impact on the world in the coming years.\n\n#### Authors\n\n- [@filippo-ferrando](https://www.github.com/filippo-ferrando)","src/content/blog/15/index.md",[278],"./post_img15.jpg","985fb05d61a4a92d",{"html":281,"metadata":282},"\u003Ch1 id=\"5g-connection\">5G connection\u003C/h1>\n\u003Cp>5G is the fifth generation of wireless technology that promises to revolutionize the way we communicate, work, and live. It is the latest standard of mobile networks that offers lightning-fast data speeds, greater bandwidth, and low latency. In this blog post, we will discuss the new standard of 5G and its impact on the world.\u003C/p>\n\u003Ch2 id=\"what-is-5g\">What is 5G?\u003C/h2>\n\u003Cp>5G is the latest standard of mobile networks that promises to offer faster data speeds, lower latency, and greater bandwidth. It is the successor to 4G and 3G networks, and it uses advanced technologies such as millimeter-wave spectrum, massive MIMO, and beamforming to provide faster speeds and greater capacity.\u003C/p>\n\u003Ch2 id=\"5g-new-standard\">5G New Standard:\u003C/h2>\n\u003Cp>The new standard of 5G is designed to offer even faster data speeds and greater capacity. The new standard includes three different types of 5G networks: low-band, mid-band, and high-band. Each type of network offers different speeds and capacity, depending on the frequency spectrum used.\u003C/p>\n\u003Ch3 id=\"low-band-5g\">Low-band 5G:\u003C/h3>\n\u003Cp>Low-band 5G operates on frequencies below 1GHz and offers the widest coverage area of the three types of networks. It is designed to provide better indoor coverage and reliable connectivity in rural areas. The data speeds offered by low-band 5G are slightly faster than 4G.\u003C/p>\n\u003Ch3 id=\"mid-band-5g\">Mid-band 5G:\u003C/h3>\n\u003Cp>Mid-band 5G operates on frequencies between 1GHz and 6GHz and offers faster data speeds and greater capacity than low-band 5G. It is designed to provide better outdoor coverage and reliable connectivity in urban areas. The data speeds offered by mid-band 5G are several times faster than 4G.\u003C/p>\n\u003Ch3 id=\"high-band-5g\">High-band 5G:\u003C/h3>\n\u003Cp>High-band 5G operates on frequencies above 24GHz and offers the fastest data speeds and the greatest capacity of the three types of networks. It is designed to provide ultra-fast connectivity for high-bandwidth applications such as video streaming and virtual reality. The data speeds offered by high-band 5G are up to 100 times faster than 4G.\u003C/p>\n\u003Ch3 id=\"impact-of-5g\">Impact of 5G:\u003C/h3>\n\u003Cp>The impact of 5G on the world is expected to be significant. It will enable the development of new applications and services that were not possible with previous generations of wireless technology. Here are some of the ways 5G is expected to impact the world:\u003C/p>\n\u003Col>\n\u003Cli>\n\u003Cp>Faster Internet Speeds:\u003C/p>\n\u003Cp>With 5G, users can expect much faster internet speeds than before. This means that downloading and uploading large files will take much less time, and streaming high-quality video content will be smoother.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>Improved Mobile Connectivity:\u003C/p>\n\u003Cp>5G will provide better connectivity for mobile devices in crowded urban areas. This means that users will be able to access the internet and make phone calls with fewer dropped connections.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>Enhanced Virtual Reality and Augmented Reality:\u003C/p>\n\u003Cp>The high-band 5G network will enable the development of advanced virtual and augmented reality applications. This technology will revolutionize the way we interact with the world and enable new possibilities in gaming, education, and healthcare.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>Increased Automation and Efficiency:\u003C/p>\n\u003Cp>5G will enable the development of advanced automation technologies that can improve efficiency in manufacturing and logistics. The low latency and high-speed connectivity of 5G will allow machines to communicate with each other in real-time, reducing errors and improving productivity.\u003C/p>\n\u003C/li>\n\u003C/ol>\n\u003Ch2 id=\"conclusion\">Conclusion:\u003C/h2>\n\u003Cp>The new standard of 5G is set to revolutionize the way we communicate, work, and live. It offers faster data speeds, greater bandwidth, and lower latency than previous generations of wireless technology. With the development of new applications and services, 5G is expected to have a significant impact on the world in the coming years.\u003C/p>\n\u003Ch4 id=\"authors\">Authors\u003C/h4>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://www.github.com/filippo-ferrando\" target=\"_blank\" rel=\"noopener\">@filippo-ferrando\u003C/a>\u003C/li>\n\u003C/ul>",{"headings":283,"localImagePaths":307,"remoteImagePaths":308,"frontmatter":309,"imagePaths":312},[284,286,289,292,295,298,301,304,306],{"depth":55,"slug":265,"text":285},"5G connection",{"depth":59,"slug":287,"text":288},"what-is-5g","What is 5G?",{"depth":59,"slug":290,"text":291},"5g-new-standard","5G New Standard:",{"depth":63,"slug":293,"text":294},"low-band-5g","Low-band 5G:",{"depth":63,"slug":296,"text":297},"mid-band-5g","Mid-band 5G:",{"depth":63,"slug":299,"text":300},"high-band-5g","High-band 5G:",{"depth":63,"slug":302,"text":303},"impact-of-5g","Impact of 5G:",{"depth":59,"slug":91,"text":305},"Conclusion:",{"depth":94,"slug":95,"text":96},[],[],{"title":268,"slug":265,"description":269,"pubDate":310,"tags":311,"coverImage":278},"Apr 05 2023",[272,273],[],"15/index.md","bpsc-in-deatils",{"id":314,"data":316,"body":323,"filePath":324,"assetImports":325,"digest":327,"rendered":328,"legacyId":352},{"title":317,"description":318,"pubDate":319,"tags":320,"coverImage":322},"BPSC in details","Detailed overview about smart blockchain",["Date","2023-04-12T00:00:00.000Z"],[321],"Blockchain","__ASTRO_IMAGE_./post_img16.jpg","# BPSC smart blockchain in details\n\nBPSC Smart Blockchain is a new technology that promises to revolutionize the way we store and manage data. It is a decentralized, transparent, and secure database that can be used for a variety of applications, including finance, healthcare, and logistics. In this blog post, we will discuss the features and benefits of BPSC Smart Blockchain.\n\n## What is BPSC Smart Blockchain?\n\nBPSC Smart Blockchain is a new type of blockchain technology that was developed by a team of experts in blockchain and database technology. It is a decentralized, transparent, and secure database that can be used for a variety of applications. BPSC Smart Blockchain is built on the Ethereum blockchain, which is known for its security and reliability.\n\n### Features of BPSC Smart Blockchain:\n\n1. Decentralized:\n    \n    BPSC Smart Blockchain is a decentralized database, which means that there is no central authority controlling it. This makes it more secure and reliable than traditional databases, which are vulnerable to hacking and data breaches.\n    \n2. Transparent:\n    \n    BPSC Smart Blockchain is a transparent database, which means that all transactions are recorded and can be viewed by anyone. This makes it easier to track and manage data, and it also increases trust and transparency in business transactions.\n    \n3. Secure:\n    \n    BPSC Smart Blockchain is a secure database, which means that all data is encrypted and protected from unauthorized access. This makes it more secure than traditional databases, which can be hacked or compromised.\n    \n\n### Benefits of BPSC Smart Blockchain:\n\n1. Increased Efficiency:\n    \n    BPSC Smart Blockchain can increase efficiency in business processes by eliminating the need for intermediaries and reducing the time and cost of transactions.\n    \n2. Improved Transparency:\n    \n    BPSC Smart Blockchain can improve transparency in business transactions by making all transactions visible and accessible to everyone. This can help to reduce fraud and increase trust in business relationships.\n    \n3. Enhanced Security:\n    \n    BPSC Smart Blockchain can enhance security by providing a secure and transparent database that is resistant to hacking and data breaches. This can help to protect sensitive data and prevent unauthorized access.\n    \n4. Greater Flexibility:\n    \n    BPSC Smart Blockchain can be used for a variety of applications, including finance, healthcare, and logistics. This makes it a flexible and versatile technology that can be customized to meet the needs of different industries.\n    \n\n## Conclusion:\n\nBPSC Smart Blockchain is a new technology that promises to revolutionize the way we store and manage data. It is a decentralized, transparent, and secure database that can be used for a variety of applications. With its many features and benefits, BPSC Smart Blockchain has the potential to transform the way we do business and improve efficiency, transparency, and security in business transactions.\n\n#### Authors\n\n- [@filippo-ferrando](https://www.github.com/filippo-ferrando)","src/content/blog/16/index.md",[326],"./post_img16.jpg","5332a72c6cfe32c6",{"html":329,"metadata":330},"\u003Ch1 id=\"bpsc-smart-blockchain-in-details\">BPSC smart blockchain in details\u003C/h1>\n\u003Cp>BPSC Smart Blockchain is a new technology that promises to revolutionize the way we store and manage data. It is a decentralized, transparent, and secure database that can be used for a variety of applications, including finance, healthcare, and logistics. In this blog post, we will discuss the features and benefits of BPSC Smart Blockchain.\u003C/p>\n\u003Ch2 id=\"what-is-bpsc-smart-blockchain\">What is BPSC Smart Blockchain?\u003C/h2>\n\u003Cp>BPSC Smart Blockchain is a new type of blockchain technology that was developed by a team of experts in blockchain and database technology. It is a decentralized, transparent, and secure database that can be used for a variety of applications. BPSC Smart Blockchain is built on the Ethereum blockchain, which is known for its security and reliability.\u003C/p>\n\u003Ch3 id=\"features-of-bpsc-smart-blockchain\">Features of BPSC Smart Blockchain:\u003C/h3>\n\u003Col>\n\u003Cli>\n\u003Cp>Decentralized:\u003C/p>\n\u003Cp>BPSC Smart Blockchain is a decentralized database, which means that there is no central authority controlling it. This makes it more secure and reliable than traditional databases, which are vulnerable to hacking and data breaches.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>Transparent:\u003C/p>\n\u003Cp>BPSC Smart Blockchain is a transparent database, which means that all transactions are recorded and can be viewed by anyone. This makes it easier to track and manage data, and it also increases trust and transparency in business transactions.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>Secure:\u003C/p>\n\u003Cp>BPSC Smart Blockchain is a secure database, which means that all data is encrypted and protected from unauthorized access. This makes it more secure than traditional databases, which can be hacked or compromised.\u003C/p>\n\u003C/li>\n\u003C/ol>\n\u003Ch3 id=\"benefits-of-bpsc-smart-blockchain\">Benefits of BPSC Smart Blockchain:\u003C/h3>\n\u003Col>\n\u003Cli>\n\u003Cp>Increased Efficiency:\u003C/p>\n\u003Cp>BPSC Smart Blockchain can increase efficiency in business processes by eliminating the need for intermediaries and reducing the time and cost of transactions.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>Improved Transparency:\u003C/p>\n\u003Cp>BPSC Smart Blockchain can improve transparency in business transactions by making all transactions visible and accessible to everyone. This can help to reduce fraud and increase trust in business relationships.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>Enhanced Security:\u003C/p>\n\u003Cp>BPSC Smart Blockchain can enhance security by providing a secure and transparent database that is resistant to hacking and data breaches. This can help to protect sensitive data and prevent unauthorized access.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>Greater Flexibility:\u003C/p>\n\u003Cp>BPSC Smart Blockchain can be used for a variety of applications, including finance, healthcare, and logistics. This makes it a flexible and versatile technology that can be customized to meet the needs of different industries.\u003C/p>\n\u003C/li>\n\u003C/ol>\n\u003Ch2 id=\"conclusion\">Conclusion:\u003C/h2>\n\u003Cp>BPSC Smart Blockchain is a new technology that promises to revolutionize the way we store and manage data. It is a decentralized, transparent, and secure database that can be used for a variety of applications. With its many features and benefits, BPSC Smart Blockchain has the potential to transform the way we do business and improve efficiency, transparency, and security in business transactions.\u003C/p>\n\u003Ch4 id=\"authors\">Authors\u003C/h4>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://www.github.com/filippo-ferrando\" target=\"_blank\" rel=\"noopener\">@filippo-ferrando\u003C/a>\u003C/li>\n\u003C/ul>",{"headings":331,"localImagePaths":346,"remoteImagePaths":347,"frontmatter":348,"imagePaths":351},[332,335,338,341,344,345],{"depth":55,"slug":333,"text":334},"bpsc-smart-blockchain-in-details","BPSC smart blockchain in details",{"depth":59,"slug":336,"text":337},"what-is-bpsc-smart-blockchain","What is BPSC Smart Blockchain?",{"depth":63,"slug":339,"text":340},"features-of-bpsc-smart-blockchain","Features of BPSC Smart Blockchain:",{"depth":63,"slug":342,"text":343},"benefits-of-bpsc-smart-blockchain","Benefits of BPSC Smart Blockchain:",{"depth":59,"slug":91,"text":305},{"depth":94,"slug":95,"text":96},[],[],{"title":317,"slug":314,"description":318,"pubDate":349,"tags":350,"coverImage":326},"Apr 12 2023",[321],[],"16/index.md","kde-plasma",{"id":353,"data":355,"body":362,"filePath":363,"assetImports":364,"digest":366,"rendered":367,"legacyId":392},{"title":356,"description":357,"pubDate":358,"tags":359,"coverImage":361},"KDE Plasma","My point of view about KDE Desktop Environment",["Date","2023-04-26T00:00:00.000Z"],[43,360],"Desktop","__ASTRO_IMAGE_./post_img17.png","# Why KDE Plasma is One of the Best Desktop Environments for Linux\n\nIf you're a Linux user, you know that the desktop environment (DE) you choose can significantly impact your experience. KDE Plasma is a popular DE that offers users a sleek, customizable, and efficient desktop environment. Here are some reasons why KDE Plasma is one of the best DEs for Linux.\n\n## Customizability\n\nKDE Plasma is one of the most customizable DEs out there. You can change almost everything from the appearance of your desktop to the behavior of your applications. KDE Plasma's flexibility allows you to tailor your desktop to your specific needs and preferences. The Plasma desktop offers multiple themes, icons, and widgets to help you make your desktop look and feel the way you want it to.\n\n## Performance\n\nKDE Plasma is a lightweight DE that can run smoothly on older hardware. Plasma's efficient resource management and optimized codebase make it an excellent choice for machines with limited resources. Plasma's smooth animations and responsiveness make it feel snappy and enjoyable to use, even on older hardware.\n\n## Functionality\n\nKDE Plasma offers a wealth of features out of the box. The KDE Plasma desktop comes with a robust set of default applications that cover a wide range of use cases. KDE Plasma also integrates well with other applications and services, including Google Drive and Dropbox, making it an excellent choice for users who need to work across multiple platforms.\n\n## Community Support\n\nKDE Plasma has a large and active community that is always working on improving and expanding the DE's functionality. The community's passion for KDE Plasma means that users can expect frequent updates, bug fixes, and new features. Additionally, the KDE community is incredibly welcoming and helpful, making it easy for users to get help and contribute to the development of the DE.\n\n## Conclusion\n\nKDE Plasma is a powerful, customizable, and efficient DE that offers users an excellent Linux desktop experience. Its customizability, performance, functionality, and community support make it an excellent choice for users who want to tailor their desktop to their specific needs and preferences. Whether you're a seasoned Linux user or just getting started, KDE Plasma is an excellent DE to consider.\n\n#### Authors\n\n- [@filippo-ferrando](https://www.github.com/filippo-ferrando)","src/content/blog/17/index.md",[365],"./post_img17.png","7452632dd460053a",{"html":368,"metadata":369},"\u003Ch1 id=\"why-kde-plasma-is-one-of-the-best-desktop-environments-for-linux\">Why KDE Plasma is One of the Best Desktop Environments for Linux\u003C/h1>\n\u003Cp>If you’re a Linux user, you know that the desktop environment (DE) you choose can significantly impact your experience. KDE Plasma is a popular DE that offers users a sleek, customizable, and efficient desktop environment. Here are some reasons why KDE Plasma is one of the best DEs for Linux.\u003C/p>\n\u003Ch2 id=\"customizability\">Customizability\u003C/h2>\n\u003Cp>KDE Plasma is one of the most customizable DEs out there. You can change almost everything from the appearance of your desktop to the behavior of your applications. KDE Plasma’s flexibility allows you to tailor your desktop to your specific needs and preferences. The Plasma desktop offers multiple themes, icons, and widgets to help you make your desktop look and feel the way you want it to.\u003C/p>\n\u003Ch2 id=\"performance\">Performance\u003C/h2>\n\u003Cp>KDE Plasma is a lightweight DE that can run smoothly on older hardware. Plasma’s efficient resource management and optimized codebase make it an excellent choice for machines with limited resources. Plasma’s smooth animations and responsiveness make it feel snappy and enjoyable to use, even on older hardware.\u003C/p>\n\u003Ch2 id=\"functionality\">Functionality\u003C/h2>\n\u003Cp>KDE Plasma offers a wealth of features out of the box. The KDE Plasma desktop comes with a robust set of default applications that cover a wide range of use cases. KDE Plasma also integrates well with other applications and services, including Google Drive and Dropbox, making it an excellent choice for users who need to work across multiple platforms.\u003C/p>\n\u003Ch2 id=\"community-support\">Community Support\u003C/h2>\n\u003Cp>KDE Plasma has a large and active community that is always working on improving and expanding the DE’s functionality. The community’s passion for KDE Plasma means that users can expect frequent updates, bug fixes, and new features. Additionally, the KDE community is incredibly welcoming and helpful, making it easy for users to get help and contribute to the development of the DE.\u003C/p>\n\u003Ch2 id=\"conclusion\">Conclusion\u003C/h2>\n\u003Cp>KDE Plasma is a powerful, customizable, and efficient DE that offers users an excellent Linux desktop experience. Its customizability, performance, functionality, and community support make it an excellent choice for users who want to tailor their desktop to their specific needs and preferences. Whether you’re a seasoned Linux user or just getting started, KDE Plasma is an excellent DE to consider.\u003C/p>\n\u003Ch4 id=\"authors\">Authors\u003C/h4>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://www.github.com/filippo-ferrando\" target=\"_blank\" rel=\"noopener\">@filippo-ferrando\u003C/a>\u003C/li>\n\u003C/ul>",{"headings":370,"localImagePaths":386,"remoteImagePaths":387,"frontmatter":388,"imagePaths":391},[371,374,377,378,381,384,385],{"depth":55,"slug":372,"text":373},"why-kde-plasma-is-one-of-the-best-desktop-environments-for-linux","Why KDE Plasma is One of the Best Desktop Environments for Linux",{"depth":59,"slug":375,"text":376},"customizability","Customizability",{"depth":59,"slug":79,"text":80},{"depth":59,"slug":379,"text":380},"functionality","Functionality",{"depth":59,"slug":382,"text":383},"community-support","Community Support",{"depth":59,"slug":91,"text":92},{"depth":94,"slug":95,"text":96},[],[],{"title":356,"slug":353,"description":357,"pubDate":389,"tags":390,"coverImage":365},"Apr 26 2023",[43,360],[],"17/index.md","ssl",{"id":393,"data":395,"body":401,"filePath":402,"assetImports":403,"digest":405,"rendered":406,"legacyId":430},{"title":396,"description":397,"pubDate":398,"tags":399,"coverImage":400},"SSL","Brief explanation of SSL encryption",["Date","2023-05-05T00:00:00.000Z"],[272],"__ASTRO_IMAGE_./post_img18.jpg","# What is the \"S\" in HTTPS?\n\nIf you've ever visited a website, you've probably noticed that some URLs start with \"http://\" while others start with \"https://\". The extra \"s\" in \"https\" stands for \"secure\". But what does that mean, exactly?\n\n## HTTP vs. HTTPS\n\nHTTP (Hypertext Transfer Protocol) is the protocol used to transmit data between a web server and a web browser. When you enter a URL into your browser, your browser sends an HTTP request to the web server asking for the webpage. The server then sends back an HTTP response with the requested page.\n\nHTTPS is an extension of HTTP that adds an extra layer of security to the communication between the web server and the browser. HTTPS uses SSL/TLS (Secure Sockets Layer/Transport Layer Security) to encrypt the data transmitted between the server and the browser, making it much harder for third parties to intercept and read the data.\n\n## Why is HTTPS Important?\n\nHTTPS is essential for ensuring the security and privacy of data transmitted over the internet. Without HTTPS, it would be easy for attackers to intercept and read sensitive data, such as login credentials or credit card information. By encrypting the data transmitted between the server and the browser, HTTPS makes it much harder for attackers to access and read that data.\n\nHTTPS is particularly important for websites that handle sensitive information, such as e-commerce sites, online banking sites, and social networking sites. These sites must ensure that their users' data is kept secure and private.\n\n## How to Tell if a Site is Using HTTPS\n\nYou can tell if a website is using HTTPS by looking at the URL. If the URL starts with \"https://\" instead of \"http://\", the site is using HTTPS. You can also look for a padlock icon in the browser's address bar. The padlock icon indicates that the site is using HTTPS and that the data transmitted between the server and the browser is encrypted.\n\n## Conclusion\n\nThe \"S\" in HTTPS stands for \"secure\". HTTPS is an extension of HTTP that adds an extra layer of security to the communication between the web server and the browser. HTTPS is essential for ensuring the security and privacy of data transmitted over the internet, particularly for sites that handle sensitive information. By using HTTPS, websites can help protect their users' data and provide a more secure browsing experience.\n\n#### Authors\n\n- [@filippo-ferrando](https://www.github.com/filippo-ferrando)","src/content/blog/18/index.md",[404],"./post_img18.jpg","19dba38cabe5975d",{"html":407,"metadata":408},"\u003Ch1 id=\"what-is-the-s-in-https\">What is the “S” in HTTPS?\u003C/h1>\n\u003Cp>If you’ve ever visited a website, you’ve probably noticed that some URLs start with “http://” while others start with “https://”. The extra “s” in “https” stands for “secure”. But what does that mean, exactly?\u003C/p>\n\u003Ch2 id=\"http-vs-https\">HTTP vs. HTTPS\u003C/h2>\n\u003Cp>HTTP (Hypertext Transfer Protocol) is the protocol used to transmit data between a web server and a web browser. When you enter a URL into your browser, your browser sends an HTTP request to the web server asking for the webpage. The server then sends back an HTTP response with the requested page.\u003C/p>\n\u003Cp>HTTPS is an extension of HTTP that adds an extra layer of security to the communication between the web server and the browser. HTTPS uses SSL/TLS (Secure Sockets Layer/Transport Layer Security) to encrypt the data transmitted between the server and the browser, making it much harder for third parties to intercept and read the data.\u003C/p>\n\u003Ch2 id=\"why-is-https-important\">Why is HTTPS Important?\u003C/h2>\n\u003Cp>HTTPS is essential for ensuring the security and privacy of data transmitted over the internet. Without HTTPS, it would be easy for attackers to intercept and read sensitive data, such as login credentials or credit card information. By encrypting the data transmitted between the server and the browser, HTTPS makes it much harder for attackers to access and read that data.\u003C/p>\n\u003Cp>HTTPS is particularly important for websites that handle sensitive information, such as e-commerce sites, online banking sites, and social networking sites. These sites must ensure that their users’ data is kept secure and private.\u003C/p>\n\u003Ch2 id=\"how-to-tell-if-a-site-is-using-https\">How to Tell if a Site is Using HTTPS\u003C/h2>\n\u003Cp>You can tell if a website is using HTTPS by looking at the URL. If the URL starts with “https://” instead of “http://”, the site is using HTTPS. You can also look for a padlock icon in the browser’s address bar. The padlock icon indicates that the site is using HTTPS and that the data transmitted between the server and the browser is encrypted.\u003C/p>\n\u003Ch2 id=\"conclusion\">Conclusion\u003C/h2>\n\u003Cp>The “S” in HTTPS stands for “secure”. HTTPS is an extension of HTTP that adds an extra layer of security to the communication between the web server and the browser. HTTPS is essential for ensuring the security and privacy of data transmitted over the internet, particularly for sites that handle sensitive information. By using HTTPS, websites can help protect their users’ data and provide a more secure browsing experience.\u003C/p>\n\u003Ch4 id=\"authors\">Authors\u003C/h4>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://www.github.com/filippo-ferrando\" target=\"_blank\" rel=\"noopener\">@filippo-ferrando\u003C/a>\u003C/li>\n\u003C/ul>",{"headings":409,"localImagePaths":424,"remoteImagePaths":425,"frontmatter":426,"imagePaths":429},[410,413,416,419,422,423],{"depth":55,"slug":411,"text":412},"what-is-the-s-in-https","What is the “S” in HTTPS?",{"depth":59,"slug":414,"text":415},"http-vs-https","HTTP vs. HTTPS",{"depth":59,"slug":417,"text":418},"why-is-https-important","Why is HTTPS Important?",{"depth":59,"slug":420,"text":421},"how-to-tell-if-a-site-is-using-https","How to Tell if a Site is Using HTTPS",{"depth":59,"slug":91,"text":92},{"depth":94,"slug":95,"text":96},[],[],{"title":396,"slug":393,"description":397,"pubDate":427,"tags":428,"coverImage":404},"May 05 2023",[272],[],"18/index.md","ia-vs-cybersecurity",{"id":431,"data":433,"body":441,"filePath":442,"assetImports":443,"digest":445,"rendered":446,"legacyId":476},{"title":434,"description":435,"pubDate":436,"tags":437,"coverImage":440},"IA vs. Cybersecurity","Pros and Cons about IA in Cybersecurity filed",["Date","2023-05-12T00:00:00.000Z"],[438,439],"IA","CyberSec","__ASTRO_IMAGE_./post_img19.png","# The Implications of AI in Cybersecurity\n\nArtificial intelligence (AI) has the potential to revolutionize many industries, including cybersecurity. With the increasing complexity and sophistication of cyber attacks, AI can help security professionals detect, prevent, and respond to attacks in real-time. Here are some of the implications of AI in cybersecurity.\n\n## Improved Detection and Prevention\n\nOne of the primary benefits of AI in cybersecurity is improved detection and prevention of cyber attacks. AI-powered systems can analyze vast amounts of data and identify patterns and anomalies that humans may not be able to detect. This can help security professionals identify potential threats and take action to prevent attacks before they happen.\n\n## Real-time Response\n\nAI can also help security teams respond to attacks in real-time. AI-powered systems can detect and respond to threats much faster than human analysts, which is crucial in the case of a cyber attack. Real-time response can help prevent further damage and minimize the impact of an attack.\n\n## Advanced Threat Prediction\n\nAI can help security teams predict future cyber attacks by analyzing data and identifying patterns. This can help security teams take proactive measures to prevent attacks before they happen. Predictive analytics can also help security teams identify potential vulnerabilities and take action to mitigate them before they are exploited by attackers.\n\n## Increased Automation\n\nAI can help automate many tasks related to cybersecurity, such as threat detection, incident response, and vulnerability management. This can help security teams operate more efficiently and effectively, allowing them to focus on more complex tasks that require human intervention.\n\n## Challenges\n\nWhile AI has the potential to improve cybersecurity, it also poses some challenges. One of the biggest challenges is the lack of data. AI-powered systems require large amounts of data to learn and make accurate predictions. Without sufficient data, AI-powered systems may not be able to detect threats effectively.\n\nAnother challenge is the lack of skilled professionals. AI-powered systems require skilled professionals to design, implement, and maintain them. However, there is a shortage of cybersecurity professionals with the necessary skills to work with AI-powered systems.\n\n## Conclusion\n\nAI has the potential to revolutionize cybersecurity by improving detection and prevention, providing real-time response, and predicting future attacks. While there are some challenges to implementing AI in cybersecurity, the benefits are significant. As cyber attacks become more sophisticated, AI-powered systems will become increasingly important in protecting organizations from cyber threats.\n\n#### Authors\n\n- [@filippo-ferrando](https://www.github.com/filippo-ferrando)","src/content/blog/19/index.md",[444],"./post_img19.png","38e7f52c3e911494",{"html":447,"metadata":448},"\u003Ch1 id=\"the-implications-of-ai-in-cybersecurity\">The Implications of AI in Cybersecurity\u003C/h1>\n\u003Cp>Artificial intelligence (AI) has the potential to revolutionize many industries, including cybersecurity. With the increasing complexity and sophistication of cyber attacks, AI can help security professionals detect, prevent, and respond to attacks in real-time. Here are some of the implications of AI in cybersecurity.\u003C/p>\n\u003Ch2 id=\"improved-detection-and-prevention\">Improved Detection and Prevention\u003C/h2>\n\u003Cp>One of the primary benefits of AI in cybersecurity is improved detection and prevention of cyber attacks. AI-powered systems can analyze vast amounts of data and identify patterns and anomalies that humans may not be able to detect. This can help security professionals identify potential threats and take action to prevent attacks before they happen.\u003C/p>\n\u003Ch2 id=\"real-time-response\">Real-time Response\u003C/h2>\n\u003Cp>AI can also help security teams respond to attacks in real-time. AI-powered systems can detect and respond to threats much faster than human analysts, which is crucial in the case of a cyber attack. Real-time response can help prevent further damage and minimize the impact of an attack.\u003C/p>\n\u003Ch2 id=\"advanced-threat-prediction\">Advanced Threat Prediction\u003C/h2>\n\u003Cp>AI can help security teams predict future cyber attacks by analyzing data and identifying patterns. This can help security teams take proactive measures to prevent attacks before they happen. Predictive analytics can also help security teams identify potential vulnerabilities and take action to mitigate them before they are exploited by attackers.\u003C/p>\n\u003Ch2 id=\"increased-automation\">Increased Automation\u003C/h2>\n\u003Cp>AI can help automate many tasks related to cybersecurity, such as threat detection, incident response, and vulnerability management. This can help security teams operate more efficiently and effectively, allowing them to focus on more complex tasks that require human intervention.\u003C/p>\n\u003Ch2 id=\"challenges\">Challenges\u003C/h2>\n\u003Cp>While AI has the potential to improve cybersecurity, it also poses some challenges. One of the biggest challenges is the lack of data. AI-powered systems require large amounts of data to learn and make accurate predictions. Without sufficient data, AI-powered systems may not be able to detect threats effectively.\u003C/p>\n\u003Cp>Another challenge is the lack of skilled professionals. AI-powered systems require skilled professionals to design, implement, and maintain them. However, there is a shortage of cybersecurity professionals with the necessary skills to work with AI-powered systems.\u003C/p>\n\u003Ch2 id=\"conclusion\">Conclusion\u003C/h2>\n\u003Cp>AI has the potential to revolutionize cybersecurity by improving detection and prevention, providing real-time response, and predicting future attacks. While there are some challenges to implementing AI in cybersecurity, the benefits are significant. As cyber attacks become more sophisticated, AI-powered systems will become increasingly important in protecting organizations from cyber threats.\u003C/p>\n\u003Ch4 id=\"authors\">Authors\u003C/h4>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://www.github.com/filippo-ferrando\" target=\"_blank\" rel=\"noopener\">@filippo-ferrando\u003C/a>\u003C/li>\n\u003C/ul>",{"headings":449,"localImagePaths":470,"remoteImagePaths":471,"frontmatter":472,"imagePaths":475},[450,453,456,459,462,465,468,469],{"depth":55,"slug":451,"text":452},"the-implications-of-ai-in-cybersecurity","The Implications of AI in Cybersecurity",{"depth":59,"slug":454,"text":455},"improved-detection-and-prevention","Improved Detection and Prevention",{"depth":59,"slug":457,"text":458},"real-time-response","Real-time Response",{"depth":59,"slug":460,"text":461},"advanced-threat-prediction","Advanced Threat Prediction",{"depth":59,"slug":463,"text":464},"increased-automation","Increased Automation",{"depth":59,"slug":466,"text":467},"challenges","Challenges",{"depth":59,"slug":91,"text":92},{"depth":94,"slug":95,"text":96},[],[],{"title":434,"slug":431,"description":435,"pubDate":473,"tags":474,"coverImage":444},"May 12 2023",[438,439],[],"19/index.md","why-switching-to-linux",{"id":477,"data":479,"body":485,"filePath":486,"assetImports":487,"digest":489,"rendered":490,"legacyId":516},{"title":480,"description":481,"pubDate":482,"tags":483,"coverImage":484},"Switching to Linux: why?","Advantages and disadvantages (very general) of a unix-based system",["Date","2023-01-14T00:00:00.000Z"],[43],"__ASTRO_IMAGE_./post_img2.png","# Why choose Linux and which distro to choose?\n\n### Introduction\nFor a Windows (or Mac) user, that of switching to GNU/Linux could prove to be a winning choice on several levels. Let's start at the beginning, the needs of a typical user are usually always the same: Surfing the Internet, reading email, perhaps reading and writing documents, and looking at photos and videos; all these tasks can be done on essentially any operating system, but GNU/Linux offers some advantages.\n\n### Advantages\nIn many cases one does not have a computer with hardware that is always recent and up-to-date, and this operating system fits very well with any type of components.\n\nBeyond this more technical issue the many distros based on GNU/Linux can be \"easily\" customized and adapted to one's needs.\n\nMost of the programs available for Linux distros are free! and these also include many alternatives to paid programs, which means that there will be no need to pay for a program to edit photographs or read PDFs, but let's remember that many of these programs are developed by individuals and surely these people might appreciate financial support, however small.\n\n### Disadvantage\nHowever, there are several disadvantages, the difficulty of installation is definitely one of the main ones, hardly an inexperienced user will go to change the operating system of his computer to one that is not the default, and even fewer will try to access the BIOS and to start a distro installation.\nWe have reached a point where installing any operating system with simplified installers has become much easier and affordable for everyone, however, the fact remains that it takes a minimum of knowledge.\n\nDifficulty of use is again a fairly difficult hurdle to overcome; doing a system update or installing an application is not always an easy process, but this is a problem that many Distros solve by offering a graphical user interface for the package manager in use.\nMoreover, it is relatively difficult not to find online help or some kind of help given the large community of people active in the Linux world\n\n### Recommended Distro\nGiven the intent of this guide I'm going to recommend what (in my opinion) are good distros to start approaching this type of system.\n- [Ubuntu](https://www.ubuntu-it.org/)\n- [Pop_os!](https://pop.system76.com/)\n- [Elementary Os](https://elementary.io/)\n\nFor anyone who would like to try something more \"fancy\" I can suggest these:\n- [Endeavour Os](https://endeavouros.com/)\n- [Nobara Project](https://nobaraproject.org/)\n- [Void Linux](https://voidlinux.org/)\n\nThese last 3 distros leave them alone unless you have intermediate - advanced knowledge, it might just cause you stress and living ache\n\n#### Authors\n\n- [@filippo-ferrando](https://www.github.com/filippo-ferrando)","src/content/blog/2/index.md",[488],"./post_img2.png","2171b613aefcd6e3",{"html":491,"metadata":492},"\u003Ch1 id=\"why-choose-linux-and-which-distro-to-choose\">Why choose Linux and which distro to choose?\u003C/h1>\n\u003Ch3 id=\"introduction\">Introduction\u003C/h3>\n\u003Cp>For a Windows (or Mac) user, that of switching to GNU/Linux could prove to be a winning choice on several levels. Let’s start at the beginning, the needs of a typical user are usually always the same: Surfing the Internet, reading email, perhaps reading and writing documents, and looking at photos and videos; all these tasks can be done on essentially any operating system, but GNU/Linux offers some advantages.\u003C/p>\n\u003Ch3 id=\"advantages\">Advantages\u003C/h3>\n\u003Cp>In many cases one does not have a computer with hardware that is always recent and up-to-date, and this operating system fits very well with any type of components.\u003C/p>\n\u003Cp>Beyond this more technical issue the many distros based on GNU/Linux can be “easily” customized and adapted to one’s needs.\u003C/p>\n\u003Cp>Most of the programs available for Linux distros are free! and these also include many alternatives to paid programs, which means that there will be no need to pay for a program to edit photographs or read PDFs, but let’s remember that many of these programs are developed by individuals and surely these people might appreciate financial support, however small.\u003C/p>\n\u003Ch3 id=\"disadvantage\">Disadvantage\u003C/h3>\n\u003Cp>However, there are several disadvantages, the difficulty of installation is definitely one of the main ones, hardly an inexperienced user will go to change the operating system of his computer to one that is not the default, and even fewer will try to access the BIOS and to start a distro installation.\nWe have reached a point where installing any operating system with simplified installers has become much easier and affordable for everyone, however, the fact remains that it takes a minimum of knowledge.\u003C/p>\n\u003Cp>Difficulty of use is again a fairly difficult hurdle to overcome; doing a system update or installing an application is not always an easy process, but this is a problem that many Distros solve by offering a graphical user interface for the package manager in use.\nMoreover, it is relatively difficult not to find online help or some kind of help given the large community of people active in the Linux world\u003C/p>\n\u003Ch3 id=\"recommended-distro\">Recommended Distro\u003C/h3>\n\u003Cp>Given the intent of this guide I’m going to recommend what (in my opinion) are good distros to start approaching this type of system.\u003C/p>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://www.ubuntu-it.org/\" target=\"_blank\" rel=\"noopener\">Ubuntu\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://pop.system76.com/\" target=\"_blank\" rel=\"noopener\">Pop_os!\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://elementary.io/\" target=\"_blank\" rel=\"noopener\">Elementary Os\u003C/a>\u003C/li>\n\u003C/ul>\n\u003Cp>For anyone who would like to try something more “fancy” I can suggest these:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://endeavouros.com/\" target=\"_blank\" rel=\"noopener\">Endeavour Os\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://nobaraproject.org/\" target=\"_blank\" rel=\"noopener\">Nobara Project\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://voidlinux.org/\" target=\"_blank\" rel=\"noopener\">Void Linux\u003C/a>\u003C/li>\n\u003C/ul>\n\u003Cp>These last 3 distros leave them alone unless you have intermediate - advanced knowledge, it might just cause you stress and living ache\u003C/p>\n\u003Ch4 id=\"authors\">Authors\u003C/h4>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://www.github.com/filippo-ferrando\" target=\"_blank\" rel=\"noopener\">@filippo-ferrando\u003C/a>\u003C/li>\n\u003C/ul>",{"headings":493,"localImagePaths":510,"remoteImagePaths":511,"frontmatter":512,"imagePaths":515},[494,497,500,503,506,509],{"depth":55,"slug":495,"text":496},"why-choose-linux-and-which-distro-to-choose","Why choose Linux and which distro to choose?",{"depth":63,"slug":498,"text":499},"introduction","Introduction",{"depth":63,"slug":501,"text":502},"advantages","Advantages",{"depth":63,"slug":504,"text":505},"disadvantage","Disadvantage",{"depth":63,"slug":507,"text":508},"recommended-distro","Recommended Distro",{"depth":94,"slug":95,"text":96},[],[],{"title":480,"slug":477,"description":481,"tags":513,"pubDate":514,"coverImage":488},[43],"Jan 14 2023",[],"2/index.md","macos-kernel",{"id":517,"data":519,"body":526,"filePath":527,"assetImports":528,"digest":530,"rendered":531,"legacyId":552},{"title":520,"description":521,"pubDate":522,"tags":523,"coverImage":525},"MacOs Kernel","Apple copied Linux?",["Date","2023-05-25T00:00:00.000Z"],[43,524],"MacOS","__ASTRO_IMAGE_./post_img20.jpg","# Is the macOS Kernel a Copy of the Linux One?\n\nThere is a common misconception that the macOS kernel is a copy of the Linux kernel. However, this is not entirely true. While there are similarities between the two kernels, they are not the same.\n\n## The macOS Kernel\n\nThe macOS kernel, known as XNU (XNU is Not Unix), is a hybrid kernel that is based on two other kernels: the Mach kernel and the BSD kernel. The Mach kernel provides the low-level functionality, such as memory management and thread scheduling, while the BSD kernel provides the higher-level functionality, such as file systems and networking.\n\nWhile the macOS kernel is not a copy of the Linux kernel, it does share some similarities. Both kernels are based on the Unix operating system and share many of the same features, such as virtual memory management and multitasking.\n\n## The Linux Kernel\n\nThe Linux kernel is a monolithic kernel, which means that all the kernel services are run in the same address space. The Linux kernel was created by Linus Torvalds in 1991 and is released under the GNU General Public License. The Linux kernel has a modular design, which allows it to support a wide range of hardware and software configurations.\n\nWhile the macOS kernel and the Linux kernel share many similarities, they are not identical. The Linux kernel is open source and is developed by a large community of developers, while the macOS kernel is closed source and is developed by Apple.\n\n## Conclusion\n\nWhile the macOS kernel and the Linux kernel share many similarities, they are not the same. The macOS kernel is a hybrid kernel that is based on the Mach and BSD kernels, while the Linux kernel is a monolithic kernel that is developed by a large community of developers. While both kernels share many of the same features, they have different architectures and designs. It is important to understand the differences between the two kernels to avoid confusion and misconceptions.\n\n#### Authors\n\n- [@filippo-ferrando](https://www.github.com/filippo-ferrando)","src/content/blog/20/index.md",[529],"./post_img20.jpg","a987674cd8e8e31c",{"html":532,"metadata":533},"\u003Ch1 id=\"is-the-macos-kernel-a-copy-of-the-linux-one\">Is the macOS Kernel a Copy of the Linux One?\u003C/h1>\n\u003Cp>There is a common misconception that the macOS kernel is a copy of the Linux kernel. However, this is not entirely true. While there are similarities between the two kernels, they are not the same.\u003C/p>\n\u003Ch2 id=\"the-macos-kernel\">The macOS Kernel\u003C/h2>\n\u003Cp>The macOS kernel, known as XNU (XNU is Not Unix), is a hybrid kernel that is based on two other kernels: the Mach kernel and the BSD kernel. The Mach kernel provides the low-level functionality, such as memory management and thread scheduling, while the BSD kernel provides the higher-level functionality, such as file systems and networking.\u003C/p>\n\u003Cp>While the macOS kernel is not a copy of the Linux kernel, it does share some similarities. Both kernels are based on the Unix operating system and share many of the same features, such as virtual memory management and multitasking.\u003C/p>\n\u003Ch2 id=\"the-linux-kernel\">The Linux Kernel\u003C/h2>\n\u003Cp>The Linux kernel is a monolithic kernel, which means that all the kernel services are run in the same address space. The Linux kernel was created by Linus Torvalds in 1991 and is released under the GNU General Public License. The Linux kernel has a modular design, which allows it to support a wide range of hardware and software configurations.\u003C/p>\n\u003Cp>While the macOS kernel and the Linux kernel share many similarities, they are not identical. The Linux kernel is open source and is developed by a large community of developers, while the macOS kernel is closed source and is developed by Apple.\u003C/p>\n\u003Ch2 id=\"conclusion\">Conclusion\u003C/h2>\n\u003Cp>While the macOS kernel and the Linux kernel share many similarities, they are not the same. The macOS kernel is a hybrid kernel that is based on the Mach and BSD kernels, while the Linux kernel is a monolithic kernel that is developed by a large community of developers. While both kernels share many of the same features, they have different architectures and designs. It is important to understand the differences between the two kernels to avoid confusion and misconceptions.\u003C/p>\n\u003Ch4 id=\"authors\">Authors\u003C/h4>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://www.github.com/filippo-ferrando\" target=\"_blank\" rel=\"noopener\">@filippo-ferrando\u003C/a>\u003C/li>\n\u003C/ul>",{"headings":534,"localImagePaths":546,"remoteImagePaths":547,"frontmatter":548,"imagePaths":551},[535,538,541,544,545],{"depth":55,"slug":536,"text":537},"is-the-macos-kernel-a-copy-of-the-linux-one","Is the macOS Kernel a Copy of the Linux One?",{"depth":59,"slug":539,"text":540},"the-macos-kernel","The macOS Kernel",{"depth":59,"slug":542,"text":543},"the-linux-kernel","The Linux Kernel",{"depth":59,"slug":91,"text":92},{"depth":94,"slug":95,"text":96},[],[],{"title":520,"slug":517,"description":521,"pubDate":549,"tags":550,"coverImage":529},"May 25 2023",[43,524],[],"20/index.md","yay",{"id":553,"data":555,"body":561,"filePath":562,"assetImports":563,"digest":565,"rendered":566,"legacyId":602},{"title":556,"description":557,"pubDate":558,"tags":559,"coverImage":560},"Yay","My helper of choice",["Date","2023-06-12T00:00:00.000Z"],[43],"__ASTRO_IMAGE_./post_img22.webp","# An Introduction to Yay Package Manager\n\nIf you're a Linux user, you're probably familiar with the concept of package managers. These are tools that make it easy to install software packages and manage dependencies on your system. One such package manager that's gaining popularity is Yay. In this article, we'll take a closer look at what Yay is, how it works, and why you might want to consider using it.\n\n## What is Yay?\n\nYay is a package manager for Arch Linux and other Arch-based distributions. It's a fork of the popular Yaourt package manager, which was discontinued in 2018. Yay is written in Go and is designed to be fast, reliable, and easy to use. It's also fully compatible with the Arch User Repository (AUR), which is a community-driven repository of packages for Arch Linux.\n\n## How does Yay work?\n\nYay works by using the Pacman package manager as its backend. Pacman is the default package manager for Arch Linux and is used to manage packages from the official Arch repositories. Yay extends the functionality of Pacman by adding support for the AUR. This means that you can use Yay to install packages from both the official Arch repositories and the AUR.\n\nTo use Yay, you first need to install it on your system. You can do this by running the following command in your terminal:\n\n```\nsudo pacman -S yay\n\n```\n\nOnce Yay is installed, you can use it to search for packages, install and remove packages, and update your system. For example, to search for a package, you can run:\n\n```\nyay -Ss {package name}\n\n```\n\nTo install a package, you can run:\n\n```\nyay -S {package name}\n\n```\n\nAnd to remove a package, you can run:\n\n```\nyay -R {package name}\n\n```\n\n## Why use Yay?\n\nSo why might you want to use Yay instead of the default Pacman package manager? Here are a few reasons:\n\n### AUR support\n\nAs mentioned earlier, Yay is fully compatible with the Arch User Repository (AUR). This means that you can use Yay to install packages that aren't available in the official Arch repositories. The AUR is a great resource for finding and installing packages that aren't included in the default Arch installation.\n\n### Enhanced search functionality\n\nYay includes some enhancements to the search functionality that make it easier to find the packages you're looking for. For example, you can use regular expressions to search for packages, and you can search for packages by maintainer or package description.\n\n### Improved dependency handling\n\nYay includes some improvements to the dependency handling that make it easier to manage packages on your system. For example, Yay can automatically resolve dependencies and handle conflicts between packages.\n\n### Faster updates\n\nYay is designed to be fast and efficient. This means that updates are typically faster than with other package managers. Yay also includes a feature called \"update all,\" which allows you to update all of the packages on your system with a single command.\n\n## Conclusion\n\nYay is a powerful and easy-to-use package manager for Arch Linux and other Arch-based distributions. With its support for the AUR, enhanced search functionality, improved dependency handling, and faster updates, Yay is quickly becoming a popular choice among Linux users. If you're looking for a package manager that's easy to use and gives you access to a wide range of packages, Yay is definitely worth checking out.\n\n#### Authors\n\n- [@filippo-ferrando](https://www.github.com/filippo-ferrando)","src/content/blog/22/index.md",[564],"./post_img22.webp","8d15e24eaff3324a",{"html":567,"metadata":568},"\u003Ch1 id=\"an-introduction-to-yay-package-manager\">An Introduction to Yay Package Manager\u003C/h1>\n\u003Cp>If you’re a Linux user, you’re probably familiar with the concept of package managers. These are tools that make it easy to install software packages and manage dependencies on your system. One such package manager that’s gaining popularity is Yay. In this article, we’ll take a closer look at what Yay is, how it works, and why you might want to consider using it.\u003C/p>\n\u003Ch2 id=\"what-is-yay\">What is Yay?\u003C/h2>\n\u003Cp>Yay is a package manager for Arch Linux and other Arch-based distributions. It’s a fork of the popular Yaourt package manager, which was discontinued in 2018. Yay is written in Go and is designed to be fast, reliable, and easy to use. It’s also fully compatible with the Arch User Repository (AUR), which is a community-driven repository of packages for Arch Linux.\u003C/p>\n\u003Ch2 id=\"how-does-yay-work\">How does Yay work?\u003C/h2>\n\u003Cp>Yay works by using the Pacman package manager as its backend. Pacman is the default package manager for Arch Linux and is used to manage packages from the official Arch repositories. Yay extends the functionality of Pacman by adding support for the AUR. This means that you can use Yay to install packages from both the official Arch repositories and the AUR.\u003C/p>\n\u003Cp>To use Yay, you first need to install it on your system. You can do this by running the following command in your terminal:\u003C/p>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>sudo pacman -S yay\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Once Yay is installed, you can use it to search for packages, install and remove packages, and update your system. For example, to search for a package, you can run:\u003C/p>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>yay -Ss {package name}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>To install a package, you can run:\u003C/p>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>yay -S {package name}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>And to remove a package, you can run:\u003C/p>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>yay -R {package name}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch2 id=\"why-use-yay\">Why use Yay?\u003C/h2>\n\u003Cp>So why might you want to use Yay instead of the default Pacman package manager? Here are a few reasons:\u003C/p>\n\u003Ch3 id=\"aur-support\">AUR support\u003C/h3>\n\u003Cp>As mentioned earlier, Yay is fully compatible with the Arch User Repository (AUR). This means that you can use Yay to install packages that aren’t available in the official Arch repositories. The AUR is a great resource for finding and installing packages that aren’t included in the default Arch installation.\u003C/p>\n\u003Ch3 id=\"enhanced-search-functionality\">Enhanced search functionality\u003C/h3>\n\u003Cp>Yay includes some enhancements to the search functionality that make it easier to find the packages you’re looking for. For example, you can use regular expressions to search for packages, and you can search for packages by maintainer or package description.\u003C/p>\n\u003Ch3 id=\"improved-dependency-handling\">Improved dependency handling\u003C/h3>\n\u003Cp>Yay includes some improvements to the dependency handling that make it easier to manage packages on your system. For example, Yay can automatically resolve dependencies and handle conflicts between packages.\u003C/p>\n\u003Ch3 id=\"faster-updates\">Faster updates\u003C/h3>\n\u003Cp>Yay is designed to be fast and efficient. This means that updates are typically faster than with other package managers. Yay also includes a feature called “update all,” which allows you to update all of the packages on your system with a single command.\u003C/p>\n\u003Ch2 id=\"conclusion\">Conclusion\u003C/h2>\n\u003Cp>Yay is a powerful and easy-to-use package manager for Arch Linux and other Arch-based distributions. With its support for the AUR, enhanced search functionality, improved dependency handling, and faster updates, Yay is quickly becoming a popular choice among Linux users. If you’re looking for a package manager that’s easy to use and gives you access to a wide range of packages, Yay is definitely worth checking out.\u003C/p>\n\u003Ch4 id=\"authors\">Authors\u003C/h4>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://www.github.com/filippo-ferrando\" target=\"_blank\" rel=\"noopener\">@filippo-ferrando\u003C/a>\u003C/li>\n\u003C/ul>",{"headings":569,"localImagePaths":596,"remoteImagePaths":597,"frontmatter":598,"imagePaths":601},[570,573,576,579,582,585,588,591,594,595],{"depth":55,"slug":571,"text":572},"an-introduction-to-yay-package-manager","An Introduction to Yay Package Manager",{"depth":59,"slug":574,"text":575},"what-is-yay","What is Yay?",{"depth":59,"slug":577,"text":578},"how-does-yay-work","How does Yay work?",{"depth":59,"slug":580,"text":581},"why-use-yay","Why use Yay?",{"depth":63,"slug":583,"text":584},"aur-support","AUR support",{"depth":63,"slug":586,"text":587},"enhanced-search-functionality","Enhanced search functionality",{"depth":63,"slug":589,"text":590},"improved-dependency-handling","Improved dependency handling",{"depth":63,"slug":592,"text":593},"faster-updates","Faster updates",{"depth":59,"slug":91,"text":92},{"depth":94,"slug":95,"text":96},[],[],{"title":556,"slug":553,"description":557,"pubDate":599,"tags":600,"coverImage":564},"Jun 12 2023",[43],[],"22/index.md","iot",{"id":603,"data":605,"body":612,"filePath":613,"assetImports":614,"digest":616,"rendered":617,"legacyId":631},{"title":606,"description":607,"pubDate":608,"tags":609,"coverImage":611},"IoT","Overview on Intenet of Things",["Date","2023-07-16T00:00:00.000Z"],[610,272],"Embedded","__ASTRO_IMAGE_./post_img23.webp","# The Evolution of IoT: Connecting the World One Device at a Time\n\nIn a world driven by technological advancements, the Internet of Things (IoT) has emerged as a transformative force, weaving the fabric of connectivity between our physical and digital realms. This network of interconnected devices, ranging from everyday objects to complex machinery, has opened the door to a new era of efficiency, convenience, and innovation.\nUnderstanding IoT\n\nAt its core, the Internet of Things refers to the interconnection of devices via the internet, enabling them to collect and exchange data without direct human intervention. This data exchange can occur between devices of the same kind or between heterogeneous devices, creating a seamless ecosystem of information flow.\n\nImagine waking up in the morning as your smart alarm clock synchronizes with your coffee machine to brew your favorite blend, while simultaneously adjusting your thermostat to your preferred temperature – all before you even step out of bed. This level of automation and synchronization is the essence of IoT, simplifying and enhancing various aspects of our lives.\nImpact on Industries\n\n- 1. Retail: IoT has revolutionized the retail industry by enabling personalized shopping experiences. RFID tags, for instance, track inventory in real-time, alerting store managers when items are running low. Smart shelves interact with customers' smartphones, providing product information and promotions as they browse the aisles.\n\n- 2. Healthcare: The healthcare sector has embraced IoT to monitor patient health remotely and optimize hospital operations. Wearable devices, such as fitness trackers and continuous glucose monitors, collect data that can be shared with medical professionals for timely interventions. Additionally, IoT-enabled equipment in hospitals ensures efficient utilization and maintenance of critical resources.\n\n- 3. Manufacturing: IoT has ushered in the era of Industry 4.0, where factories are equipped with interconnected sensors, machines, and systems. This connectivity enables real-time monitoring of production lines, predictive maintenance, and even remote control of equipment, leading to increased productivity and reduced downtime.\nChallenges and Considerations\n\nWhile the potential of IoT is vast, it is not without its challenges:\n\n- 1. Security: With the increasing number of interconnected devices, the potential attack surface for cybercriminals widens. Ensuring the security and privacy of the data being exchanged is of paramount importance.\n\n- 2. Data Management: The sheer volume of data generated by IoT devices can be overwhelming. Effectively managing, storing, and analyzing this data requires robust infrastructure and advanced analytics capabilities.\n\n- 3. Interoperability: IoT devices often come from different manufacturers, leading to compatibility issues. Establishing common communication protocols and standards is essential to ensure seamless integration.\nThe Road Ahead\n\nAs IoT continues to evolve, we can expect to see even more innovative applications across various domains. Smart cities will leverage IoT to optimize traffic flow and resource management, while agriculture will benefit from precision farming techniques driven by sensor data. The integration of AI and machine learning will further enhance IoT's capabilities, enabling devices to make intelligent decisions based on the data they collect.\n\nIn conclusion, the Internet of Things stands as a testament to human ingenuity and our ability to connect and innovate. As we navigate the challenges and opportunities it presents, one thing remains clear: IoT is not just about connecting devices; it's about connecting lives and transforming the way we interact with the world around us. So, whether we're talking about smart homes, intelligent factories, or connected vehicles, the IoT landscape is one that promises to shape our future in ways we're only beginning to comprehend.\n\n#### Authors\n\n- [@filippo-ferrando](https://www.github.com/filippo-ferrando)","src/content/blog/23/index.md",[615],"./post_img23.webp","7c79644d453acd6d",{"html":618,"metadata":619},"\u003Ch1 id=\"the-evolution-of-iot-connecting-the-world-one-device-at-a-time\">The Evolution of IoT: Connecting the World One Device at a Time\u003C/h1>\n\u003Cp>In a world driven by technological advancements, the Internet of Things (IoT) has emerged as a transformative force, weaving the fabric of connectivity between our physical and digital realms. This network of interconnected devices, ranging from everyday objects to complex machinery, has opened the door to a new era of efficiency, convenience, and innovation.\nUnderstanding IoT\u003C/p>\n\u003Cp>At its core, the Internet of Things refers to the interconnection of devices via the internet, enabling them to collect and exchange data without direct human intervention. This data exchange can occur between devices of the same kind or between heterogeneous devices, creating a seamless ecosystem of information flow.\u003C/p>\n\u003Cp>Imagine waking up in the morning as your smart alarm clock synchronizes with your coffee machine to brew your favorite blend, while simultaneously adjusting your thermostat to your preferred temperature – all before you even step out of bed. This level of automation and synchronization is the essence of IoT, simplifying and enhancing various aspects of our lives.\nImpact on Industries\u003C/p>\n\u003Cul>\n\u003Cli>\n\u003Col>\n\u003Cli>Retail: IoT has revolutionized the retail industry by enabling personalized shopping experiences. RFID tags, for instance, track inventory in real-time, alerting store managers when items are running low. Smart shelves interact with customers’ smartphones, providing product information and promotions as they browse the aisles.\u003C/li>\n\u003C/ol>\n\u003C/li>\n\u003Cli>\n\u003Col start=\"2\">\n\u003Cli>Healthcare: The healthcare sector has embraced IoT to monitor patient health remotely and optimize hospital operations. Wearable devices, such as fitness trackers and continuous glucose monitors, collect data that can be shared with medical professionals for timely interventions. Additionally, IoT-enabled equipment in hospitals ensures efficient utilization and maintenance of critical resources.\u003C/li>\n\u003C/ol>\n\u003C/li>\n\u003Cli>\n\u003Col start=\"3\">\n\u003Cli>Manufacturing: IoT has ushered in the era of Industry 4.0, where factories are equipped with interconnected sensors, machines, and systems. This connectivity enables real-time monitoring of production lines, predictive maintenance, and even remote control of equipment, leading to increased productivity and reduced downtime.\nChallenges and Considerations\u003C/li>\n\u003C/ol>\n\u003C/li>\n\u003C/ul>\n\u003Cp>While the potential of IoT is vast, it is not without its challenges:\u003C/p>\n\u003Cul>\n\u003Cli>\n\u003Col>\n\u003Cli>Security: With the increasing number of interconnected devices, the potential attack surface for cybercriminals widens. Ensuring the security and privacy of the data being exchanged is of paramount importance.\u003C/li>\n\u003C/ol>\n\u003C/li>\n\u003Cli>\n\u003Col start=\"2\">\n\u003Cli>Data Management: The sheer volume of data generated by IoT devices can be overwhelming. Effectively managing, storing, and analyzing this data requires robust infrastructure and advanced analytics capabilities.\u003C/li>\n\u003C/ol>\n\u003C/li>\n\u003Cli>\n\u003Col start=\"3\">\n\u003Cli>Interoperability: IoT devices often come from different manufacturers, leading to compatibility issues. Establishing common communication protocols and standards is essential to ensure seamless integration.\nThe Road Ahead\u003C/li>\n\u003C/ol>\n\u003C/li>\n\u003C/ul>\n\u003Cp>As IoT continues to evolve, we can expect to see even more innovative applications across various domains. Smart cities will leverage IoT to optimize traffic flow and resource management, while agriculture will benefit from precision farming techniques driven by sensor data. The integration of AI and machine learning will further enhance IoT’s capabilities, enabling devices to make intelligent decisions based on the data they collect.\u003C/p>\n\u003Cp>In conclusion, the Internet of Things stands as a testament to human ingenuity and our ability to connect and innovate. As we navigate the challenges and opportunities it presents, one thing remains clear: IoT is not just about connecting devices; it’s about connecting lives and transforming the way we interact with the world around us. So, whether we’re talking about smart homes, intelligent factories, or connected vehicles, the IoT landscape is one that promises to shape our future in ways we’re only beginning to comprehend.\u003C/p>\n\u003Ch4 id=\"authors\">Authors\u003C/h4>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://www.github.com/filippo-ferrando\" target=\"_blank\" rel=\"noopener\">@filippo-ferrando\u003C/a>\u003C/li>\n\u003C/ul>",{"headings":620,"localImagePaths":625,"remoteImagePaths":626,"frontmatter":627,"imagePaths":630},[621,624],{"depth":55,"slug":622,"text":623},"the-evolution-of-iot-connecting-the-world-one-device-at-a-time","The Evolution of IoT: Connecting the World One Device at a Time",{"depth":94,"slug":95,"text":96},[],[],{"title":606,"slug":603,"description":607,"pubDate":628,"tags":629,"coverImage":615},"Jul 16 2023",[610,272],[],"23/index.md","switching-to-gnome",{"id":632,"data":634,"body":640,"filePath":641,"assetImports":642,"digest":644,"rendered":645,"legacyId":660},{"title":635,"description":636,"pubDate":637,"tags":638,"coverImage":639},"I switched to Gnome...  But Why?","Embracing Simplicity and Functionality",["Date","2023-08-14T00:00:00.000Z"],[43,360],"__ASTRO_IMAGE_./post_img24.jpg","# Choosing GNOME Over KDE as My Everyday Desktop Environment\n\nWhen it comes to selecting a desktop environment for our daily computing needs, the choices can be overwhelming. As a user who values a balance between simplicity and functionality, I found myself gravitating towards GNOME over KDE. In this blog post, I'll delve into the reasons behind my decision and why GNOME has become my go-to choice for an everyday desktop environment.\n\n- 1. Clean and Intuitive Design\n\nOne of the first things that drew me to GNOME is its clean and intuitive design. The user interface is uncluttered, with a focus on minimalism and ease of use. The activities overview and dynamic workspaces make multitasking a breeze, allowing me to seamlessly switch between applications and manage my workflow more efficiently.\n\n- 2. Streamlined User Experience\n\nGNOME's user experience feels polished and consistent throughout. From the top bar with its centralized activities menu to the application launcher, there's a sense of coherence that enhances my overall interaction with the system. This uniformity not only looks visually appealing but also contributes to a smoother and more seamless workflow.\n\n- 3. Thoughtful Default Applications\n\nThe default applications bundled with GNOME are well-chosen and fulfill everyday tasks effectively. Applications like Files (Nautilus), Terminal, and Web (Firefox) are simple yet robust, providing a solid foundation for productivity without overwhelming me with unnecessary features, although personally I chose Alacritty as terminal emulator because I find it faster, stable and visually better.\n\n- 4. Performance and Resource Efficiency\n\nGNOME's commitment to performance improvements in recent versions has significantly paid off. With each update, I've noticed smoother animations and quicker responsiveness, even on older hardware. This optimization is crucial for ensuring a pleasant user experience without straining system resources.\n\n- 5. Extensibility and Customization\n\nWhile GNOME's default setup is appealing, the system also offers a level of extensibility that I appreciate. GNOME Shell extensions allow me to tailor the desktop environment to my specific needs. From productivity enhancements to visual tweaks, these extensions enable me to mold GNOME into a personalized environment that suits my workflow.\n\n- 6. Community and Development\n\nBeing an open-source project, GNOME benefits from a thriving community and active development. Regular updates and improvements ensure that the desktop environment stays current and relevant. The GNOME project's commitment to accessibility and inclusivity is also a factor that resonates with my values.\n\n### Conclusion\n\nIn the end, the choice between GNOME and KDE is a matter of personal preference, and both desktop environments have their merits. For me, GNOME's clean design, streamlined user experience, performance optimization, and flexibility align perfectly with my needs and preferences. It strikes the right balance between functionality and simplicity, making it the ideal companion for my everyday computing tasks. While my choice may not be the same for everyone, I'm confident that the thoughtful design and continuous evolution of GNOME will continue to cater to users who share my desire for a polished and effective desktop environment.\n\n#### Authors\n\n- [@filippo-ferrando](https://www.github.com/filippo-ferrando)","src/content/blog/24/index.md",[643],"./post_img24.jpg","1e80702bbf50c693",{"html":646,"metadata":647},"\u003Ch1 id=\"choosing-gnome-over-kde-as-my-everyday-desktop-environment\">Choosing GNOME Over KDE as My Everyday Desktop Environment\u003C/h1>\n\u003Cp>When it comes to selecting a desktop environment for our daily computing needs, the choices can be overwhelming. As a user who values a balance between simplicity and functionality, I found myself gravitating towards GNOME over KDE. In this blog post, I’ll delve into the reasons behind my decision and why GNOME has become my go-to choice for an everyday desktop environment.\u003C/p>\n\u003Cul>\n\u003Cli>\n\u003Col>\n\u003Cli>Clean and Intuitive Design\u003C/li>\n\u003C/ol>\n\u003C/li>\n\u003C/ul>\n\u003Cp>One of the first things that drew me to GNOME is its clean and intuitive design. The user interface is uncluttered, with a focus on minimalism and ease of use. The activities overview and dynamic workspaces make multitasking a breeze, allowing me to seamlessly switch between applications and manage my workflow more efficiently.\u003C/p>\n\u003Cul>\n\u003Cli>\n\u003Col start=\"2\">\n\u003Cli>Streamlined User Experience\u003C/li>\n\u003C/ol>\n\u003C/li>\n\u003C/ul>\n\u003Cp>GNOME’s user experience feels polished and consistent throughout. From the top bar with its centralized activities menu to the application launcher, there’s a sense of coherence that enhances my overall interaction with the system. This uniformity not only looks visually appealing but also contributes to a smoother and more seamless workflow.\u003C/p>\n\u003Cul>\n\u003Cli>\n\u003Col start=\"3\">\n\u003Cli>Thoughtful Default Applications\u003C/li>\n\u003C/ol>\n\u003C/li>\n\u003C/ul>\n\u003Cp>The default applications bundled with GNOME are well-chosen and fulfill everyday tasks effectively. Applications like Files (Nautilus), Terminal, and Web (Firefox) are simple yet robust, providing a solid foundation for productivity without overwhelming me with unnecessary features, although personally I chose Alacritty as terminal emulator because I find it faster, stable and visually better.\u003C/p>\n\u003Cul>\n\u003Cli>\n\u003Col start=\"4\">\n\u003Cli>Performance and Resource Efficiency\u003C/li>\n\u003C/ol>\n\u003C/li>\n\u003C/ul>\n\u003Cp>GNOME’s commitment to performance improvements in recent versions has significantly paid off. With each update, I’ve noticed smoother animations and quicker responsiveness, even on older hardware. This optimization is crucial for ensuring a pleasant user experience without straining system resources.\u003C/p>\n\u003Cul>\n\u003Cli>\n\u003Col start=\"5\">\n\u003Cli>Extensibility and Customization\u003C/li>\n\u003C/ol>\n\u003C/li>\n\u003C/ul>\n\u003Cp>While GNOME’s default setup is appealing, the system also offers a level of extensibility that I appreciate. GNOME Shell extensions allow me to tailor the desktop environment to my specific needs. From productivity enhancements to visual tweaks, these extensions enable me to mold GNOME into a personalized environment that suits my workflow.\u003C/p>\n\u003Cul>\n\u003Cli>\n\u003Col start=\"6\">\n\u003Cli>Community and Development\u003C/li>\n\u003C/ol>\n\u003C/li>\n\u003C/ul>\n\u003Cp>Being an open-source project, GNOME benefits from a thriving community and active development. Regular updates and improvements ensure that the desktop environment stays current and relevant. The GNOME project’s commitment to accessibility and inclusivity is also a factor that resonates with my values.\u003C/p>\n\u003Ch3 id=\"conclusion\">Conclusion\u003C/h3>\n\u003Cp>In the end, the choice between GNOME and KDE is a matter of personal preference, and both desktop environments have their merits. For me, GNOME’s clean design, streamlined user experience, performance optimization, and flexibility align perfectly with my needs and preferences. It strikes the right balance between functionality and simplicity, making it the ideal companion for my everyday computing tasks. While my choice may not be the same for everyone, I’m confident that the thoughtful design and continuous evolution of GNOME will continue to cater to users who share my desire for a polished and effective desktop environment.\u003C/p>\n\u003Ch4 id=\"authors\">Authors\u003C/h4>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://www.github.com/filippo-ferrando\" target=\"_blank\" rel=\"noopener\">@filippo-ferrando\u003C/a>\u003C/li>\n\u003C/ul>",{"headings":648,"localImagePaths":654,"remoteImagePaths":655,"frontmatter":656,"imagePaths":659},[649,652,653],{"depth":55,"slug":650,"text":651},"choosing-gnome-over-kde-as-my-everyday-desktop-environment","Choosing GNOME Over KDE as My Everyday Desktop Environment",{"depth":63,"slug":91,"text":92},{"depth":94,"slug":95,"text":96},[],[],{"title":635,"slug":632,"description":636,"pubDate":657,"tags":658,"coverImage":643},"Aug 14 2023",[43,360],[],"24/index.md","the-snap-conundrum",{"id":661,"data":663,"body":670,"filePath":671,"assetImports":672,"digest":674,"rendered":675,"legacyId":697},{"title":664,"description":665,"pubDate":666,"tags":667,"coverImage":669},"The Snap Conundrum","Pros and Cons of the Canonical 'new apt'",["Date","2023-11-14T00:00:00.000Z"],[43,668],"PacketManager","__ASTRO_IMAGE_./post_img26.png","# Introduction\n\nIn the ever-evolving landscape of Linux package management, Snap packages have emerged as a controversial player. Developed by Canonical, the company behind Ubuntu, Snap packages aim to simplify software distribution across different Linux distributions. While they bring certain advantages to the table, there's an undercurrent of discontent among some users. In this blog post, we'll delve into the pros and cons of Snap packages, exploring whether they truly serve the best interests of the end user.\n\n## Pros of Snap Packages\n1. **Cross-Distribution Compatibility**\n\nSnap packages are designed to work across various Linux distributions, breaking down the traditional barriers associated with package management. This means developers can create a single Snap package that runs seamlessly on different distros, streamlining the software distribution process.\n\n2. **Automated Updates**\n\nOne of the touted benefits of Snap packages is their automatic update feature. Software updates are managed by the Snap daemon in the background, ensuring users always have the latest version without manual intervention. This can enhance security by promptly delivering patches and bug fixes.\n\n3. **Sandboxed Environment**\n\nSnap packages are built with a sandboxing mechanism that isolates the application from the rest of the system. This can enhance security and stability by preventing conflicts with system libraries and dependencies.\n\n## Cons of Snap Packages\n\n1. **Resource Overhead**\n\nCritics argue that Snap packages introduce a level of resource overhead compared to native packages. The sandboxing and bundling of dependencies can lead to larger file sizes and increased memory usage, potentially affecting system performance.\n\n2. **Snap Store Integration**\n\nSnap packages are tightly integrated with the Snap Store, Canonical's centralized repository. Some users express concerns about this centralized control, as it shifts power away from traditional package managers and raises questions about data privacy and security.\n\n3. **Fragmentation and Duplication**\n\nThe introduction of Snap packages adds another layer of complexity to the already fragmented Linux ecosystem. With Snap competing with other packaging formats like Flatpak, users may find themselves navigating a maze of different packaging systems, leading to confusion and duplication of efforts.\n\n# The Dark Side of Snap\n\nWhile Snap packages tout cross-distribution compatibility, automated updates, and sandboxed environments, a closer inspection reveals a set of concerns that extend beyond the surface. Critics argue that Snap introduces a level of centralization and potential security risks that warrant careful consideration.\nCentralization Woes\n\nThe heart of the discontent lies in the Snap Store, Canonical's centralized repository for Snap packages. The integration of Snap packages with the Snap Store has raised eyebrows, as it marks a departure from the decentralized ethos that has long been a cornerstone of the Linux community. Some users express apprehension about relying on a single, controlled entity for software distribution, fearing that it could concentrate too much power in the hands of a single company.\n\nThis shift towards centralization can potentially stifle the collaborative spirit of open-source development. Traditional package managers, which allow for community-driven curation and distribution, might face overshadowing by the proprietary control exerted by a centralized Snap Store.\nSecurity Implications\n\nSecurity is a paramount concern in the Linux ecosystem, and Snap packages attempt to address this by employing sandboxing to isolate applications from the system. However, the Snap Store itself has not been immune to security issues. In the past, security vulnerabilities in the Snap Store have been discovered, raising questions about the overall robustness of the Snap ecosystem.\n\nThe centralized nature of the Snap Store also raises concerns about data privacy. Users may be uneasy about the amount of data collected and shared during interactions with the Snap Store, especially considering Canonical's influence over the ecosystem.\nSnap Store Trustworthiness\n\nThe question of trustworthiness is crucial when considering any centralized software repository. Users must place a significant degree of trust in the Snap Store to deliver secure, reliable, and up-to-date software. Any lapses in the curation and security processes of the Snap Store can have far-reaching implications for the Linux community.\n\nThe Snap Store's control over the distribution and updating process can also lead to potential issues. If the Snap Store were to experience downtime or disruptions, users might find themselves unable to install or update software, highlighting the vulnerability introduced by relying on a centralized repository.\n\n## Conclusion\n\nIn the ever-expanding universe of Linux package management, Snap packages offer a mixed bag of advantages and drawbacks. Users must weigh the convenience of a universal packaging format against the potential compromises in performance, control, and the ethos of the open-source community. The discontent surrounding Snap's centralization and security concerns prompts users to make informed decisions about the software distribution model they choose to embrace. As the Linux ecosystem continues to evolve, the fate of Snap packages remains uncertain, leaving users to decide whether they embrace this technology or stick with more traditional package management systems.\n\n#### Authors\n\n- [@filippo-ferrando](https://www.github.com/filippo-ferrando)","src/content/blog/26/index.md",[673],"./post_img26.png","a88a7a421d37a75d",{"html":676,"metadata":677},"\u003Ch1 id=\"introduction\">Introduction\u003C/h1>\n\u003Cp>In the ever-evolving landscape of Linux package management, Snap packages have emerged as a controversial player. Developed by Canonical, the company behind Ubuntu, Snap packages aim to simplify software distribution across different Linux distributions. While they bring certain advantages to the table, there’s an undercurrent of discontent among some users. In this blog post, we’ll delve into the pros and cons of Snap packages, exploring whether they truly serve the best interests of the end user.\u003C/p>\n\u003Ch2 id=\"pros-of-snap-packages\">Pros of Snap Packages\u003C/h2>\n\u003Col>\n\u003Cli>\u003Cstrong>Cross-Distribution Compatibility\u003C/strong>\u003C/li>\n\u003C/ol>\n\u003Cp>Snap packages are designed to work across various Linux distributions, breaking down the traditional barriers associated with package management. This means developers can create a single Snap package that runs seamlessly on different distros, streamlining the software distribution process.\u003C/p>\n\u003Col start=\"2\">\n\u003Cli>\u003Cstrong>Automated Updates\u003C/strong>\u003C/li>\n\u003C/ol>\n\u003Cp>One of the touted benefits of Snap packages is their automatic update feature. Software updates are managed by the Snap daemon in the background, ensuring users always have the latest version without manual intervention. This can enhance security by promptly delivering patches and bug fixes.\u003C/p>\n\u003Col start=\"3\">\n\u003Cli>\u003Cstrong>Sandboxed Environment\u003C/strong>\u003C/li>\n\u003C/ol>\n\u003Cp>Snap packages are built with a sandboxing mechanism that isolates the application from the rest of the system. This can enhance security and stability by preventing conflicts with system libraries and dependencies.\u003C/p>\n\u003Ch2 id=\"cons-of-snap-packages\">Cons of Snap Packages\u003C/h2>\n\u003Col>\n\u003Cli>\u003Cstrong>Resource Overhead\u003C/strong>\u003C/li>\n\u003C/ol>\n\u003Cp>Critics argue that Snap packages introduce a level of resource overhead compared to native packages. The sandboxing and bundling of dependencies can lead to larger file sizes and increased memory usage, potentially affecting system performance.\u003C/p>\n\u003Col start=\"2\">\n\u003Cli>\u003Cstrong>Snap Store Integration\u003C/strong>\u003C/li>\n\u003C/ol>\n\u003Cp>Snap packages are tightly integrated with the Snap Store, Canonical’s centralized repository. Some users express concerns about this centralized control, as it shifts power away from traditional package managers and raises questions about data privacy and security.\u003C/p>\n\u003Col start=\"3\">\n\u003Cli>\u003Cstrong>Fragmentation and Duplication\u003C/strong>\u003C/li>\n\u003C/ol>\n\u003Cp>The introduction of Snap packages adds another layer of complexity to the already fragmented Linux ecosystem. With Snap competing with other packaging formats like Flatpak, users may find themselves navigating a maze of different packaging systems, leading to confusion and duplication of efforts.\u003C/p>\n\u003Ch1 id=\"the-dark-side-of-snap\">The Dark Side of Snap\u003C/h1>\n\u003Cp>While Snap packages tout cross-distribution compatibility, automated updates, and sandboxed environments, a closer inspection reveals a set of concerns that extend beyond the surface. Critics argue that Snap introduces a level of centralization and potential security risks that warrant careful consideration.\nCentralization Woes\u003C/p>\n\u003Cp>The heart of the discontent lies in the Snap Store, Canonical’s centralized repository for Snap packages. The integration of Snap packages with the Snap Store has raised eyebrows, as it marks a departure from the decentralized ethos that has long been a cornerstone of the Linux community. Some users express apprehension about relying on a single, controlled entity for software distribution, fearing that it could concentrate too much power in the hands of a single company.\u003C/p>\n\u003Cp>This shift towards centralization can potentially stifle the collaborative spirit of open-source development. Traditional package managers, which allow for community-driven curation and distribution, might face overshadowing by the proprietary control exerted by a centralized Snap Store.\nSecurity Implications\u003C/p>\n\u003Cp>Security is a paramount concern in the Linux ecosystem, and Snap packages attempt to address this by employing sandboxing to isolate applications from the system. However, the Snap Store itself has not been immune to security issues. In the past, security vulnerabilities in the Snap Store have been discovered, raising questions about the overall robustness of the Snap ecosystem.\u003C/p>\n\u003Cp>The centralized nature of the Snap Store also raises concerns about data privacy. Users may be uneasy about the amount of data collected and shared during interactions with the Snap Store, especially considering Canonical’s influence over the ecosystem.\nSnap Store Trustworthiness\u003C/p>\n\u003Cp>The question of trustworthiness is crucial when considering any centralized software repository. Users must place a significant degree of trust in the Snap Store to deliver secure, reliable, and up-to-date software. Any lapses in the curation and security processes of the Snap Store can have far-reaching implications for the Linux community.\u003C/p>\n\u003Cp>The Snap Store’s control over the distribution and updating process can also lead to potential issues. If the Snap Store were to experience downtime or disruptions, users might find themselves unable to install or update software, highlighting the vulnerability introduced by relying on a centralized repository.\u003C/p>\n\u003Ch2 id=\"conclusion\">Conclusion\u003C/h2>\n\u003Cp>In the ever-expanding universe of Linux package management, Snap packages offer a mixed bag of advantages and drawbacks. Users must weigh the convenience of a universal packaging format against the potential compromises in performance, control, and the ethos of the open-source community. The discontent surrounding Snap’s centralization and security concerns prompts users to make informed decisions about the software distribution model they choose to embrace. As the Linux ecosystem continues to evolve, the fate of Snap packages remains uncertain, leaving users to decide whether they embrace this technology or stick with more traditional package management systems.\u003C/p>\n\u003Ch4 id=\"authors\">Authors\u003C/h4>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://www.github.com/filippo-ferrando\" target=\"_blank\" rel=\"noopener\">@filippo-ferrando\u003C/a>\u003C/li>\n\u003C/ul>",{"headings":678,"localImagePaths":691,"remoteImagePaths":692,"frontmatter":693,"imagePaths":696},[679,680,683,686,689,690],{"depth":55,"slug":498,"text":499},{"depth":59,"slug":681,"text":682},"pros-of-snap-packages","Pros of Snap Packages",{"depth":59,"slug":684,"text":685},"cons-of-snap-packages","Cons of Snap Packages",{"depth":55,"slug":687,"text":688},"the-dark-side-of-snap","The Dark Side of Snap",{"depth":59,"slug":91,"text":92},{"depth":94,"slug":95,"text":96},[],[],{"title":664,"slug":661,"description":665,"pubDate":694,"tags":695,"coverImage":673},"Nov 14 2023",[43,668],[],"26/index.md","tor-hidden-services",{"id":698,"data":700,"body":707,"filePath":708,"assetImports":709,"digest":711,"rendered":712,"legacyId":736},{"title":701,"description":702,"pubDate":703,"tags":704,"coverImage":706},"TOR Hidden Services","What is but mostly for what i can use it?",["Date","2023-11-22T00:00:00.000Z"],[272,705],"TOR","__ASTRO_IMAGE_./post_img27.webp","# Unveiling the Depths of Tor Hidden Services: A Comprehensive Overview\n\nIn the dynamic landscape of online anonymity and privacy, Tor Hidden Services, commonly known as .onion sites, emerge as a fascinating facet. These services offer a clandestine network within the Tor ecosystem, allowing the hosting of websites and services while ensuring the anonymity of both users and providers. Delve deeper with us as we explore their diverse use cases, advantages, and potential drawbacks.\n\n## Unraveling Use Cases\n\n1. **Anonymity and Privacy**\n\nAt the core of Tor Hidden Services lies the promise of safeguarding identities. By leveraging the Tor network, users can access .onion websites without revealing their IP addresses, thereby shielding themselves from surveillance or tracking attempts.\n\n2. **Censorship Resistance**\n\nIn regions where internet censorship prevails, Tor Hidden Services become a beacon of freedom. These sites evade traditional search engine indexing, making them harder to block. This resilience empowers individuals to access information and express themselves freely, circumventing oppressive regimes' censorship efforts.\n\n3. **Whistleblowing and Secure Communications**\n\nThe realm of Tor Hidden Services hosts platforms like SecureDrop, championing secure and anonymous submissions of sensitive information. Journalists and sources can interact without fear of retribution, ensuring the safe dissemination of crucial data.\n\n4. **Confidential Interactions**\n\nWithin the secure enclave of the Tor network, private forums, encrypted chat services, and underground marketplaces flourish. Here, users engage in confidential conversations or transactions, shielded by layers of encryption and anonymity.\n\n### Embracing Advantages\n\n1. **Anonymity Fortification**\n\nThe robust encryption and intricate routing of Tor Hidden Services serve as a bulwark, safeguarding the identities of both service providers and users. This anonymity forms the cornerstone of the network’s appeal.\n\n2. **Enhanced Security Measures**\n\nEnd-to-end encryption and an architecture designed to thwart traffic analysis bolster the security of .onion services This fortified security infrastructure offers a heightened level of protection compared to the conventional internet.\n\n3. **Accessibility and Liberated Expression**\n\nBy sidestepping censorship barriers, Tor Hidden Services foster an environment conducive to free expression and information sharing. Access to restricted content amplifies the voices of those marginalized by restrictive internet policies.\n\n### Delving into Disadvantages\n\n1. **Lagging Connection Speeds**\n\nThe intricate encryption and routing mechanisms inherent in Tor Hidden Services often translate to slower connection speeds. This trade-off between security and speed remains a notable drawback for users seeking seamless browsing experiences.\n\n2. **Potential for Unlawful Activities**\n\nWhile anonymity serves as a shield, it also creates a refuge for illicit activities. Law enforcement faces challenges in monitoring and regulating such activities within the depths of the .onion network.\n\n3. **Reliability and Trust Issues**\n\nThe decentralized nature of Tor Hidden Services leads to a varied landscape of trustworthiness. Users encounter a spectrum of sites, ranging from authentic and legitimate services to fraudulent or malicious entities, necessitating cautious navigation.\n\n## Conclusion\n\nTor Hidden Services remain a testament to the ongoing quest for online privacy and liberation. Their significance transcends mere technological innovation, embodying the principles of anonymity, freedom of expression, and resistance against surveillance.\n\nHowever, their full potential can only be harnessed through a collective commitment to responsible usage. As we navigate this complex terrain, let us strive for a harmonious balance a digital realm where anonymity coexists with accountability, and freedom flourishes within ethical boundaries.\n\n#### Authors\n\n- [@filippo-ferrando](https://www.github.com/filippo-ferrando)","src/content/blog/27/index.md",[710],"./post_img27.webp","07e4ba1c615d8c5a",{"html":713,"metadata":714},"\u003Ch1 id=\"unveiling-the-depths-of-tor-hidden-services-a-comprehensive-overview\">Unveiling the Depths of Tor Hidden Services: A Comprehensive Overview\u003C/h1>\n\u003Cp>In the dynamic landscape of online anonymity and privacy, Tor Hidden Services, commonly known as .onion sites, emerge as a fascinating facet. These services offer a clandestine network within the Tor ecosystem, allowing the hosting of websites and services while ensuring the anonymity of both users and providers. Delve deeper with us as we explore their diverse use cases, advantages, and potential drawbacks.\u003C/p>\n\u003Ch2 id=\"unraveling-use-cases\">Unraveling Use Cases\u003C/h2>\n\u003Col>\n\u003Cli>\u003Cstrong>Anonymity and Privacy\u003C/strong>\u003C/li>\n\u003C/ol>\n\u003Cp>At the core of Tor Hidden Services lies the promise of safeguarding identities. By leveraging the Tor network, users can access .onion websites without revealing their IP addresses, thereby shielding themselves from surveillance or tracking attempts.\u003C/p>\n\u003Col start=\"2\">\n\u003Cli>\u003Cstrong>Censorship Resistance\u003C/strong>\u003C/li>\n\u003C/ol>\n\u003Cp>In regions where internet censorship prevails, Tor Hidden Services become a beacon of freedom. These sites evade traditional search engine indexing, making them harder to block. This resilience empowers individuals to access information and express themselves freely, circumventing oppressive regimes’ censorship efforts.\u003C/p>\n\u003Col start=\"3\">\n\u003Cli>\u003Cstrong>Whistleblowing and Secure Communications\u003C/strong>\u003C/li>\n\u003C/ol>\n\u003Cp>The realm of Tor Hidden Services hosts platforms like SecureDrop, championing secure and anonymous submissions of sensitive information. Journalists and sources can interact without fear of retribution, ensuring the safe dissemination of crucial data.\u003C/p>\n\u003Col start=\"4\">\n\u003Cli>\u003Cstrong>Confidential Interactions\u003C/strong>\u003C/li>\n\u003C/ol>\n\u003Cp>Within the secure enclave of the Tor network, private forums, encrypted chat services, and underground marketplaces flourish. Here, users engage in confidential conversations or transactions, shielded by layers of encryption and anonymity.\u003C/p>\n\u003Ch3 id=\"embracing-advantages\">Embracing Advantages\u003C/h3>\n\u003Col>\n\u003Cli>\u003Cstrong>Anonymity Fortification\u003C/strong>\u003C/li>\n\u003C/ol>\n\u003Cp>The robust encryption and intricate routing of Tor Hidden Services serve as a bulwark, safeguarding the identities of both service providers and users. This anonymity forms the cornerstone of the network’s appeal.\u003C/p>\n\u003Col start=\"2\">\n\u003Cli>\u003Cstrong>Enhanced Security Measures\u003C/strong>\u003C/li>\n\u003C/ol>\n\u003Cp>End-to-end encryption and an architecture designed to thwart traffic analysis bolster the security of .onion services This fortified security infrastructure offers a heightened level of protection compared to the conventional internet.\u003C/p>\n\u003Col start=\"3\">\n\u003Cli>\u003Cstrong>Accessibility and Liberated Expression\u003C/strong>\u003C/li>\n\u003C/ol>\n\u003Cp>By sidestepping censorship barriers, Tor Hidden Services foster an environment conducive to free expression and information sharing. Access to restricted content amplifies the voices of those marginalized by restrictive internet policies.\u003C/p>\n\u003Ch3 id=\"delving-into-disadvantages\">Delving into Disadvantages\u003C/h3>\n\u003Col>\n\u003Cli>\u003Cstrong>Lagging Connection Speeds\u003C/strong>\u003C/li>\n\u003C/ol>\n\u003Cp>The intricate encryption and routing mechanisms inherent in Tor Hidden Services often translate to slower connection speeds. This trade-off between security and speed remains a notable drawback for users seeking seamless browsing experiences.\u003C/p>\n\u003Col start=\"2\">\n\u003Cli>\u003Cstrong>Potential for Unlawful Activities\u003C/strong>\u003C/li>\n\u003C/ol>\n\u003Cp>While anonymity serves as a shield, it also creates a refuge for illicit activities. Law enforcement faces challenges in monitoring and regulating such activities within the depths of the .onion network.\u003C/p>\n\u003Col start=\"3\">\n\u003Cli>\u003Cstrong>Reliability and Trust Issues\u003C/strong>\u003C/li>\n\u003C/ol>\n\u003Cp>The decentralized nature of Tor Hidden Services leads to a varied landscape of trustworthiness. Users encounter a spectrum of sites, ranging from authentic and legitimate services to fraudulent or malicious entities, necessitating cautious navigation.\u003C/p>\n\u003Ch2 id=\"conclusion\">Conclusion\u003C/h2>\n\u003Cp>Tor Hidden Services remain a testament to the ongoing quest for online privacy and liberation. Their significance transcends mere technological innovation, embodying the principles of anonymity, freedom of expression, and resistance against surveillance.\u003C/p>\n\u003Cp>However, their full potential can only be harnessed through a collective commitment to responsible usage. As we navigate this complex terrain, let us strive for a harmonious balance a digital realm where anonymity coexists with accountability, and freedom flourishes within ethical boundaries.\u003C/p>\n\u003Ch4 id=\"authors\">Authors\u003C/h4>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://www.github.com/filippo-ferrando\" target=\"_blank\" rel=\"noopener\">@filippo-ferrando\u003C/a>\u003C/li>\n\u003C/ul>",{"headings":715,"localImagePaths":730,"remoteImagePaths":731,"frontmatter":732,"imagePaths":735},[716,719,722,725,728,729],{"depth":55,"slug":717,"text":718},"unveiling-the-depths-of-tor-hidden-services-a-comprehensive-overview","Unveiling the Depths of Tor Hidden Services: A Comprehensive Overview",{"depth":59,"slug":720,"text":721},"unraveling-use-cases","Unraveling Use Cases",{"depth":63,"slug":723,"text":724},"embracing-advantages","Embracing Advantages",{"depth":63,"slug":726,"text":727},"delving-into-disadvantages","Delving into Disadvantages",{"depth":59,"slug":91,"text":92},{"depth":94,"slug":95,"text":96},[],[],{"title":701,"slug":698,"description":702,"pubDate":733,"tags":734,"coverImage":710},"Nov 22 2023",[272,705],[],"27/index.md","unlocking-the-potential-of-automation",{"id":737,"data":739,"body":746,"filePath":747,"assetImports":748,"digest":750,"rendered":751,"legacyId":779},{"title":740,"description":741,"pubDate":742,"tags":743,"coverImage":745},"Unlocking the Potential of Automation","Embracing Ansible for Advanced System Management",["Date","2024-03-25T00:00:00.000Z"],[744,43],"Ansible","__ASTRO_IMAGE_./post_img28.png","In today's ever-evolving technological landscape, the role of system managers has transcended mere maintenance; they are now pivotal in driving innovation, ensuring efficiency, and fortifying security within organizations' IT infrastructures. As complexities mount and demands for automation intensify, the traditional manual approaches to system management are proving insufficient. Enter Ansible - an open-source automation tool that has revolutionized system management with its simplicity, flexibility, and scalability. In this discourse, we delve deeper into the imperative for system managers to not only learn Ansible but to seamlessly integrate it into the fabric of societal infrastructure.\n\n## The Automation Imperative\n\nAt the heart of Ansible lies its automation prowess, enabling system managers to alleviate the burden of repetitive tasks such as configuration management, software deployment, and system updates. By harnessing Ansible's intuitive declarative language and lightweight agentless architecture, system managers can orchestrate complex workflows with ease, minimizing human error and unlocking significant time savings. The automation dividend extends beyond operational efficiency, empowering system managers to reallocate resources towards strategic initiatives that drive business value.\n\n## Embracing Infrastructure as Code (IaC)\n\nAnsible serves as a catalyst for embracing Infrastructure as Code (IaC) principles, allowing system managers to codify infrastructure configurations, version control them, and facilitate reproducibility across environments. With Ansible playbooks, infrastructure becomes malleable code, fostering consistency, collaboration, and agility. The transition to IaC not only streamlines infrastructure provisioning and management but also fosters a culture of continuous improvement and innovation, wherein system managers can iterate upon infrastructure configurations with confidence, knowing they can roll back changes if necessary.\n\n## Scalability and Adaptability\n\nIn an era characterized by rapid scalability and heterogeneous environments, Ansible stands as a beacon of adaptability. Whether orchestrating deployments across a sprawling cloud infrastructure or managing disparate on-premises systems, Ansible's scalability ensures that system managers can meet the demands of evolving infrastructures with ease. Moreover, Ansible's broad platform support and extensibility facilitate seamless integration with existing technologies and third-party tools, ensuring compatibility and future-proofing infrastructure investments.\n\n## Fortifying Security Posture\n\nIn an age where cybersecurity threats loom large, fortifying the security posture of IT infrastructure is paramount. Ansible empowers system managers to enforce security best practices by automating the implementation of consistent configurations, timely application of security patches, and systematic enforcement of access controls. Furthermore, Ansible's auditability features enable organizations to maintain compliance with regulatory standards and internal policies, providing assurance in an increasingly regulated landscape.\n\n## Harnessing the Power of Community and Ecosystem\n\nBeyond its technical prowess, Ansible thrives on the strength of its vibrant community and expansive ecosystem. System managers can tap into a treasure trove of modules, plugins, and playbooks contributed by the Ansible community, accelerating automation efforts and addressing diverse use cases effectively. Furthermore, Ansible's extensibility allows for seamless integration with a myriad of tools and systems, enabling comprehensive orchestration and automation across the technology stack. By leveraging the collective wisdom and innovation of the Ansible community, system managers can stay ahead of the curve and continuously enhance their infrastructure management capabilities.\n\n## Conclusion: Empowering System Managers for the Digital Era\n\nIn conclusion, the imperative for system managers to embrace Ansible transcends mere operational efficiency; it represents a strategic imperative for organizations seeking to thrive in the digital age. By harnessing the automation prowess of Ansible, embracing Infrastructure as Code principles, leveraging its scalability and adaptability, fortifying security postures, and tapping into the power of community and ecosystem, system managers can elevate their capabilities and drive tangible business outcomes. As Ansible continues to evolve and innovate, the integration of Ansible into societal infrastructure is not merely a choice but a necessity for organizations looking to unlock their full potential in an increasingly competitive landscape.","src/content/blog/28/index.md",[749],"./post_img28.png","4b7d65649a30aa87",{"html":752,"metadata":753},"\u003Cp>In today’s ever-evolving technological landscape, the role of system managers has transcended mere maintenance; they are now pivotal in driving innovation, ensuring efficiency, and fortifying security within organizations’ IT infrastructures. As complexities mount and demands for automation intensify, the traditional manual approaches to system management are proving insufficient. Enter Ansible - an open-source automation tool that has revolutionized system management with its simplicity, flexibility, and scalability. In this discourse, we delve deeper into the imperative for system managers to not only learn Ansible but to seamlessly integrate it into the fabric of societal infrastructure.\u003C/p>\n\u003Ch2 id=\"the-automation-imperative\">The Automation Imperative\u003C/h2>\n\u003Cp>At the heart of Ansible lies its automation prowess, enabling system managers to alleviate the burden of repetitive tasks such as configuration management, software deployment, and system updates. By harnessing Ansible’s intuitive declarative language and lightweight agentless architecture, system managers can orchestrate complex workflows with ease, minimizing human error and unlocking significant time savings. The automation dividend extends beyond operational efficiency, empowering system managers to reallocate resources towards strategic initiatives that drive business value.\u003C/p>\n\u003Ch2 id=\"embracing-infrastructure-as-code-iac\">Embracing Infrastructure as Code (IaC)\u003C/h2>\n\u003Cp>Ansible serves as a catalyst for embracing Infrastructure as Code (IaC) principles, allowing system managers to codify infrastructure configurations, version control them, and facilitate reproducibility across environments. With Ansible playbooks, infrastructure becomes malleable code, fostering consistency, collaboration, and agility. The transition to IaC not only streamlines infrastructure provisioning and management but also fosters a culture of continuous improvement and innovation, wherein system managers can iterate upon infrastructure configurations with confidence, knowing they can roll back changes if necessary.\u003C/p>\n\u003Ch2 id=\"scalability-and-adaptability\">Scalability and Adaptability\u003C/h2>\n\u003Cp>In an era characterized by rapid scalability and heterogeneous environments, Ansible stands as a beacon of adaptability. Whether orchestrating deployments across a sprawling cloud infrastructure or managing disparate on-premises systems, Ansible’s scalability ensures that system managers can meet the demands of evolving infrastructures with ease. Moreover, Ansible’s broad platform support and extensibility facilitate seamless integration with existing technologies and third-party tools, ensuring compatibility and future-proofing infrastructure investments.\u003C/p>\n\u003Ch2 id=\"fortifying-security-posture\">Fortifying Security Posture\u003C/h2>\n\u003Cp>In an age where cybersecurity threats loom large, fortifying the security posture of IT infrastructure is paramount. Ansible empowers system managers to enforce security best practices by automating the implementation of consistent configurations, timely application of security patches, and systematic enforcement of access controls. Furthermore, Ansible’s auditability features enable organizations to maintain compliance with regulatory standards and internal policies, providing assurance in an increasingly regulated landscape.\u003C/p>\n\u003Ch2 id=\"harnessing-the-power-of-community-and-ecosystem\">Harnessing the Power of Community and Ecosystem\u003C/h2>\n\u003Cp>Beyond its technical prowess, Ansible thrives on the strength of its vibrant community and expansive ecosystem. System managers can tap into a treasure trove of modules, plugins, and playbooks contributed by the Ansible community, accelerating automation efforts and addressing diverse use cases effectively. Furthermore, Ansible’s extensibility allows for seamless integration with a myriad of tools and systems, enabling comprehensive orchestration and automation across the technology stack. By leveraging the collective wisdom and innovation of the Ansible community, system managers can stay ahead of the curve and continuously enhance their infrastructure management capabilities.\u003C/p>\n\u003Ch2 id=\"conclusion-empowering-system-managers-for-the-digital-era\">Conclusion: Empowering System Managers for the Digital Era\u003C/h2>\n\u003Cp>In conclusion, the imperative for system managers to embrace Ansible transcends mere operational efficiency; it represents a strategic imperative for organizations seeking to thrive in the digital age. By harnessing the automation prowess of Ansible, embracing Infrastructure as Code principles, leveraging its scalability and adaptability, fortifying security postures, and tapping into the power of community and ecosystem, system managers can elevate their capabilities and drive tangible business outcomes. As Ansible continues to evolve and innovate, the integration of Ansible into societal infrastructure is not merely a choice but a necessity for organizations looking to unlock their full potential in an increasingly competitive landscape.\u003C/p>",{"headings":754,"localImagePaths":773,"remoteImagePaths":774,"frontmatter":775,"imagePaths":778},[755,758,761,764,767,770],{"depth":59,"slug":756,"text":757},"the-automation-imperative","The Automation Imperative",{"depth":59,"slug":759,"text":760},"embracing-infrastructure-as-code-iac","Embracing Infrastructure as Code (IaC)",{"depth":59,"slug":762,"text":763},"scalability-and-adaptability","Scalability and Adaptability",{"depth":59,"slug":765,"text":766},"fortifying-security-posture","Fortifying Security Posture",{"depth":59,"slug":768,"text":769},"harnessing-the-power-of-community-and-ecosystem","Harnessing the Power of Community and Ecosystem",{"depth":59,"slug":771,"text":772},"conclusion-empowering-system-managers-for-the-digital-era","Conclusion: Empowering System Managers for the Digital Era",[],[],{"title":740,"slug":737,"description":741,"pubDate":776,"tags":777,"coverImage":749},"Mar 25 2024",[744,43],[],"28/index.md","safeguarding-assets-in-interconnected-world",{"id":780,"data":782,"body":788,"filePath":789,"assetImports":790,"digest":792,"rendered":793,"legacyId":813},{"title":783,"description":784,"pubDate":785,"tags":786,"coverImage":787},"Safeguarding Digital assets in an Interconnected World","Brief talk about cybersecurity in our society",["Date","2024-07-17T00:00:00.000Z"],[439],"__ASTRO_IMAGE_./post_img29.jpg","In today's digital age, where businesses and individuals alike rely heavily on interconnected systems and networks, safeguarding digital assets has become more critical than ever. From sensitive financial data to personal information, digital assets are constantly at risk of being compromised by cyber threats. In this blog post, we'll explore the importance of safeguarding digital assets and discuss strategies for enhancing cybersecurity in an interconnected world.\n\n## Understanding the Risks\n\nCyber threats come in various forms, including malware, phishing attacks, ransomware, and DDoS (Distributed Denial of Service) attacks. These threats can originate from malicious actors seeking financial gain, political motives, or simply causing disruption. With the proliferation of interconnected devices and systems, the attack surface has expanded, making it easier for cybercriminals to exploit vulnerabilities and gain unauthorized access to digital assets.\n\nOne of the significant risks associated with interconnected systems is the potential for a single security breach to have cascading effects across multiple systems and networks. For example, a breach in one system could lead to the compromise of user credentials, which could then be used to gain access to other systems or launch targeted attacks against individuals or organizations.\n\n## Importance of Cybersecurity Measures\n\nImplementing robust cybersecurity measures is essential for protecting digital assets from unauthorized access, data breaches, and other cyber threats. This includes adopting encryption techniques to secure data in transit and at rest, implementing multi-factor authentication to verify user identities, and regularly updating software and systems to patch known vulnerabilities.\n\nMoreover, with the rise of remote work and cloud-based services, the perimeter-based approach to cybersecurity is no longer sufficient. Organizations must adopt a more holistic approach to cybersecurity that encompasses endpoint security, network security, application security, and data security to effectively protect digital assets from modern cyber threats.\n\n## Strategies for Safeguarding Digital Assets\n\n1. **Risk Assessment**: Conducting regular risk assessments to identify potential vulnerabilities and threats to digital assets. This involves analyzing the security posture of systems and networks, identifying potential weaknesses, and prioritizing mitigation efforts.\n\n2. **Access Control**: Implementing strict access controls to limit access to sensitive data and systems only to authorized users. This includes enforcing strong password policies, implementing role-based access controls, and regularly reviewing user permissions.\n\n3. **Employee Training**: Providing comprehensive cybersecurity training to employees to raise awareness of common cyber threats and best practices for safeguarding digital assets. This includes training on identifying phishing emails, practicing good password hygiene, and following security protocols when accessing company resources remotely.\n\n4. **Incident Response Planning**: Developing a robust incident response plan to effectively respond to security incidents and minimize the impact on digital assets. This involves establishing clear roles and responsibilities, defining communication protocols, and conducting regular incident response drills to ensure readiness.\n\n5. **Continuous Monitoring**: Implementing continuous monitoring solutions to detect and respond to security threats in real-time. This includes deploying intrusion detection systems (IDS), security information and event management (SIEM) solutions, and endpoint detection and response (EDR) tools to monitor for suspicious activity and anomalous behavior.\n\n## Conclusion\n\nSafeguarding digital assets in an interconnected world requires a proactive approach to cybersecurity, encompassing risk assessment, access control, employee training, incident response planning, and continuous monitoring. By implementing robust cybersecurity measures and staying vigilant against emerging threats, organizations can effectively protect their digital assets and maintain the trust of their customers and stakeholders in an increasingly interconnected world.\n\nRemember, cybersecurity is not a one-time effort but an ongoing commitment to safeguarding digital assets against evolving cyber threats. By prioritizing cybersecurity and investing in the right tools and technologies, organizations can mitigate risks, strengthen their security posture, and ensure the integrity, confidentiality, and availability of their digital assets in an interconnected world.","src/content/blog/29/index.md",[791],"./post_img29.jpg","3aec59296c8fd6e7",{"html":794,"metadata":795},"\u003Cp>In today’s digital age, where businesses and individuals alike rely heavily on interconnected systems and networks, safeguarding digital assets has become more critical than ever. From sensitive financial data to personal information, digital assets are constantly at risk of being compromised by cyber threats. In this blog post, we’ll explore the importance of safeguarding digital assets and discuss strategies for enhancing cybersecurity in an interconnected world.\u003C/p>\n\u003Ch2 id=\"understanding-the-risks\">Understanding the Risks\u003C/h2>\n\u003Cp>Cyber threats come in various forms, including malware, phishing attacks, ransomware, and DDoS (Distributed Denial of Service) attacks. These threats can originate from malicious actors seeking financial gain, political motives, or simply causing disruption. With the proliferation of interconnected devices and systems, the attack surface has expanded, making it easier for cybercriminals to exploit vulnerabilities and gain unauthorized access to digital assets.\u003C/p>\n\u003Cp>One of the significant risks associated with interconnected systems is the potential for a single security breach to have cascading effects across multiple systems and networks. For example, a breach in one system could lead to the compromise of user credentials, which could then be used to gain access to other systems or launch targeted attacks against individuals or organizations.\u003C/p>\n\u003Ch2 id=\"importance-of-cybersecurity-measures\">Importance of Cybersecurity Measures\u003C/h2>\n\u003Cp>Implementing robust cybersecurity measures is essential for protecting digital assets from unauthorized access, data breaches, and other cyber threats. This includes adopting encryption techniques to secure data in transit and at rest, implementing multi-factor authentication to verify user identities, and regularly updating software and systems to patch known vulnerabilities.\u003C/p>\n\u003Cp>Moreover, with the rise of remote work and cloud-based services, the perimeter-based approach to cybersecurity is no longer sufficient. Organizations must adopt a more holistic approach to cybersecurity that encompasses endpoint security, network security, application security, and data security to effectively protect digital assets from modern cyber threats.\u003C/p>\n\u003Ch2 id=\"strategies-for-safeguarding-digital-assets\">Strategies for Safeguarding Digital Assets\u003C/h2>\n\u003Col>\n\u003Cli>\n\u003Cp>\u003Cstrong>Risk Assessment\u003C/strong>: Conducting regular risk assessments to identify potential vulnerabilities and threats to digital assets. This involves analyzing the security posture of systems and networks, identifying potential weaknesses, and prioritizing mitigation efforts.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Access Control\u003C/strong>: Implementing strict access controls to limit access to sensitive data and systems only to authorized users. This includes enforcing strong password policies, implementing role-based access controls, and regularly reviewing user permissions.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Employee Training\u003C/strong>: Providing comprehensive cybersecurity training to employees to raise awareness of common cyber threats and best practices for safeguarding digital assets. This includes training on identifying phishing emails, practicing good password hygiene, and following security protocols when accessing company resources remotely.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Incident Response Planning\u003C/strong>: Developing a robust incident response plan to effectively respond to security incidents and minimize the impact on digital assets. This involves establishing clear roles and responsibilities, defining communication protocols, and conducting regular incident response drills to ensure readiness.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Continuous Monitoring\u003C/strong>: Implementing continuous monitoring solutions to detect and respond to security threats in real-time. This includes deploying intrusion detection systems (IDS), security information and event management (SIEM) solutions, and endpoint detection and response (EDR) tools to monitor for suspicious activity and anomalous behavior.\u003C/p>\n\u003C/li>\n\u003C/ol>\n\u003Ch2 id=\"conclusion\">Conclusion\u003C/h2>\n\u003Cp>Safeguarding digital assets in an interconnected world requires a proactive approach to cybersecurity, encompassing risk assessment, access control, employee training, incident response planning, and continuous monitoring. By implementing robust cybersecurity measures and staying vigilant against emerging threats, organizations can effectively protect their digital assets and maintain the trust of their customers and stakeholders in an increasingly interconnected world.\u003C/p>\n\u003Cp>Remember, cybersecurity is not a one-time effort but an ongoing commitment to safeguarding digital assets against evolving cyber threats. By prioritizing cybersecurity and investing in the right tools and technologies, organizations can mitigate risks, strengthen their security posture, and ensure the integrity, confidentiality, and availability of their digital assets in an interconnected world.\u003C/p>",{"headings":796,"localImagePaths":807,"remoteImagePaths":808,"frontmatter":809,"imagePaths":812},[797,800,803,806],{"depth":59,"slug":798,"text":799},"understanding-the-risks","Understanding the Risks",{"depth":59,"slug":801,"text":802},"importance-of-cybersecurity-measures","Importance of Cybersecurity Measures",{"depth":59,"slug":804,"text":805},"strategies-for-safeguarding-digital-assets","Strategies for Safeguarding Digital Assets",{"depth":59,"slug":91,"text":92},[],[],{"title":783,"slug":780,"description":784,"pubDate":810,"tags":811,"coverImage":791},"Jul 17 2024",[439],[],"29/index.md","systemd-nspawn",{"id":814,"data":816,"body":824,"filePath":825,"assetImports":826,"digest":828,"rendered":829,"legacyId":848},{"title":817,"description":818,"pubDate":819,"tags":820,"coverImage":823},"Systemd-nspawn","Conteiners are the VM of the future",["Date","2023-05-31T00:00:00.000Z"],[43,821,822],"Docker","Container","__ASTRO_IMAGE_./post_img21.webp","# An Introduction to systemd-nspawn\n\nSystemd-nspawn is a tool that provides lightweight containerization in Linux systems. It is a part of the systemd suite, a system and service manager for Linux operating systems. Systemd-nspawn allows users to run an isolated operating system instance inside a container on a host system. In this blog post, we will explore systemd-nspawn, its features, and how to use it.\n\n## Features\n\nSystemd-nspawn offers several features that make it a useful tool for system administrators and developers. Here are some of its key features:\n\n- **Lightweight containerization:** Systemd-nspawn provides a lightweight approach to containerization. Unlike full-fledged virtual machines, containers created with systemd-nspawn share the same kernel as the host system, which reduces resource overhead and improves performance.\n- **Easy to use:** Systemd-nspawn is easy to use and does not require advanced knowledge of containerization. It provides a simple command-line interface for creating, managing, and accessing containers.\n- **Secure:** Systemd-nspawn provides a secure environment for running applications. Containers created with systemd-nspawn are isolated from the host system, preventing them from accessing critical system resources.\n\n## Getting Started\n\nBefore you can use systemd-nspawn, you need to ensure that it is installed on your system. If you are using a Linux distribution that uses systemd, systemd-nspawn is likely already installed. To check if systemd-nspawn is installed, run the following command:\n\n```bash\nsystemd-nspawn --version\n```\n\nIf systemd-nspawn is not installed, you can install it using your distribution's package manager. For example, on Ubuntu, you can install systemd-nspawn using the following command:\n\n```bash\nsudo apt-get install systemd-container\n```\n\nOnce you have installed systemd-nspawn, you can create a container by running the following command:\n\n```bash\nsudo systemd-nspawn -bD /path/to/rootfs\n```\n\nThis command creates a new container and starts a new shell inside it. The `-b` flag tells systemd-nspawn to boot the container, while the `-D` flag specifies the path to the root file system of the container. The `rootfs` directory should contain the root file system of the operating system you want to run inside the container.\n\nYou can exit the container by running the `exit` command. To start the container again, run the `systemd-nspawn` command with the same options as before.\n\n## Conclusion\n\nSystemd-nspawn is a lightweight containerization tool that provides a simple and secure environment for running applications. It is easy to use and does not require advanced knowledge of containerization. If you are looking for a lightweight and secure way to run applications on a Linux system, systemd-nspawn is worth considering.\n\n#### Authors\n\n- [@filippo-ferrando](https://www.github.com/filippo-ferrando)","src/content/blog/21/index.md",[827],"./post_img21.webp","79528d0b96056e9d",{"html":830,"metadata":831},"\u003Ch1 id=\"an-introduction-to-systemd-nspawn\">An Introduction to systemd-nspawn\u003C/h1>\n\u003Cp>Systemd-nspawn is a tool that provides lightweight containerization in Linux systems. It is a part of the systemd suite, a system and service manager for Linux operating systems. Systemd-nspawn allows users to run an isolated operating system instance inside a container on a host system. In this blog post, we will explore systemd-nspawn, its features, and how to use it.\u003C/p>\n\u003Ch2 id=\"features\">Features\u003C/h2>\n\u003Cp>Systemd-nspawn offers several features that make it a useful tool for system administrators and developers. Here are some of its key features:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Lightweight containerization:\u003C/strong> Systemd-nspawn provides a lightweight approach to containerization. Unlike full-fledged virtual machines, containers created with systemd-nspawn share the same kernel as the host system, which reduces resource overhead and improves performance.\u003C/li>\n\u003Cli>\u003Cstrong>Easy to use:\u003C/strong> Systemd-nspawn is easy to use and does not require advanced knowledge of containerization. It provides a simple command-line interface for creating, managing, and accessing containers.\u003C/li>\n\u003Cli>\u003Cstrong>Secure:\u003C/strong> Systemd-nspawn provides a secure environment for running applications. Containers created with systemd-nspawn are isolated from the host system, preventing them from accessing critical system resources.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"getting-started\">Getting Started\u003C/h2>\n\u003Cp>Before you can use systemd-nspawn, you need to ensure that it is installed on your system. If you are using a Linux distribution that uses systemd, systemd-nspawn is likely already installed. To check if systemd-nspawn is installed, run the following command:\u003C/p>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#A6E22E\">systemd-nspawn\u003C/span>\u003Cspan style=\"color:#AE81FF\"> --version\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>If systemd-nspawn is not installed, you can install it using your distribution’s package manager. For example, on Ubuntu, you can install systemd-nspawn using the following command:\u003C/p>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#A6E22E\">sudo\u003C/span>\u003Cspan style=\"color:#E6DB74\"> apt-get\u003C/span>\u003Cspan style=\"color:#E6DB74\"> install\u003C/span>\u003Cspan style=\"color:#E6DB74\"> systemd-container\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Once you have installed systemd-nspawn, you can create a container by running the following command:\u003C/p>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#A6E22E\">sudo\u003C/span>\u003Cspan style=\"color:#E6DB74\"> systemd-nspawn\u003C/span>\u003Cspan style=\"color:#AE81FF\"> -bD\u003C/span>\u003Cspan style=\"color:#E6DB74\"> /path/to/rootfs\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>This command creates a new container and starts a new shell inside it. The \u003Ccode>-b\u003C/code> flag tells systemd-nspawn to boot the container, while the \u003Ccode>-D\u003C/code> flag specifies the path to the root file system of the container. The \u003Ccode>rootfs\u003C/code> directory should contain the root file system of the operating system you want to run inside the container.\u003C/p>\n\u003Cp>You can exit the container by running the \u003Ccode>exit\u003C/code> command. To start the container again, run the \u003Ccode>systemd-nspawn\u003C/code> command with the same options as before.\u003C/p>\n\u003Ch2 id=\"conclusion\">Conclusion\u003C/h2>\n\u003Cp>Systemd-nspawn is a lightweight containerization tool that provides a simple and secure environment for running applications. It is easy to use and does not require advanced knowledge of containerization. If you are looking for a lightweight and secure way to run applications on a Linux system, systemd-nspawn is worth considering.\u003C/p>\n\u003Ch4 id=\"authors\">Authors\u003C/h4>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://www.github.com/filippo-ferrando\" target=\"_blank\" rel=\"noopener\">@filippo-ferrando\u003C/a>\u003C/li>\n\u003C/ul>",{"headings":832,"localImagePaths":842,"remoteImagePaths":843,"frontmatter":844,"imagePaths":847},[833,836,837,840,841],{"depth":55,"slug":834,"text":835},"an-introduction-to-systemd-nspawn","An Introduction to systemd-nspawn",{"depth":59,"slug":60,"text":61},{"depth":59,"slug":838,"text":839},"getting-started","Getting Started",{"depth":59,"slug":91,"text":92},{"depth":94,"slug":95,"text":96},[],[],{"title":817,"slug":814,"description":818,"pubDate":845,"tags":846,"coverImage":827},"May 31 2023",[43,821,822],[],"21/index.md","how-to-flash-raspberrypi",{"id":849,"data":851,"body":858,"filePath":859,"assetImports":860,"digest":862,"rendered":863,"legacyId":901},{"title":852,"description":853,"pubDate":854,"tags":855,"coverImage":857},"How-to: Flash a Raspberry Pi","Guide on how to install raspbian on a Raspberry pi",["Date","2023-01-28T00:00:00.000Z"],[610,43,856],"RaspberryPI","__ASTRO_IMAGE_./post_img3.png","# How to flash raspbian on a Rasperry Pi\n\n**First you need some tools**\n\n1. balenaEtcher\n\n2. raspbian iso\n\n3. a pc\n\nEstimate Time: 15-20 minutes\n\n## Let's install etcher\n\ngo to the balena [download page](https://www.balena.io/etcher/) and retrive the\nsuitable version for your needs.\n\u003C!-- ![1](https://github.com/filippo-ferrando/howToFlashARaspberry/blob/master/assets/1.png) -->\n\n## Download the raspbian ISO\n\nfor this guide we'll use the lite version 'cause we don't need a GUI\n\u003C!-- ![2](https://github.com/filippo-ferrando/howToFlashARaspberry/blob/master/assets/2.png) -->\n\n## Now we have all we need!\n\nTake you micro sd and insert it into the pc.\n\nThen open balenaEtcher and select your iso image.\n\u003C!-- ![3](https://github.com/filippo-ferrando/howToFlashARaspberry/blob/master/assets/3.png) -->\n\n##### Then select the media you want to flash and click \"Flash!\"\n\n## Create ssh folder in boot partition\n\nWe have to activate ssh at the first boot because we want to use the Pi in\nheadless mode (without keyboard and monitor).\n\nTo do that mount the sd card just flashed in your pc and open the **boot**\npartition.\n\ncreate a folder named \"ssh\" and the job is done!\n\n## Configure wpa_supplicant for wifi connection\n\nThe Pi will connect to the lan via WiFi, so you need to setup the connection\nbefore the Pi boots.\n\nTo do that mount again the sd in your pc, but this time go into the root\npartition.\n\nWe'll modify the file _/etc/wpa_supplicant/wpa_supplicant.conf_ and add the ssid\nand password of the designed wifi network.\n\n**The file requires root permissions to be modified**\n\nOnce you opened that the file, it has to look like these:\n\n```bash\ncountry=it\nupdate_config=1\nctrl_interface=/var/run/wpa_supplicant\n\nnetwork={\n ssid=\"YOURSSID\"\n psk=\"YOURPASSWORD\"\n}\n```\n\nAfter that you can finally put the sd in your Raspberry Pi and power it up!\n\n## First boot and basic config\n\nWhen the pi ha booted up you have to find its ip address, to do it run the\ncommand _ip-scan_ and connect your pc to the board via ssh\n\n```bash\nssh pi@*pi ip*\n```\n\nThe default password is **raspberry**\n\nOnce you're connected, launch the command:\n\n```bash\nsudo raspi-config\n```\n\nThis will prompt a menu from which we can change some important settings:\n\u003C!-- ![4](https://github.com/filippo-ferrando/howToFlashARaspberry/blob/master/assets/4.png) -->\n\n1. hostname --> the device name on the network\n\n2. locale --> Country (language)\n\n3. timezone\n\n4. password --> change pi and root password\n\n**optionally** we can enable the _\"wait for network at boot\"_ which won't fully\nboot the operating system until a Wi-Fi or cable connection is established.\n\n## Update and Upgrade\n\nAnother importat thing is to keep the software up-to-date, so we can run:\n\n```bash\nsudo apt update && sudo apt upgrade\n```\n\nto update the index of packages and upgrade the installed software.\n\nThen we reboot the pi and we're ready to rock!","src/content/blog/3/index.md",[861],"./post_img3.png","3db138a21ecaaec6",{"html":864,"metadata":865},"\u003Ch1 id=\"how-to-flash-raspbian-on-a-rasperry-pi\">How to flash raspbian on a Rasperry Pi\u003C/h1>\n\u003Cp>\u003Cstrong>First you need some tools\u003C/strong>\u003C/p>\n\u003Col>\n\u003Cli>\n\u003Cp>balenaEtcher\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>raspbian iso\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>a pc\u003C/p>\n\u003C/li>\n\u003C/ol>\n\u003Cp>Estimate Time: 15-20 minutes\u003C/p>\n\u003Ch2 id=\"lets-install-etcher\">Let’s install etcher\u003C/h2>\n\u003Cp>go to the balena \u003Ca href=\"https://www.balena.io/etcher/\" target=\"_blank\" rel=\"noopener\">download page\u003C/a> and retrive the\nsuitable version for your needs.\u003C/p>\n\u003C!-- ![1](https://github.com/filippo-ferrando/howToFlashARaspberry/blob/master/assets/1.png) -->\n\u003Ch2 id=\"download-the-raspbian-iso\">Download the raspbian ISO\u003C/h2>\n\u003Cp>for this guide we’ll use the lite version ‘cause we don’t need a GUI\u003C/p>\n\u003C!-- ![2](https://github.com/filippo-ferrando/howToFlashARaspberry/blob/master/assets/2.png) -->\n\u003Ch2 id=\"now-we-have-all-we-need\">Now we have all we need!\u003C/h2>\n\u003Cp>Take you micro sd and insert it into the pc.\u003C/p>\n\u003Cp>Then open balenaEtcher and select your iso image.\u003C/p>\n\u003C!-- ![3](https://github.com/filippo-ferrando/howToFlashARaspberry/blob/master/assets/3.png) -->\n\u003Ch5 id=\"then-select-the-media-you-want-to-flash-and-click-flash\">Then select the media you want to flash and click “Flash!”\u003C/h5>\n\u003Ch2 id=\"create-ssh-folder-in-boot-partition\">Create ssh folder in boot partition\u003C/h2>\n\u003Cp>We have to activate ssh at the first boot because we want to use the Pi in\nheadless mode (without keyboard and monitor).\u003C/p>\n\u003Cp>To do that mount the sd card just flashed in your pc and open the \u003Cstrong>boot\u003C/strong>\npartition.\u003C/p>\n\u003Cp>create a folder named “ssh” and the job is done!\u003C/p>\n\u003Ch2 id=\"configure-wpa_supplicant-for-wifi-connection\">Configure wpa_supplicant for wifi connection\u003C/h2>\n\u003Cp>The Pi will connect to the lan via WiFi, so you need to setup the connection\nbefore the Pi boots.\u003C/p>\n\u003Cp>To do that mount again the sd in your pc, but this time go into the root\npartition.\u003C/p>\n\u003Cp>We’ll modify the file \u003Cem>/etc/wpa_supplicant/wpa_supplicant.conf\u003C/em> and add the ssid\nand password of the designed wifi network.\u003C/p>\n\u003Cp>\u003Cstrong>The file requires root permissions to be modified\u003C/strong>\u003C/p>\n\u003Cp>Once you opened that the file, it has to look like these:\u003C/p>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#F8F8F2\">country\u003C/span>\u003Cspan style=\"color:#F92672\">=\u003C/span>\u003Cspan style=\"color:#E6DB74\">it\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F8F8F2\">update_config\u003C/span>\u003Cspan style=\"color:#F92672\">=\u003C/span>\u003Cspan style=\"color:#E6DB74\">1\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F8F8F2\">ctrl_interface\u003C/span>\u003Cspan style=\"color:#F92672\">=\u003C/span>\u003Cspan style=\"color:#E6DB74\">/var/run/wpa_supplicant\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F8F8F2\">network\u003C/span>\u003Cspan style=\"color:#F92672\">=\u003C/span>\u003Cspan style=\"color:#F8F8F2\">{\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F8F8F2\"> ssid\u003C/span>\u003Cspan style=\"color:#F92672\">=\u003C/span>\u003Cspan style=\"color:#E6DB74\">\"YOURSSID\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F8F8F2\"> psk\u003C/span>\u003Cspan style=\"color:#F92672\">=\u003C/span>\u003Cspan style=\"color:#E6DB74\">\"YOURPASSWORD\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F8F8F2\">}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>After that you can finally put the sd in your Raspberry Pi and power it up!\u003C/p>\n\u003Ch2 id=\"first-boot-and-basic-config\">First boot and basic config\u003C/h2>\n\u003Cp>When the pi ha booted up you have to find its ip address, to do it run the\ncommand \u003Cem>ip-scan\u003C/em> and connect your pc to the board via ssh\u003C/p>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#A6E22E\">ssh\u003C/span>\u003Cspan style=\"color:#E6DB74\"> pi@\u003C/span>\u003Cspan style=\"color:#FD971F\">*\u003C/span>\u003Cspan style=\"color:#E6DB74\">pi\u003C/span>\u003Cspan style=\"color:#E6DB74\"> ip\u003C/span>\u003Cspan style=\"color:#FD971F\">*\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>The default password is \u003Cstrong>raspberry\u003C/strong>\u003C/p>\n\u003Cp>Once you’re connected, launch the command:\u003C/p>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#A6E22E\">sudo\u003C/span>\u003Cspan style=\"color:#E6DB74\"> raspi-config\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>This will prompt a menu from which we can change some important settings:\u003C/p>\n\u003C!-- ![4](https://github.com/filippo-ferrando/howToFlashARaspberry/blob/master/assets/4.png) -->\n\u003Col>\n\u003Cli>\n\u003Cp>hostname —> the device name on the network\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>locale —> Country (language)\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>timezone\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>password —> change pi and root password\u003C/p>\n\u003C/li>\n\u003C/ol>\n\u003Cp>\u003Cstrong>optionally\u003C/strong> we can enable the \u003Cem>“wait for network at boot”\u003C/em> which won’t fully\nboot the operating system until a Wi-Fi or cable connection is established.\u003C/p>\n\u003Ch2 id=\"update-and-upgrade\">Update and Upgrade\u003C/h2>\n\u003Cp>Another importat thing is to keep the software up-to-date, so we can run:\u003C/p>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#A6E22E\">sudo\u003C/span>\u003Cspan style=\"color:#E6DB74\"> apt\u003C/span>\u003Cspan style=\"color:#E6DB74\"> update\u003C/span>\u003Cspan style=\"color:#F8F8F2\"> &#x26;&#x26; \u003C/span>\u003Cspan style=\"color:#A6E22E\">sudo\u003C/span>\u003Cspan style=\"color:#E6DB74\"> apt\u003C/span>\u003Cspan style=\"color:#E6DB74\"> upgrade\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>to update the index of packages and upgrade the installed software.\u003C/p>\n\u003Cp>Then we reboot the pi and we’re ready to rock!\u003C/p>",{"headings":866,"localImagePaths":895,"remoteImagePaths":896,"frontmatter":897,"imagePaths":900},[867,870,873,876,879,883,886,889,892],{"depth":55,"slug":868,"text":869},"how-to-flash-raspbian-on-a-rasperry-pi","How to flash raspbian on a Rasperry Pi",{"depth":59,"slug":871,"text":872},"lets-install-etcher","Let’s install etcher",{"depth":59,"slug":874,"text":875},"download-the-raspbian-iso","Download the raspbian ISO",{"depth":59,"slug":877,"text":878},"now-we-have-all-we-need","Now we have all we need!",{"depth":880,"slug":881,"text":882},5,"then-select-the-media-you-want-to-flash-and-click-flash","Then select the media you want to flash and click “Flash!”",{"depth":59,"slug":884,"text":885},"create-ssh-folder-in-boot-partition","Create ssh folder in boot partition",{"depth":59,"slug":887,"text":888},"configure-wpa_supplicant-for-wifi-connection","Configure wpa_supplicant for wifi connection",{"depth":59,"slug":890,"text":891},"first-boot-and-basic-config","First boot and basic config",{"depth":59,"slug":893,"text":894},"update-and-upgrade","Update and Upgrade",[],[],{"title":852,"slug":849,"description":853,"tags":898,"pubDate":899,"coverImage":861},[610,43,856],"Jan 28 2023",[],"3/index.md","git-101",{"id":902,"data":904,"body":912,"filePath":913,"assetImports":914,"digest":916,"rendered":917,"legacyId":952},{"title":905,"description":906,"pubDate":907,"tags":908,"coverImage":911},"GIT 101","Short and simple git introduction",["Date","2023-10-08T00:00:00.000Z"],[909,910],"Git","CI","__ASTRO_IMAGE_./post_img25.webp","# Basic Commands\n\n### Create a new repository\n\nNavigate in the directory where you want to init a new local git repo:\n\n```bash\ngit init\n```\n\n### Adding a remote repo\n\nTo push you change on a remote server you have to add the origin:\n\n```bash\ngit remote add origin \u003Clink to your repo>\n```\n\n## Work with files | Staging\n\nWhenever you create new file or modify existing one you have to stage, commit and push; i'll show the way i use it (but there are more)\n\n```bash\ngit add .\n```\n\nThis command stage all the new/modified file in the working folder\nYour can add only certain file specifing the file name\n\n```bash\ngit add \u003Cfilename>\n```\n\nNow we have staged file ready to be committed, you can prepare the commit with the commnad\n```bash\ngit commit -m \"useful message\"\n```\n\nIs good practice writing useful commit message for tracking and rollbacking purpose\n\nThe modifies are now ready to be push, so let's push them on the remote\n\n```bash\ngit push\n```\n\n## Branching\n\nBranching is a very useful practice when you use a version-control system, i'll leave you a more detailed description on how it works [here](https://git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell), in this artcle i will only show you the basic command to efficently manage this feature.\n\n### Create a branch\nMove in your working directory and type the command:\n```bash\ngit checkout -b \u003Cbranch name>\n```\nThis instruction tell git to create a branch named ```\u003Cbranch name>``` \n\n### Enter a branch that alredy exists\nTo work in a pre-existing branch you can use:\n```bash\ngit checkout \u003Cbranch name>\n```\n\n## Ending\nFor this post is all, maybe i will do a 102 post later this year.\n\n#### Authors\n\n- [@filippo-ferrando](https://www.github.com/filippo-ferrando)","src/content/blog/25/index.md",[915],"./post_img25.webp","ecdc15667b9ba07d",{"html":918,"metadata":919},"\u003Ch1 id=\"basic-commands\">Basic Commands\u003C/h1>\n\u003Ch3 id=\"create-a-new-repository\">Create a new repository\u003C/h3>\n\u003Cp>Navigate in the directory where you want to init a new local git repo:\u003C/p>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#A6E22E\">git\u003C/span>\u003Cspan style=\"color:#E6DB74\"> init\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch3 id=\"adding-a-remote-repo\">Adding a remote repo\u003C/h3>\n\u003Cp>To push you change on a remote server you have to add the origin:\u003C/p>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#A6E22E\">git\u003C/span>\u003Cspan style=\"color:#E6DB74\"> remote\u003C/span>\u003Cspan style=\"color:#E6DB74\"> add\u003C/span>\u003Cspan style=\"color:#E6DB74\"> origin\u003C/span>\u003Cspan style=\"color:#F92672\"> &#x3C;\u003C/span>\u003Cspan style=\"color:#E6DB74\">link\u003C/span>\u003Cspan style=\"color:#E6DB74\"> to\u003C/span>\u003Cspan style=\"color:#E6DB74\"> your\u003C/span>\u003Cspan style=\"color:#E6DB74\"> rep\u003C/span>\u003Cspan style=\"color:#F8F8F2\">o\u003C/span>\u003Cspan style=\"color:#F92672\">>\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch2 id=\"work-with-files--staging\">Work with files | Staging\u003C/h2>\n\u003Cp>Whenever you create new file or modify existing one you have to stage, commit and push; i’ll show the way i use it (but there are more)\u003C/p>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#A6E22E\">git\u003C/span>\u003Cspan style=\"color:#E6DB74\"> add\u003C/span>\u003Cspan style=\"color:#E6DB74\"> .\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>This command stage all the new/modified file in the working folder\nYour can add only certain file specifing the file name\u003C/p>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#A6E22E\">git\u003C/span>\u003Cspan style=\"color:#E6DB74\"> add\u003C/span>\u003Cspan style=\"color:#F92672\"> &#x3C;\u003C/span>\u003Cspan style=\"color:#E6DB74\">filenam\u003C/span>\u003Cspan style=\"color:#F8F8F2\">e\u003C/span>\u003Cspan style=\"color:#F92672\">>\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Now we have staged file ready to be committed, you can prepare the commit with the commnad\u003C/p>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#A6E22E\">git\u003C/span>\u003Cspan style=\"color:#E6DB74\"> commit\u003C/span>\u003Cspan style=\"color:#AE81FF\"> -m\u003C/span>\u003Cspan style=\"color:#E6DB74\"> \"useful message\"\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Is good practice writing useful commit message for tracking and rollbacking purpose\u003C/p>\n\u003Cp>The modifies are now ready to be push, so let’s push them on the remote\u003C/p>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#A6E22E\">git\u003C/span>\u003Cspan style=\"color:#E6DB74\"> push\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch2 id=\"branching\">Branching\u003C/h2>\n\u003Cp>Branching is a very useful practice when you use a version-control system, i’ll leave you a more detailed description on how it works \u003Ca href=\"https://git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell\" target=\"_blank\" rel=\"noopener\">here\u003C/a>, in this artcle i will only show you the basic command to efficently manage this feature.\u003C/p>\n\u003Ch3 id=\"create-a-branch\">Create a branch\u003C/h3>\n\u003Cp>Move in your working directory and type the command:\u003C/p>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#A6E22E\">git\u003C/span>\u003Cspan style=\"color:#E6DB74\"> checkout\u003C/span>\u003Cspan style=\"color:#AE81FF\"> -b\u003C/span>\u003Cspan style=\"color:#F92672\"> &#x3C;\u003C/span>\u003Cspan style=\"color:#E6DB74\">branch\u003C/span>\u003Cspan style=\"color:#E6DB74\"> nam\u003C/span>\u003Cspan style=\"color:#F8F8F2\">e\u003C/span>\u003Cspan style=\"color:#F92672\">>\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>This instruction tell git to create a branch named \u003Ccode>&#x3C;branch name>\u003C/code>\u003C/p>\n\u003Ch3 id=\"enter-a-branch-that-alredy-exists\">Enter a branch that alredy exists\u003C/h3>\n\u003Cp>To work in a pre-existing branch you can use:\u003C/p>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#A6E22E\">git\u003C/span>\u003Cspan style=\"color:#E6DB74\"> checkout\u003C/span>\u003Cspan style=\"color:#F92672\"> &#x3C;\u003C/span>\u003Cspan style=\"color:#E6DB74\">branch\u003C/span>\u003Cspan style=\"color:#E6DB74\"> nam\u003C/span>\u003Cspan style=\"color:#F8F8F2\">e\u003C/span>\u003Cspan style=\"color:#F92672\">>\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch2 id=\"ending\">Ending\u003C/h2>\n\u003Cp>For this post is all, maybe i will do a 102 post later this year.\u003C/p>\n\u003Ch4 id=\"authors\">Authors\u003C/h4>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://www.github.com/filippo-ferrando\" target=\"_blank\" rel=\"noopener\">@filippo-ferrando\u003C/a>\u003C/li>\n\u003C/ul>",{"headings":920,"localImagePaths":946,"remoteImagePaths":947,"frontmatter":948,"imagePaths":951},[921,924,927,930,933,936,939,942,945],{"depth":55,"slug":922,"text":923},"basic-commands","Basic Commands",{"depth":63,"slug":925,"text":926},"create-a-new-repository","Create a new repository",{"depth":63,"slug":928,"text":929},"adding-a-remote-repo","Adding a remote repo",{"depth":59,"slug":931,"text":932},"work-with-files--staging","Work with files | Staging",{"depth":59,"slug":934,"text":935},"branching","Branching",{"depth":63,"slug":937,"text":938},"create-a-branch","Create a branch",{"depth":63,"slug":940,"text":941},"enter-a-branch-that-alredy-exists","Enter a branch that alredy exists",{"depth":59,"slug":943,"text":944},"ending","Ending",{"depth":94,"slug":95,"text":96},[],[],{"title":905,"slug":902,"description":906,"pubDate":949,"tags":950,"coverImage":915},"Oct 8 2023",[909,910],[],"25/index.md","gh-action",{"id":953,"data":955,"body":961,"filePath":962,"assetImports":963,"digest":965,"rendered":966,"legacyId":989},{"title":956,"description":957,"pubDate":958,"tags":959,"coverImage":960},"Github Actions","Automatize processes with Github Action",["Date","2024-10-02T00:00:00.000Z"],[910],"__ASTRO_IMAGE_./post_img31.jpg","# Automatize build processes with Github\n\nIn the last period I have been dealing with Github Action for different applications, from translation to building projects in different programming languages and for different platforms; there are a number of features that make this product (in my opinion) superior to other CI/CD tools on the market today, this the reason why I chose to use this tool.\nActions offers terrific integration with repositories on Github, as it should and this makes it very intuitive and “easy” to use, through just writing a yaml file you can automate most of the tasks that previously had to be done by hand.\n\n## Pros and Cons\n\nLet's start by saying that all that glitters is not gold, nice integration and simplicity, but there are some shortcomings, such as the lack of runners with specific platforms not directly provided, etc...\nIt remains that actions are a really great tool if you want to automate any process within a repository, on the dedicated marketplace there are a lot of ready-to-use actions that you just need to import into your yaml file, greatly reducing the time needed to complete simple and repetitive tasks (such as ssh connection to another client or setup of a specific java version)\n\nThe biggest downside, in my experience, is the usage limits that GitHub imposes. In order to test and develop an action from scratch that meets all your needs in the best way, and most importantly, works, it takes time and a huge number of runs on the platform. Sooner or later, this will impact the final costs of this solution.\n\n## How it works (in short)\n\nThe setup is very intuitive. On the repository where you want to use this tool, navigate to the \"Actions\" tab and start creating a new action, either from scratch or using a pre-existing template. Once the file is completed, it will be runned on a specified runner (defined in the file), which can be Linux, Windows, or Mac, at specific moments (also defined in the file) called triggers. Triggers can vary, from a push to a specific branch to the acceptance of a pull request on another branch. This allows for great flexibility and enables the automation of not only build processes but also tasks that are not strictly related to code.\n\n## My favorite functionality\n\nAs I explained in the previous post, I had quite a few issues automating code signing on a remote Mac runner, so I opted for a local runners instead. In my opinion, this is one of the best features GitHub offers to developers, as it allows automations triggered by the usual events to run on local machines, which can be configured exactly as needed, with all the necessary precautions to keep certificates, tokens, and passwords of various kinds \"on-site\".\n\nBy using a local runner, you can avoid using the YAML file to configure secrets or other sensitive information that you may prefer not to store online, even if you're using a private repository or other form of protection.\n\n## Conclusion\nIn conclusion, GitHub Actions has proven to be an incredibly powerful and flexible tool for automating build processes and more. While there are some limitations, such as platform-specific runner availability and potential cost concerns from extensive usage, the ease of integration with GitHub repositories and the extensive marketplace of pre-built actions make it a strong choice for developers. The ability to use local runners for more secure, customizable automation only adds to its appeal. Whether you are managing builds, deployments, or other repetitive tasks, GitHub Actions can significantly streamline your workflow and improve productivity.","src/content/blog/31/index.md",[964],"./post_img31.jpg","6a5051cc003a17a7",{"html":967,"metadata":968},"\u003Ch1 id=\"automatize-build-processes-with-github\">Automatize build processes with Github\u003C/h1>\n\u003Cp>In the last period I have been dealing with Github Action for different applications, from translation to building projects in different programming languages and for different platforms; there are a number of features that make this product (in my opinion) superior to other CI/CD tools on the market today, this the reason why I chose to use this tool.\nActions offers terrific integration with repositories on Github, as it should and this makes it very intuitive and “easy” to use, through just writing a yaml file you can automate most of the tasks that previously had to be done by hand.\u003C/p>\n\u003Ch2 id=\"pros-and-cons\">Pros and Cons\u003C/h2>\n\u003Cp>Let’s start by saying that all that glitters is not gold, nice integration and simplicity, but there are some shortcomings, such as the lack of runners with specific platforms not directly provided, etc…\nIt remains that actions are a really great tool if you want to automate any process within a repository, on the dedicated marketplace there are a lot of ready-to-use actions that you just need to import into your yaml file, greatly reducing the time needed to complete simple and repetitive tasks (such as ssh connection to another client or setup of a specific java version)\u003C/p>\n\u003Cp>The biggest downside, in my experience, is the usage limits that GitHub imposes. In order to test and develop an action from scratch that meets all your needs in the best way, and most importantly, works, it takes time and a huge number of runs on the platform. Sooner or later, this will impact the final costs of this solution.\u003C/p>\n\u003Ch2 id=\"how-it-works-in-short\">How it works (in short)\u003C/h2>\n\u003Cp>The setup is very intuitive. On the repository where you want to use this tool, navigate to the “Actions” tab and start creating a new action, either from scratch or using a pre-existing template. Once the file is completed, it will be runned on a specified runner (defined in the file), which can be Linux, Windows, or Mac, at specific moments (also defined in the file) called triggers. Triggers can vary, from a push to a specific branch to the acceptance of a pull request on another branch. This allows for great flexibility and enables the automation of not only build processes but also tasks that are not strictly related to code.\u003C/p>\n\u003Ch2 id=\"my-favorite-functionality\">My favorite functionality\u003C/h2>\n\u003Cp>As I explained in the previous post, I had quite a few issues automating code signing on a remote Mac runner, so I opted for a local runners instead. In my opinion, this is one of the best features GitHub offers to developers, as it allows automations triggered by the usual events to run on local machines, which can be configured exactly as needed, with all the necessary precautions to keep certificates, tokens, and passwords of various kinds “on-site”.\u003C/p>\n\u003Cp>By using a local runner, you can avoid using the YAML file to configure secrets or other sensitive information that you may prefer not to store online, even if you’re using a private repository or other form of protection.\u003C/p>\n\u003Ch2 id=\"conclusion\">Conclusion\u003C/h2>\n\u003Cp>In conclusion, GitHub Actions has proven to be an incredibly powerful and flexible tool for automating build processes and more. While there are some limitations, such as platform-specific runner availability and potential cost concerns from extensive usage, the ease of integration with GitHub repositories and the extensive marketplace of pre-built actions make it a strong choice for developers. The ability to use local runners for more secure, customizable automation only adds to its appeal. Whether you are managing builds, deployments, or other repetitive tasks, GitHub Actions can significantly streamline your workflow and improve productivity.\u003C/p>",{"headings":969,"localImagePaths":983,"remoteImagePaths":984,"frontmatter":985,"imagePaths":988},[970,973,976,979,982],{"depth":55,"slug":971,"text":972},"automatize-build-processes-with-github","Automatize build processes with Github",{"depth":59,"slug":974,"text":975},"pros-and-cons","Pros and Cons",{"depth":59,"slug":977,"text":978},"how-it-works-in-short","How it works (in short)",{"depth":59,"slug":980,"text":981},"my-favorite-functionality","My favorite functionality",{"depth":59,"slug":91,"text":92},[],[],{"title":956,"slug":953,"description":957,"pubDate":986,"tags":987,"coverImage":964},"Oct 2 2024",[910],[],"31/index.md","signing-macos-packages",{"id":990,"data":992,"body":999,"filePath":1000,"assetImports":1001,"digest":1003,"rendered":1004,"legacyId":1040},{"title":993,"description":994,"pubDate":995,"updatedDate":996,"tags":997,"coverImage":998},"Signing Mac OS packages","My experience after a month of working on macOS",["Date","2024-08-07T00:00:00.000Z"],["Date","2024-09-30T00:00:00.000Z"],[524,910],"__ASTRO_IMAGE_./post_img30.avif","# macOS Code Signing: How It Works\n\nLet's start by saying that the process actually seems very simple. It consists of three main steps: code signing, notarisation, and stapling. We'll delve into what each step involves and why they are necessary.\n\nHere is a simple pipeline of the process:\n\n```\n[Code Signing] --> [Notarisation] --(Apple approval)--> [Stapling] --(Notarisation \"applied\" to package)--> [Signed App]\n```\n\n## Prerequisites\n\nBefore we can sign code, we must have an **identity** to certify the validity of the signature. This identity is provided by a certificate issued by Apple, known as the **Developer ID Application**. Once assigned to your specific Apple Developer account, this certificate allows you to *sign* pieces of code.\n\nYou will also need a valid Apple ID and an application-specific password, both generated and issued by Apple Developer. This complex process involves multiple certificates and passwords rather than a single token directly assigned to the Apple account.\n\n#### Small Tip\n\nDo not try to import certificates directly into your **keychain**. **Though it may seem logical, it's not the best approach.** Instead, download Xcode and use its settings to import the certificates assigned to your account.\n\n## Code Signing\n\nThe first step is signing the code. Initially, I thought this would be done with a ready-made package, such as a .dmg or a working package. However, this is not the case. When signing a macOS application, all the files that are part of the final application must be signed individually using the ```codesign``` utility.\nIs this useful? I'm not sure. Necessary? Perhaps. A waste of time? Absolutely. To sign the files, we need to import our identity provided by the Developer certificate into our system keychain. This gives us a \"valid identity\" we can use. Wouldn't it have been easier to sign the final package directly? I think we'll never know.\n\n## Notarisation\n\nThe second step is notarisation. After signing the entire package, we need to ask Apple to confirm that we haven't messed anything up. We take our signed app, zip it, and send it to Apple, which will conduct thorough checks on our package and return it in a *reasonable* timeframe.\n\nTo do this, we need our Apple ID, app-specific password, and Team ID. When we get the package back, we'll know if it has been properly notarised or if there's something considered unworthy or unsafe for publication.\n\n## Stapling\n\nThe final stage is stapling. This process involves integrating the notarisation ticket into the final package, making it distributable as a secure package. This ensures that the package has not been tampered with after notarisation. Without this step, we will not be able to run the application on macOS since Gatekeeper, not seeing the embedded ticket, will not recognise the package as signed.\n\n## My Thoughts\n\nI've been a Linux user for 10 years, and this is the first time I've had to go through such a process to run an application on an operating system. There are too many steps that can go wrong and too many requirements for developers. While this process greatly reduces poorly written applications, it does not justify the time wasted. In my case, build times increased by about 20 minutes due to Apple's response times for notarisation, which also does not always succeed regardless of the code sent. This increase in build times can translate to higher costs, especially in automated build scenarios hosted on platforms like GitHub or any CI/CD platform.\n\nThe cost is not the main issue, though. The real difficulty lies in configuring a remote macOS machine to sign applications, from importing certificates to managing responses after stapling. Most of my headaches come from automating these processes. It is not guaranteed that importing the certificate from the command line will be effective, nor that invoking notarisation on a partially configured remote runner will be successful. The worst part is the vague error messages from Apple or the tools themselves, making it very difficult to understand where the error lies.\n\n## Conclusion\n\nAt this point, I have not yet successfully configured a remote runner that can return a correctly signed package, despite positive responses from all the used tools. I can't explain why signing, notarisation, and stapling, which consistently yield positive responses, still fail to sign a package. I will probably opt for using self-hosted runners, so I can configure everything once via the graphical interface. I'll keep you updated.\n\nThanks, Apple, for these weeks of intense fun.\n\n## Update\n\nIn the last month I was able to complete the project by signing it!\n\nThe solution turned out to be using self-hosted runners with a signature-enabled account on them and with the certificates imported BY HAND from xcode.\nDoing it this way, you need (if you want to use a CI/CD system) to configure the machine as a local runner (in the case of github it is a very very simple process) and prepare the environment where the application will be built.\nThis solution is very efficient as well as not very scalable on builds of multiple applications or very very large applications as the time would be quite stretched unless multiple runners are configured on the same repository.\n\nIt seems that apple doesn't really like automations to make our lives a little easier....","src/content/blog/30/index.md",[1002],"./post_img30.avif","94308892c88a7c41",{"html":1005,"metadata":1006},"\u003Ch1 id=\"macos-code-signing-how-it-works\">macOS Code Signing: How It Works\u003C/h1>\n\u003Cp>Let’s start by saying that the process actually seems very simple. It consists of three main steps: code signing, notarisation, and stapling. We’ll delve into what each step involves and why they are necessary.\u003C/p>\n\u003Cp>Here is a simple pipeline of the process:\u003C/p>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>[Code Signing] --> [Notarisation] --(Apple approval)--> [Stapling] --(Notarisation \"applied\" to package)--> [Signed App]\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch2 id=\"prerequisites\">Prerequisites\u003C/h2>\n\u003Cp>Before we can sign code, we must have an \u003Cstrong>identity\u003C/strong> to certify the validity of the signature. This identity is provided by a certificate issued by Apple, known as the \u003Cstrong>Developer ID Application\u003C/strong>. Once assigned to your specific Apple Developer account, this certificate allows you to \u003Cem>sign\u003C/em> pieces of code.\u003C/p>\n\u003Cp>You will also need a valid Apple ID and an application-specific password, both generated and issued by Apple Developer. This complex process involves multiple certificates and passwords rather than a single token directly assigned to the Apple account.\u003C/p>\n\u003Ch4 id=\"small-tip\">Small Tip\u003C/h4>\n\u003Cp>Do not try to import certificates directly into your \u003Cstrong>keychain\u003C/strong>. \u003Cstrong>Though it may seem logical, it’s not the best approach.\u003C/strong> Instead, download Xcode and use its settings to import the certificates assigned to your account.\u003C/p>\n\u003Ch2 id=\"code-signing\">Code Signing\u003C/h2>\n\u003Cp>The first step is signing the code. Initially, I thought this would be done with a ready-made package, such as a .dmg or a working package. However, this is not the case. When signing a macOS application, all the files that are part of the final application must be signed individually using the \u003Ccode>codesign\u003C/code> utility.\nIs this useful? I’m not sure. Necessary? Perhaps. A waste of time? Absolutely. To sign the files, we need to import our identity provided by the Developer certificate into our system keychain. This gives us a “valid identity” we can use. Wouldn’t it have been easier to sign the final package directly? I think we’ll never know.\u003C/p>\n\u003Ch2 id=\"notarisation\">Notarisation\u003C/h2>\n\u003Cp>The second step is notarisation. After signing the entire package, we need to ask Apple to confirm that we haven’t messed anything up. We take our signed app, zip it, and send it to Apple, which will conduct thorough checks on our package and return it in a \u003Cem>reasonable\u003C/em> timeframe.\u003C/p>\n\u003Cp>To do this, we need our Apple ID, app-specific password, and Team ID. When we get the package back, we’ll know if it has been properly notarised or if there’s something considered unworthy or unsafe for publication.\u003C/p>\n\u003Ch2 id=\"stapling\">Stapling\u003C/h2>\n\u003Cp>The final stage is stapling. This process involves integrating the notarisation ticket into the final package, making it distributable as a secure package. This ensures that the package has not been tampered with after notarisation. Without this step, we will not be able to run the application on macOS since Gatekeeper, not seeing the embedded ticket, will not recognise the package as signed.\u003C/p>\n\u003Ch2 id=\"my-thoughts\">My Thoughts\u003C/h2>\n\u003Cp>I’ve been a Linux user for 10 years, and this is the first time I’ve had to go through such a process to run an application on an operating system. There are too many steps that can go wrong and too many requirements for developers. While this process greatly reduces poorly written applications, it does not justify the time wasted. In my case, build times increased by about 20 minutes due to Apple’s response times for notarisation, which also does not always succeed regardless of the code sent. This increase in build times can translate to higher costs, especially in automated build scenarios hosted on platforms like GitHub or any CI/CD platform.\u003C/p>\n\u003Cp>The cost is not the main issue, though. The real difficulty lies in configuring a remote macOS machine to sign applications, from importing certificates to managing responses after stapling. Most of my headaches come from automating these processes. It is not guaranteed that importing the certificate from the command line will be effective, nor that invoking notarisation on a partially configured remote runner will be successful. The worst part is the vague error messages from Apple or the tools themselves, making it very difficult to understand where the error lies.\u003C/p>\n\u003Ch2 id=\"conclusion\">Conclusion\u003C/h2>\n\u003Cp>At this point, I have not yet successfully configured a remote runner that can return a correctly signed package, despite positive responses from all the used tools. I can’t explain why signing, notarisation, and stapling, which consistently yield positive responses, still fail to sign a package. I will probably opt for using self-hosted runners, so I can configure everything once via the graphical interface. I’ll keep you updated.\u003C/p>\n\u003Cp>Thanks, Apple, for these weeks of intense fun.\u003C/p>\n\u003Ch2 id=\"update\">Update\u003C/h2>\n\u003Cp>In the last month I was able to complete the project by signing it!\u003C/p>\n\u003Cp>The solution turned out to be using self-hosted runners with a signature-enabled account on them and with the certificates imported BY HAND from xcode.\nDoing it this way, you need (if you want to use a CI/CD system) to configure the machine as a local runner (in the case of github it is a very very simple process) and prepare the environment where the application will be built.\nThis solution is very efficient as well as not very scalable on builds of multiple applications or very very large applications as the time would be quite stretched unless multiple runners are configured on the same repository.\u003C/p>\n\u003Cp>It seems that apple doesn’t really like automations to make our lives a little easier…\u003C/p>",{"headings":1007,"localImagePaths":1033,"remoteImagePaths":1034,"frontmatter":1035,"imagePaths":1039},[1008,1011,1014,1017,1020,1023,1026,1029,1030],{"depth":55,"slug":1009,"text":1010},"macos-code-signing-how-it-works","macOS Code Signing: How It Works",{"depth":59,"slug":1012,"text":1013},"prerequisites","Prerequisites",{"depth":94,"slug":1015,"text":1016},"small-tip","Small Tip",{"depth":59,"slug":1018,"text":1019},"code-signing","Code Signing",{"depth":59,"slug":1021,"text":1022},"notarisation","Notarisation",{"depth":59,"slug":1024,"text":1025},"stapling","Stapling",{"depth":59,"slug":1027,"text":1028},"my-thoughts","My Thoughts",{"depth":59,"slug":91,"text":92},{"depth":59,"slug":1031,"text":1032},"update","Update",[],[],{"title":993,"slug":990,"description":994,"pubDate":1036,"updatedDate":1037,"tags":1038,"coverImage":1002},"Aug 7 2024","Sep 30 2024",[524,910],[],"30/index.md","steamdeck-and-steamos",{"id":1041,"data":1043,"body":1050,"filePath":1051,"assetImports":1052,"digest":1054,"rendered":1055,"legacyId":1082},{"title":1044,"description":1045,"pubDate":1046,"tags":1047,"coverImage":1049},"Steamdeck and SteamOS","The device after ~6 month of use",["Date","2024-10-17T00:00:00.000Z"],[43,1048],"Gaming","__ASTRO_IMAGE_./post_img32.webp","## Steam Deck\n\nAfter a few months following the release of the Steam Deck OLED in November, I finally decided to purchase one for myself this past May. The decision was driven by my need for a portable gaming system, especially for the frequent train rides and long breaks between university classes.\n\nHowever, this device wasn't my first choice. Initially, I planned to buy and mod a Nintendo Switch OLED. It checked all the boxes: a great game library, long battery life, and a good screen, not to mention its lightweight design. But then I discovered the Steam Deck, which completely changed my perception of handheld devices. It offered a versatile system that could be used not only as a console but also as a fully functional computer.\n\n## Gaming Experience\n\nI've always been a PC gamer, ever since I was a kid. I've rarely used controllers—my only experiences were with a PS2 rescued from a landfill and a relative's original Xbox. So, I felt a bit out of my element when I started playing titles that I was used to playing with a keyboard and mouse on a controller. However, I quickly adapted, thanks to the quality of the Steam Deck's controller.\n\nSteam's Big Picture interface is intuitive and easily customizable with the right plugins. It provides a clean view of your library and the store, although there are occasional minor issues. The only real downside I’ve noticed isn’t the console’s power or battery life—it’s the weight. During gaming sessions lasting 1-2 hours, it starts to take a toll on your wrists.\n\n## Performance\n\nOn the hardware side, I have no complaints. Games like Borderlands 3, High on Life, and even Helldivers 2 are absolutely enjoyable on this system, with decent battery life ranging from 1 to 2 hours depending on the game’s demands. With indie games like Industria or Hyper Light Drifter, the battery lasts up to 4 hours without any issues.\n\n## Docked Usage\n\nI've often used the Deck for work-related tasks with a dock connected to two monitors, along with a keyboard and mouse (in desktop mode, of course). Whether I was using an IDE like VSCode (via Flatpak), attending meetings, or responding to emails, it worked flawlessly. The Deck performed excellently, handling everything smoothly without any problems.\n\n## Non-Steam Games\n\nSince the Steam Deck is essentially a Linux machine, you can install any launcher or non-Steam game and add it to \"Game Mode.\" This was one of the biggest reasons I chose to buy it. You can use software like EmuDeck for emulating consoles like the Switch, PS2, PS3, Wii, Xbox 360, and more, or launchers like Heroic to access games from Prime Gaming, GOG, or Epic. This opens the Deck up to a fantastic array of titles, from Mario Kart to Burnout Paradise, not to mention classic arcade games for emulation, and a huge indie game library thanks to Prime Gaming’s monthly freebies.\n\n## Tips and Tricks\n\nSteamOS is an Arch-based but immutable distro, meaning you can’t install packages in the traditional Arch way. Instead, you have to rely on methods like Flatpak, Snap, or AppImage. For my specific use case, I continue to use Flatpak apps, installing anything I can’t find via an Arch Distrobox, giving me a more traditional way to get things done.\n\n## Conclusion\n\nThat’s my personal experience with the Steam Deck. I’ve seen both positive and negative reviews about the device, and I’d say it’s a great compromise between a gaming laptop and a Nintendo Switch, but not much more than that. I wouldn’t replace my laptop or a more powerful home system with it. However, it’s an excellent addition if you want to take your games on the go or have a terminal available without carrying a full laptop around.","src/content/blog/32/index.md",[1053],"./post_img32.webp","a93fefd837f31dde",{"html":1056,"metadata":1057},"\u003Ch2 id=\"steam-deck\">Steam Deck\u003C/h2>\n\u003Cp>After a few months following the release of the Steam Deck OLED in November, I finally decided to purchase one for myself this past May. The decision was driven by my need for a portable gaming system, especially for the frequent train rides and long breaks between university classes.\u003C/p>\n\u003Cp>However, this device wasn’t my first choice. Initially, I planned to buy and mod a Nintendo Switch OLED. It checked all the boxes: a great game library, long battery life, and a good screen, not to mention its lightweight design. But then I discovered the Steam Deck, which completely changed my perception of handheld devices. It offered a versatile system that could be used not only as a console but also as a fully functional computer.\u003C/p>\n\u003Ch2 id=\"gaming-experience\">Gaming Experience\u003C/h2>\n\u003Cp>I’ve always been a PC gamer, ever since I was a kid. I’ve rarely used controllers—my only experiences were with a PS2 rescued from a landfill and a relative’s original Xbox. So, I felt a bit out of my element when I started playing titles that I was used to playing with a keyboard and mouse on a controller. However, I quickly adapted, thanks to the quality of the Steam Deck’s controller.\u003C/p>\n\u003Cp>Steam’s Big Picture interface is intuitive and easily customizable with the right plugins. It provides a clean view of your library and the store, although there are occasional minor issues. The only real downside I’ve noticed isn’t the console’s power or battery life—it’s the weight. During gaming sessions lasting 1-2 hours, it starts to take a toll on your wrists.\u003C/p>\n\u003Ch2 id=\"performance\">Performance\u003C/h2>\n\u003Cp>On the hardware side, I have no complaints. Games like Borderlands 3, High on Life, and even Helldivers 2 are absolutely enjoyable on this system, with decent battery life ranging from 1 to 2 hours depending on the game’s demands. With indie games like Industria or Hyper Light Drifter, the battery lasts up to 4 hours without any issues.\u003C/p>\n\u003Ch2 id=\"docked-usage\">Docked Usage\u003C/h2>\n\u003Cp>I’ve often used the Deck for work-related tasks with a dock connected to two monitors, along with a keyboard and mouse (in desktop mode, of course). Whether I was using an IDE like VSCode (via Flatpak), attending meetings, or responding to emails, it worked flawlessly. The Deck performed excellently, handling everything smoothly without any problems.\u003C/p>\n\u003Ch2 id=\"non-steam-games\">Non-Steam Games\u003C/h2>\n\u003Cp>Since the Steam Deck is essentially a Linux machine, you can install any launcher or non-Steam game and add it to “Game Mode.” This was one of the biggest reasons I chose to buy it. You can use software like EmuDeck for emulating consoles like the Switch, PS2, PS3, Wii, Xbox 360, and more, or launchers like Heroic to access games from Prime Gaming, GOG, or Epic. This opens the Deck up to a fantastic array of titles, from Mario Kart to Burnout Paradise, not to mention classic arcade games for emulation, and a huge indie game library thanks to Prime Gaming’s monthly freebies.\u003C/p>\n\u003Ch2 id=\"tips-and-tricks\">Tips and Tricks\u003C/h2>\n\u003Cp>SteamOS is an Arch-based but immutable distro, meaning you can’t install packages in the traditional Arch way. Instead, you have to rely on methods like Flatpak, Snap, or AppImage. For my specific use case, I continue to use Flatpak apps, installing anything I can’t find via an Arch Distrobox, giving me a more traditional way to get things done.\u003C/p>\n\u003Ch2 id=\"conclusion\">Conclusion\u003C/h2>\n\u003Cp>That’s my personal experience with the Steam Deck. I’ve seen both positive and negative reviews about the device, and I’d say it’s a great compromise between a gaming laptop and a Nintendo Switch, but not much more than that. I wouldn’t replace my laptop or a more powerful home system with it. However, it’s an excellent addition if you want to take your games on the go or have a terminal available without carrying a full laptop around.\u003C/p>",{"headings":1058,"localImagePaths":1076,"remoteImagePaths":1077,"frontmatter":1078,"imagePaths":1081},[1059,1062,1065,1066,1069,1072,1075],{"depth":59,"slug":1060,"text":1061},"steam-deck","Steam Deck",{"depth":59,"slug":1063,"text":1064},"gaming-experience","Gaming Experience",{"depth":59,"slug":79,"text":80},{"depth":59,"slug":1067,"text":1068},"docked-usage","Docked Usage",{"depth":59,"slug":1070,"text":1071},"non-steam-games","Non-Steam Games",{"depth":59,"slug":1073,"text":1074},"tips-and-tricks","Tips and Tricks",{"depth":59,"slug":91,"text":92},[],[],{"title":1044,"slug":1041,"description":1045,"pubDate":1079,"tags":1080,"coverImage":1053},"Oct 17 2024",[43,1048],[],"32/index.md","ecc-ram",{"id":1083,"data":1085,"body":1092,"filePath":1093,"assetImports":1094,"digest":1096,"rendered":1097,"legacyId":1127},{"title":1086,"description":1087,"pubDate":1088,"tags":1089,"coverImage":1091},"ECC Memory","What is, How it works, Where is it used",["Date","2025-01-30T00:00:00.000Z"],[1090],"Hardware","__ASTRO_IMAGE_./post_img34.png","# Why It’s a Big Deal and Why Gamers Don’t Care\n\n## Introduction\n\nEver had your PC crash randomly and thought, “What the heck just happened?” Well, sometimes, it’s a tiny, invisible gremlin flipping a single bit in your RAM. That’s where ECC (Error-Correcting Code) memory comes in. It’s like your RAM but with built-in superpowers to detect and fix errors before they mess up your system. If you’re a server admin, data scientist, or just someone who hates unexplained crashes, you probably already know ECC memory is a big deal. But if you’re a gamer or casual user? Eh, not so much. Let’s break it down.\n\n## Chapter 1: What Even Is ECC Memory?\n\nECC memory is RAM that comes with built-in error detection and correction. Normal RAM just stores data and hopes for the best. ECC RAM actually checks itself for mistakes and fixes minor ones before they cause problems.\n\n**Key Features of ECC Memory**:\n\n - Detects and corrects single-bit errors\n\n - Detects multi-bit errors (but can’t fix them)\n\n - Requires motherboard and CPU support\n\n - Mostly used in servers, workstations, and scientific computing\n\n## Chapter 2: How ECC Memory Works (Without Too Much Math)\n\nAlright, so ECC memory is basically like a grammar-checker for your RAM. Here’s the simple version:\n\n - **Data Encoding**: When your system writes data to RAM, it generates an extra chunk of code that can be used to detect and fix errors.\n\n - **Error Detection**: When reading the data, the system checks that extra code to see if anything got messed up.\n\n - **Error Correction**: If a single-bit error is found, ECC memory just corrects it like nothing happened. If there’s a bigger issue, your system might throw an error or crash instead of silently corrupting data.\n\nFor mission-critical systems, this is a lifesaver. For your gaming rig? Not so much.\n\n## Chapter 3: ECC vs. Non-ECC Memory – Do You Even Need It?\n\nLet’s compare ECC and non-ECC memory side by side:\n\n|Feature|ECC Memory|Non-ECC Memory|\n|-------|----------|--------------|\n|Error Detection|Yes|No|\n|Error Correction|Yes (single-bit errors)||Nope|\n|Performance Impact|Slight slowdown due to extra processing|Slightly faster|\n|Cost|More expensive|Cheaper|\n|Use Case|Servers, workstations, data centers|Gaming, general consumer PCs|\n\n*TL;DR: If you need reliability over everything, go ECC. If you just want max FPS in Call of Duty, stick to regular RAM.*\n\n## Chapter 4: Why ECC Memory is a Must-Have for Servers\n\nSo why does ECC memory dominate in the server world? Because no one wants a tiny RAM error taking down an entire company’s database. Here’s why ECC memory is essential in high-end computing:\n\n1. **Data Integrity**\n\n    One flipped bit could corrupt a database, break an AI model, or ruin critical financial data. ECC prevents that.\n\n2. **System Stability**\n\n    Crashes due to RAM errors? Not in a server farm running ECC.\n\n3. **Uptime and Reliability**\n\n    Servers are expected to run 24/7. ECC helps prevent random failures that could cost thousands in downtime.\n\n4. **Security Benefits**\n\n    Memory corruption isn’t just an inconvenience—sometimes, it’s an exploit. ECC helps mitigate [certain attack](https://en.wikipedia.org/wiki/Row_hammer) vectors by keeping memory errors in check.\n\n## Chapter 5: Why Consumer PCs Don’t Bother with ECC\n\nIf ECC is so great, why don’t gaming PCs and regular desktops use it? Here’s why:\n\n - **It’s Expensive**\n\n    ECC RAM costs more, and you need a compatible motherboard and CPU. That’s extra $$$ for a feature most users don’t need.\n\n - **Slight Performance Penalty**\n\n    ECC has to do extra work to check and correct errors, which can slightly impact speed. In gaming and high-performance consumer applications, every bit of speed counts.\n\n - **Most Consumer Apps Don’t Need It**\n\n    Your Netflix stream won’t die because of a single-bit memory error. Neither will your game saves or Photoshop edits.\n\n - **Hardware Compatibility Issues**\n\n    Many consumer CPUs and motherboards just don’t support ECC. Even if you wanted it, you’d have to build a system specifically for it.\n\n## Conclusion\n\nECC memory is a game-changer for enterprise, science, and mission-critical applications. If you’re running a server, a financial system, or crunching big data, you absolutely need ECC.\nBut if you’re just gaming, browsing Reddit, or editing videos? Save your money, non-ECC memory is just fine.","src/content/blog/34/index.md",[1095],"./post_img34.png","9ba9f810cfeaf372",{"html":1098,"metadata":1099},"\u003Ch1 id=\"why-its-a-big-deal-and-why-gamers-dont-care\">Why It’s a Big Deal and Why Gamers Don’t Care\u003C/h1>\n\u003Ch2 id=\"introduction\">Introduction\u003C/h2>\n\u003Cp>Ever had your PC crash randomly and thought, “What the heck just happened?” Well, sometimes, it’s a tiny, invisible gremlin flipping a single bit in your RAM. That’s where ECC (Error-Correcting Code) memory comes in. It’s like your RAM but with built-in superpowers to detect and fix errors before they mess up your system. If you’re a server admin, data scientist, or just someone who hates unexplained crashes, you probably already know ECC memory is a big deal. But if you’re a gamer or casual user? Eh, not so much. Let’s break it down.\u003C/p>\n\u003Ch2 id=\"chapter-1-what-even-is-ecc-memory\">Chapter 1: What Even Is ECC Memory?\u003C/h2>\n\u003Cp>ECC memory is RAM that comes with built-in error detection and correction. Normal RAM just stores data and hopes for the best. ECC RAM actually checks itself for mistakes and fixes minor ones before they cause problems.\u003C/p>\n\u003Cp>\u003Cstrong>Key Features of ECC Memory\u003C/strong>:\u003C/p>\n\u003Cul>\n\u003Cli>\n\u003Cp>Detects and corrects single-bit errors\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>Detects multi-bit errors (but can’t fix them)\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>Requires motherboard and CPU support\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>Mostly used in servers, workstations, and scientific computing\u003C/p>\n\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"chapter-2-how-ecc-memory-works-without-too-much-math\">Chapter 2: How ECC Memory Works (Without Too Much Math)\u003C/h2>\n\u003Cp>Alright, so ECC memory is basically like a grammar-checker for your RAM. Here’s the simple version:\u003C/p>\n\u003Cul>\n\u003Cli>\n\u003Cp>\u003Cstrong>Data Encoding\u003C/strong>: When your system writes data to RAM, it generates an extra chunk of code that can be used to detect and fix errors.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Error Detection\u003C/strong>: When reading the data, the system checks that extra code to see if anything got messed up.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Error Correction\u003C/strong>: If a single-bit error is found, ECC memory just corrects it like nothing happened. If there’s a bigger issue, your system might throw an error or crash instead of silently corrupting data.\u003C/p>\n\u003C/li>\n\u003C/ul>\n\u003Cp>For mission-critical systems, this is a lifesaver. For your gaming rig? Not so much.\u003C/p>\n\u003Ch2 id=\"chapter-3-ecc-vs-non-ecc-memory--do-you-even-need-it\">Chapter 3: ECC vs. Non-ECC Memory – Do You Even Need It?\u003C/h2>\n\u003Cp>Let’s compare ECC and non-ECC memory side by side:\u003C/p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>Feature\u003C/th>\u003Cth>ECC Memory\u003C/th>\u003Cth>Non-ECC Memory\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>Error Detection\u003C/td>\u003Ctd>Yes\u003C/td>\u003Ctd>No\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Error Correction\u003C/td>\u003Ctd>Yes (single-bit errors)\u003C/td>\u003Ctd>\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Performance Impact\u003C/td>\u003Ctd>Slight slowdown due to extra processing\u003C/td>\u003Ctd>Slightly faster\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Cost\u003C/td>\u003Ctd>More expensive\u003C/td>\u003Ctd>Cheaper\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Use Case\u003C/td>\u003Ctd>Servers, workstations, data centers\u003C/td>\u003Ctd>Gaming, general consumer PCs\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Cp>\u003Cem>TL;DR: If you need reliability over everything, go ECC. If you just want max FPS in Call of Duty, stick to regular RAM.\u003C/em>\u003C/p>\n\u003Ch2 id=\"chapter-4-why-ecc-memory-is-a-must-have-for-servers\">Chapter 4: Why ECC Memory is a Must-Have for Servers\u003C/h2>\n\u003Cp>So why does ECC memory dominate in the server world? Because no one wants a tiny RAM error taking down an entire company’s database. Here’s why ECC memory is essential in high-end computing:\u003C/p>\n\u003Col>\n\u003Cli>\n\u003Cp>\u003Cstrong>Data Integrity\u003C/strong>\u003C/p>\n\u003Cp>One flipped bit could corrupt a database, break an AI model, or ruin critical financial data. ECC prevents that.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>System Stability\u003C/strong>\u003C/p>\n\u003Cp>Crashes due to RAM errors? Not in a server farm running ECC.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Uptime and Reliability\u003C/strong>\u003C/p>\n\u003Cp>Servers are expected to run 24/7. ECC helps prevent random failures that could cost thousands in downtime.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Security Benefits\u003C/strong>\u003C/p>\n\u003Cp>Memory corruption isn’t just an inconvenience—sometimes, it’s an exploit. ECC helps mitigate \u003Ca href=\"https://en.wikipedia.org/wiki/Row_hammer\" target=\"_blank\" rel=\"noopener\">certain attack\u003C/a> vectors by keeping memory errors in check.\u003C/p>\n\u003C/li>\n\u003C/ol>\n\u003Ch2 id=\"chapter-5-why-consumer-pcs-dont-bother-with-ecc\">Chapter 5: Why Consumer PCs Don’t Bother with ECC\u003C/h2>\n\u003Cp>If ECC is so great, why don’t gaming PCs and regular desktops use it? Here’s why:\u003C/p>\n\u003Cul>\n\u003Cli>\n\u003Cp>\u003Cstrong>It’s Expensive\u003C/strong>\u003C/p>\n\u003Cp>ECC RAM costs more, and you need a compatible motherboard and CPU. That’s extra $$$ for a feature most users don’t need.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Slight Performance Penalty\u003C/strong>\u003C/p>\n\u003Cp>ECC has to do extra work to check and correct errors, which can slightly impact speed. In gaming and high-performance consumer applications, every bit of speed counts.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Most Consumer Apps Don’t Need It\u003C/strong>\u003C/p>\n\u003Cp>Your Netflix stream won’t die because of a single-bit memory error. Neither will your game saves or Photoshop edits.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Hardware Compatibility Issues\u003C/strong>\u003C/p>\n\u003Cp>Many consumer CPUs and motherboards just don’t support ECC. Even if you wanted it, you’d have to build a system specifically for it.\u003C/p>\n\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"conclusion\">Conclusion\u003C/h2>\n\u003Cp>ECC memory is a game-changer for enterprise, science, and mission-critical applications. If you’re running a server, a financial system, or crunching big data, you absolutely need ECC.\nBut if you’re just gaming, browsing Reddit, or editing videos? Save your money, non-ECC memory is just fine.\u003C/p>",{"headings":1100,"localImagePaths":1121,"remoteImagePaths":1122,"frontmatter":1123,"imagePaths":1126},[1101,1104,1105,1108,1111,1114,1117,1120],{"depth":55,"slug":1102,"text":1103},"why-its-a-big-deal-and-why-gamers-dont-care","Why It’s a Big Deal and Why Gamers Don’t Care",{"depth":59,"slug":498,"text":499},{"depth":59,"slug":1106,"text":1107},"chapter-1-what-even-is-ecc-memory","Chapter 1: What Even Is ECC Memory?",{"depth":59,"slug":1109,"text":1110},"chapter-2-how-ecc-memory-works-without-too-much-math","Chapter 2: How ECC Memory Works (Without Too Much Math)",{"depth":59,"slug":1112,"text":1113},"chapter-3-ecc-vs-non-ecc-memory--do-you-even-need-it","Chapter 3: ECC vs. Non-ECC Memory – Do You Even Need It?",{"depth":59,"slug":1115,"text":1116},"chapter-4-why-ecc-memory-is-a-must-have-for-servers","Chapter 4: Why ECC Memory is a Must-Have for Servers",{"depth":59,"slug":1118,"text":1119},"chapter-5-why-consumer-pcs-dont-bother-with-ecc","Chapter 5: Why Consumer PCs Don’t Bother with ECC",{"depth":59,"slug":91,"text":92},[],[],{"title":1086,"slug":1083,"description":1087,"pubDate":1124,"tags":1125,"coverImage":1095},"Jan 30 2025",[1090],[],"34/index.md","hypervisors-types",{"id":1128,"data":1130,"body":1137,"filePath":1138,"assetImports":1139,"digest":1141,"rendered":1142,"legacyId":1194},{"title":1131,"description":1132,"pubDate":1133,"tags":1134,"coverImage":1136},"Understanding Hypervisors","Short comparison of hypervisors based on the case of use",["Date","2025-02-13T00:00:00.000Z"],[1135],"Virtualization","__ASTRO_IMAGE_./post_img35.webp","# Demystifying Hypervisors: A Deep Dive into Type 1 and Type 2\n\nIf you're into **virtualization**, you've probably heard the terms **\"Type 1\"** and **\"Type 2\"** hypervisors being thrown around. But what exactly are they, and why should you care? Let's break it down and get into the nitty-gritty details!\n\n---\n## 🖥️ What is a Hypervisor?\n\nBefore diving into the specifics, let's start with the basics.\n\nA **hypervisor**, also known as a **Virtual Machine Monitor (VMM)**, is software that allows you to **create and run** virtual machines (VMs). Think of it as a **magical layer that sits between the hardware and the OS**, enabling multiple VMs to run on a single physical machine. Pretty cool, huh?\n\nHypervisors are the **backbone of virtualization technology**. They allow you to **abstract the physical hardware** and create **isolated environments** where you can run different operating systems and applications. This is incredibly useful for everything from **testing and development** to **resource optimization and disaster recovery**.\n\n---\n## 🏆 Type 1 Hypervisors: The Bare-Metal Beasts\n\n### 🔍 How Do They Work?\n\nType 1 hypervisors, also known as **\"bare-metal\" hypervisors**, run **directly on the host's hardware**—no underlying OS required! This direct interaction means they are **super efficient** and can offer **near-native performance** for the VMs. They act as the traffic cop, allocating CPU, memory, and storage to the various VMs running on the host machine.\n\n### 🔥 Popular Type 1 Hypervisors\n\nOne of the most well-known Type 1 hypervisors is **VMware ESXi**, praised for its robustness and enterprise-grade features. Another big player is **Microsoft Hyper-V**, which is tightly integrated with Windows Server and offers many management features. However, in today's market, **KVM (Kernel-based Virtual Machine)** stands out as a powerful open-source alternative. Many modern Type 1 hypervisors, like **Proxmox** and **AtomOS**, are built upon KVM, making it a go-to choice for flexibility and scalability.\n\n### 🚀 Benefits\n\nSince Type 1 hypervisors run **directly on the hardware**, they offer **excellent performance** and **near-native speeds**. This makes them ideal for resource-intensive applications. Additionally, they provide a **more secure** environment because they don't rely on an underlying OS that could be compromised. Their **efficient resource management** ensures better utilization of hardware, which is crucial for data centers and large-scale deployments.\n\n### ⚠️ Drawbacks\n\nHowever, Type 1 hypervisors aren't without their challenges. Setting up and managing them **can be complex**—you need a solid understanding of both hardware and hypervisor software to maximize their potential. Additionally, **cost** can be a concern, especially for enterprise solutions like VMware ESXi. While open-source alternatives like KVM are available, they **may require more expertise** to configure and maintain properly. Also, Type 1 hypervisors tend to have stricter **hardware compatibility** requirements.\n\n---\n## 🏡 Type 2 Hypervisors: The Hosted Heroes\n\n### 🔍 How Do They Work?\n\nType 2 hypervisors, also known as **\"hosted\" hypervisors**, run **on top of an existing OS**. Unlike their Type 1 counterparts, they **rely on the host OS** to manage hardware interactions, which makes them **easier to set up** but slightly **less efficient**.\n\n### 🔥 Popular Type 2 Hypervisors\n\nA well-known example is **VMware Workstation**, valued for its ease of use and rich feature set. Another widely used hypervisor is **Oracle VirtualBox**, an open-source solution that is **flexible and beginner-friendly**. For Mac users, **Parallels Desktop** is a favorite, providing seamless integration with macOS and excellent performance. A powerful open-source alternative is **QEMU**, which supports a wide variety of CPU architectures but can be tricky for the average user to configure.\n\n### 🚀 Benefits\n\nType 2 hypervisors are **easy to install and use**, making them a great choice for beginners, testers, and developers. They work **on most operating systems**, removing compatibility concerns. Since many Type 2 hypervisors are **free or low-cost**, they are **budget-friendly** options for those looking to explore virtualization.\n\n### ⚠️ Drawbacks\n\nHowever, Type 2 hypervisors come with some trade-offs. Since they depend on the host OS, they are generally **less efficient** than Type 1 hypervisors. The extra **layer of abstraction** impacts performance, especially in high-resource applications. Additionally, they are **more vulnerable** to security risks, as any compromise in the host OS can affect all running VMs. They can also consume **more system resources**, which might be a limitation on machines with low hardware specifications.\n\n---\n## 🤔 Which One Should You Choose?\n\nSo, which type of hypervisor is right for you? It all depends on your **needs** and **use case**.\n\nIf you need **high performance, security, and enterprise-grade virtualization**, go for **Type 1 hypervisors**. They are best suited for **data centers, large-scale deployments, and mission-critical applications**.\n\nOn the other hand, if you're a **developer, tester, or just exploring virtualization**, a **Type 2 hypervisor** is a better fit. They are **easier to set up**, work well for **personal use**, and are **more flexible** for running multiple OS environments on a single machine.\n\n---\n## 🔮 The Future of Hypervisors\n\nThe world of hypervisors is **constantly evolving**. As virtualization technology advances, we can expect even **more powerful and efficient solutions**. Some key trends include:\n\n- **Containers vs. Hypervisors** – While containers (like Docker) provide lightweight alternatives, hypervisors remain essential for **full OS virtualization and isolation**.\n- **Edge Computing & AI** – Hypervisors are playing a crucial role in **managing distributed computing resources** for AI and machine learning workloads.\n- **Hybrid Virtualization Models** – Future hypervisors may integrate **cloud-based and on-prem solutions** for better flexibility.\n\n---\n## 🎯 Wrapping Up\n\nAnd there you have it, a deep dive into **Type 1 and Type 2 hypervisors**! Whether you're a **seasoned IT pro** or just dipping your toes into the world of virtualization, understanding these tools can **make a big difference** in how you manage your computing environments.\n\n🚀 **Now go forth and virtualize!**","src/content/blog/35/index.md",[1140],"./post_img35.webp","02cd9117642a4561",{"html":1143,"metadata":1144},"\u003Ch1 id=\"demystifying-hypervisors-a-deep-dive-into-type-1-and-type-2\">Demystifying Hypervisors: A Deep Dive into Type 1 and Type 2\u003C/h1>\n\u003Cp>If you’re into \u003Cstrong>virtualization\u003C/strong>, you’ve probably heard the terms \u003Cstrong>“Type 1”\u003C/strong> and \u003Cstrong>“Type 2”\u003C/strong> hypervisors being thrown around. But what exactly are they, and why should you care? Let’s break it down and get into the nitty-gritty details!\u003C/p>\n\u003Chr>\n\u003Ch2 id=\"️-what-is-a-hypervisor\">🖥️ What is a Hypervisor?\u003C/h2>\n\u003Cp>Before diving into the specifics, let’s start with the basics.\u003C/p>\n\u003Cp>A \u003Cstrong>hypervisor\u003C/strong>, also known as a \u003Cstrong>Virtual Machine Monitor (VMM)\u003C/strong>, is software that allows you to \u003Cstrong>create and run\u003C/strong> virtual machines (VMs). Think of it as a \u003Cstrong>magical layer that sits between the hardware and the OS\u003C/strong>, enabling multiple VMs to run on a single physical machine. Pretty cool, huh?\u003C/p>\n\u003Cp>Hypervisors are the \u003Cstrong>backbone of virtualization technology\u003C/strong>. They allow you to \u003Cstrong>abstract the physical hardware\u003C/strong> and create \u003Cstrong>isolated environments\u003C/strong> where you can run different operating systems and applications. This is incredibly useful for everything from \u003Cstrong>testing and development\u003C/strong> to \u003Cstrong>resource optimization and disaster recovery\u003C/strong>.\u003C/p>\n\u003Chr>\n\u003Ch2 id=\"-type-1-hypervisors-the-bare-metal-beasts\">🏆 Type 1 Hypervisors: The Bare-Metal Beasts\u003C/h2>\n\u003Ch3 id=\"-how-do-they-work\">🔍 How Do They Work?\u003C/h3>\n\u003Cp>Type 1 hypervisors, also known as \u003Cstrong>“bare-metal” hypervisors\u003C/strong>, run \u003Cstrong>directly on the host’s hardware\u003C/strong>—no underlying OS required! This direct interaction means they are \u003Cstrong>super efficient\u003C/strong> and can offer \u003Cstrong>near-native performance\u003C/strong> for the VMs. They act as the traffic cop, allocating CPU, memory, and storage to the various VMs running on the host machine.\u003C/p>\n\u003Ch3 id=\"-popular-type-1-hypervisors\">🔥 Popular Type 1 Hypervisors\u003C/h3>\n\u003Cp>One of the most well-known Type 1 hypervisors is \u003Cstrong>VMware ESXi\u003C/strong>, praised for its robustness and enterprise-grade features. Another big player is \u003Cstrong>Microsoft Hyper-V\u003C/strong>, which is tightly integrated with Windows Server and offers many management features. However, in today’s market, \u003Cstrong>KVM (Kernel-based Virtual Machine)\u003C/strong> stands out as a powerful open-source alternative. Many modern Type 1 hypervisors, like \u003Cstrong>Proxmox\u003C/strong> and \u003Cstrong>AtomOS\u003C/strong>, are built upon KVM, making it a go-to choice for flexibility and scalability.\u003C/p>\n\u003Ch3 id=\"-benefits\">🚀 Benefits\u003C/h3>\n\u003Cp>Since Type 1 hypervisors run \u003Cstrong>directly on the hardware\u003C/strong>, they offer \u003Cstrong>excellent performance\u003C/strong> and \u003Cstrong>near-native speeds\u003C/strong>. This makes them ideal for resource-intensive applications. Additionally, they provide a \u003Cstrong>more secure\u003C/strong> environment because they don’t rely on an underlying OS that could be compromised. Their \u003Cstrong>efficient resource management\u003C/strong> ensures better utilization of hardware, which is crucial for data centers and large-scale deployments.\u003C/p>\n\u003Ch3 id=\"️-drawbacks\">⚠️ Drawbacks\u003C/h3>\n\u003Cp>However, Type 1 hypervisors aren’t without their challenges. Setting up and managing them \u003Cstrong>can be complex\u003C/strong>—you need a solid understanding of both hardware and hypervisor software to maximize their potential. Additionally, \u003Cstrong>cost\u003C/strong> can be a concern, especially for enterprise solutions like VMware ESXi. While open-source alternatives like KVM are available, they \u003Cstrong>may require more expertise\u003C/strong> to configure and maintain properly. Also, Type 1 hypervisors tend to have stricter \u003Cstrong>hardware compatibility\u003C/strong> requirements.\u003C/p>\n\u003Chr>\n\u003Ch2 id=\"-type-2-hypervisors-the-hosted-heroes\">🏡 Type 2 Hypervisors: The Hosted Heroes\u003C/h2>\n\u003Ch3 id=\"-how-do-they-work-1\">🔍 How Do They Work?\u003C/h3>\n\u003Cp>Type 2 hypervisors, also known as \u003Cstrong>“hosted” hypervisors\u003C/strong>, run \u003Cstrong>on top of an existing OS\u003C/strong>. Unlike their Type 1 counterparts, they \u003Cstrong>rely on the host OS\u003C/strong> to manage hardware interactions, which makes them \u003Cstrong>easier to set up\u003C/strong> but slightly \u003Cstrong>less efficient\u003C/strong>.\u003C/p>\n\u003Ch3 id=\"-popular-type-2-hypervisors\">🔥 Popular Type 2 Hypervisors\u003C/h3>\n\u003Cp>A well-known example is \u003Cstrong>VMware Workstation\u003C/strong>, valued for its ease of use and rich feature set. Another widely used hypervisor is \u003Cstrong>Oracle VirtualBox\u003C/strong>, an open-source solution that is \u003Cstrong>flexible and beginner-friendly\u003C/strong>. For Mac users, \u003Cstrong>Parallels Desktop\u003C/strong> is a favorite, providing seamless integration with macOS and excellent performance. A powerful open-source alternative is \u003Cstrong>QEMU\u003C/strong>, which supports a wide variety of CPU architectures but can be tricky for the average user to configure.\u003C/p>\n\u003Ch3 id=\"-benefits-1\">🚀 Benefits\u003C/h3>\n\u003Cp>Type 2 hypervisors are \u003Cstrong>easy to install and use\u003C/strong>, making them a great choice for beginners, testers, and developers. They work \u003Cstrong>on most operating systems\u003C/strong>, removing compatibility concerns. Since many Type 2 hypervisors are \u003Cstrong>free or low-cost\u003C/strong>, they are \u003Cstrong>budget-friendly\u003C/strong> options for those looking to explore virtualization.\u003C/p>\n\u003Ch3 id=\"️-drawbacks-1\">⚠️ Drawbacks\u003C/h3>\n\u003Cp>However, Type 2 hypervisors come with some trade-offs. Since they depend on the host OS, they are generally \u003Cstrong>less efficient\u003C/strong> than Type 1 hypervisors. The extra \u003Cstrong>layer of abstraction\u003C/strong> impacts performance, especially in high-resource applications. Additionally, they are \u003Cstrong>more vulnerable\u003C/strong> to security risks, as any compromise in the host OS can affect all running VMs. They can also consume \u003Cstrong>more system resources\u003C/strong>, which might be a limitation on machines with low hardware specifications.\u003C/p>\n\u003Chr>\n\u003Ch2 id=\"-which-one-should-you-choose\">🤔 Which One Should You Choose?\u003C/h2>\n\u003Cp>So, which type of hypervisor is right for you? It all depends on your \u003Cstrong>needs\u003C/strong> and \u003Cstrong>use case\u003C/strong>.\u003C/p>\n\u003Cp>If you need \u003Cstrong>high performance, security, and enterprise-grade virtualization\u003C/strong>, go for \u003Cstrong>Type 1 hypervisors\u003C/strong>. They are best suited for \u003Cstrong>data centers, large-scale deployments, and mission-critical applications\u003C/strong>.\u003C/p>\n\u003Cp>On the other hand, if you’re a \u003Cstrong>developer, tester, or just exploring virtualization\u003C/strong>, a \u003Cstrong>Type 2 hypervisor\u003C/strong> is a better fit. They are \u003Cstrong>easier to set up\u003C/strong>, work well for \u003Cstrong>personal use\u003C/strong>, and are \u003Cstrong>more flexible\u003C/strong> for running multiple OS environments on a single machine.\u003C/p>\n\u003Chr>\n\u003Ch2 id=\"-the-future-of-hypervisors\">🔮 The Future of Hypervisors\u003C/h2>\n\u003Cp>The world of hypervisors is \u003Cstrong>constantly evolving\u003C/strong>. As virtualization technology advances, we can expect even \u003Cstrong>more powerful and efficient solutions\u003C/strong>. Some key trends include:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Containers vs. Hypervisors\u003C/strong> – While containers (like Docker) provide lightweight alternatives, hypervisors remain essential for \u003Cstrong>full OS virtualization and isolation\u003C/strong>.\u003C/li>\n\u003Cli>\u003Cstrong>Edge Computing &#x26; AI\u003C/strong> – Hypervisors are playing a crucial role in \u003Cstrong>managing distributed computing resources\u003C/strong> for AI and machine learning workloads.\u003C/li>\n\u003Cli>\u003Cstrong>Hybrid Virtualization Models\u003C/strong> – Future hypervisors may integrate \u003Cstrong>cloud-based and on-prem solutions\u003C/strong> for better flexibility.\u003C/li>\n\u003C/ul>\n\u003Chr>\n\u003Ch2 id=\"-wrapping-up\">🎯 Wrapping Up\u003C/h2>\n\u003Cp>And there you have it, a deep dive into \u003Cstrong>Type 1 and Type 2 hypervisors\u003C/strong>! Whether you’re a \u003Cstrong>seasoned IT pro\u003C/strong> or just dipping your toes into the world of virtualization, understanding these tools can \u003Cstrong>make a big difference\u003C/strong> in how you manage your computing environments.\u003C/p>\n\u003Cp>🚀 \u003Cstrong>Now go forth and virtualize!\u003C/strong>\u003C/p>",{"headings":1145,"localImagePaths":1188,"remoteImagePaths":1189,"frontmatter":1190,"imagePaths":1193},[1146,1149,1152,1155,1158,1161,1164,1167,1170,1172,1175,1177,1179,1182,1185],{"depth":55,"slug":1147,"text":1148},"demystifying-hypervisors-a-deep-dive-into-type-1-and-type-2","Demystifying Hypervisors: A Deep Dive into Type 1 and Type 2",{"depth":59,"slug":1150,"text":1151},"️-what-is-a-hypervisor","🖥️ What is a Hypervisor?",{"depth":59,"slug":1153,"text":1154},"-type-1-hypervisors-the-bare-metal-beasts","🏆 Type 1 Hypervisors: The Bare-Metal Beasts",{"depth":63,"slug":1156,"text":1157},"-how-do-they-work","🔍 How Do They Work?",{"depth":63,"slug":1159,"text":1160},"-popular-type-1-hypervisors","🔥 Popular Type 1 Hypervisors",{"depth":63,"slug":1162,"text":1163},"-benefits","🚀 Benefits",{"depth":63,"slug":1165,"text":1166},"️-drawbacks","⚠️ Drawbacks",{"depth":59,"slug":1168,"text":1169},"-type-2-hypervisors-the-hosted-heroes","🏡 Type 2 Hypervisors: The Hosted Heroes",{"depth":63,"slug":1171,"text":1157},"-how-do-they-work-1",{"depth":63,"slug":1173,"text":1174},"-popular-type-2-hypervisors","🔥 Popular Type 2 Hypervisors",{"depth":63,"slug":1176,"text":1163},"-benefits-1",{"depth":63,"slug":1178,"text":1166},"️-drawbacks-1",{"depth":59,"slug":1180,"text":1181},"-which-one-should-you-choose","🤔 Which One Should You Choose?",{"depth":59,"slug":1183,"text":1184},"-the-future-of-hypervisors","🔮 The Future of Hypervisors",{"depth":59,"slug":1186,"text":1187},"-wrapping-up","🎯 Wrapping Up",[],[],{"title":1131,"slug":1128,"description":1132,"pubDate":1191,"tags":1192,"coverImage":1140},"Feb 13 2025",[1135],[],"35/index.md","first-mastodon",{"id":1195,"data":1197,"body":1204,"filePath":1205,"assetImports":1206,"digest":1208,"rendered":1209,"legacyId":1228},{"title":1198,"description":1199,"pubDate":1200,"tags":1201,"coverImage":1203},"Automatized mastodon update!","Github action now automatize publishing my posts on mastodon",["Date","2025-02-19T00:00:00.000Z"],[1202],"Social","__ASTRO_IMAGE_./post_img36.jpg","# First tests on auto-publishing posts on Mastodon!\nFrom now on, my blog posts will be posted also on mastodon!\n\n## How ?\nif you're interested on how to do this you can check out my [github repository](https://github.com/filippo-ferrando/filippo-ferrando.github.io) and check the file stored in `.github/workflows/publish-to-mastodon`\n\n## Why?\nWell, i get bored easily and this was a great way to spend a boring morning here in Turin!","src/content/blog/36/index.md",[1207],"./post_img36.jpg","bb1027d716dd3093",{"html":1210,"metadata":1211},"\u003Ch1 id=\"first-tests-on-auto-publishing-posts-on-mastodon\">First tests on auto-publishing posts on Mastodon!\u003C/h1>\n\u003Cp>From now on, my blog posts will be posted also on mastodon!\u003C/p>\n\u003Ch2 id=\"how\">How ?\u003C/h2>\n\u003Cp>if you’re interested on how to do this you can check out my \u003Ca href=\"https://github.com/filippo-ferrando/filippo-ferrando.github.io\" target=\"_blank\" rel=\"noopener\">github repository\u003C/a> and check the file stored in \u003Ccode>.github/workflows/publish-to-mastodon\u003C/code>\u003C/p>\n\u003Ch2 id=\"why\">Why?\u003C/h2>\n\u003Cp>Well, i get bored easily and this was a great way to spend a boring morning here in Turin!\u003C/p>",{"headings":1212,"localImagePaths":1222,"remoteImagePaths":1223,"frontmatter":1224,"imagePaths":1227},[1213,1216,1219],{"depth":55,"slug":1214,"text":1215},"first-tests-on-auto-publishing-posts-on-mastodon","First tests on auto-publishing posts on Mastodon!",{"depth":59,"slug":1217,"text":1218},"how","How ?",{"depth":59,"slug":1220,"text":1221},"why","Why?",[],[],{"title":1198,"slug":1195,"description":1199,"pubDate":1225,"tags":1226,"coverImage":1207},"Feb 19 2025",[1202],[],"36/index.md","homelab-tailscale-integration",{"id":1229,"data":1231,"body":1238,"filePath":1239,"assetImports":1240,"digest":1242,"rendered":1243,"legacyId":1262},{"title":1232,"description":1233,"pubDate":1234,"tags":1235,"coverImage":1237},"NAT + Homelab?","Access to your sevices even on heavily natted network",["Date","2025-11-11T00:00:00.000Z"],[272,1236],"Selfhosted","__ASTRO_IMAGE_./post_img39.png","## Tailscale + Homelab?\n\nFor a long time i accessed my local services using Twingate, with it's pros and cons, it's a very straightforward solution, but it has some limitations, like the free plan only allows 3 devices, and the performance is not the best.\nThough a collegue a discovered Tailscale, and i was amazed by it's simplicity and performance, so i decided to give it a try.\nThe limitations of the free plan are accettable for my use case and the idea of using Wireguard as the underlying protocol is very appealing, i like the idea to create a mesh network in my homelab, in case one of my devices goes down, i can still access my services from other devices.\n\n## Setup\n\nIn my homelab i have multiple physical servers (all old x64 machine and a jetson nano) running AlmaLinux with my docker instances, services, ecc.\nI've installed Tailscale via the official repo, following the [official guide](https://tailscale.com/kb/1347/installation) then setup my tailnet making my servers \"expose\" my lan address to be able to access all my hosts and services from outside my home.\n\nI also installed Tailscale on a raspberry pi zero w (DietPI) to have a \"deployable\" node to use when i'm out, this way i can expose the lan were the pi is attached to my tailnet, this way i can use it like a phisical bridge to access resource were i need.\n\n## Really cool feature\n\nAfter a couple of days of trying things, my eyes fell for a really interesting feature, the \"funnel\" option, this allow you to expose a local service to the internet via a Tailscale node, this is really useful when you want to expose a service that is behind a NAT or firewall, without the need to open ports or configure your router, or if you have a heavy NATted network like myself.\nThis is super cool then you want to share files, projects or beta with friends or collegues (with the full control, the services have a manual trigger to start the funnel, so you can decide when to expose the service).","src/content/blog/39/index.md",[1241],"./post_img39.png","65beb64a17925fe6",{"html":1244,"metadata":1245},"\u003Ch2 id=\"tailscale--homelab\">Tailscale + Homelab?\u003C/h2>\n\u003Cp>For a long time i accessed my local services using Twingate, with it’s pros and cons, it’s a very straightforward solution, but it has some limitations, like the free plan only allows 3 devices, and the performance is not the best.\nThough a collegue a discovered Tailscale, and i was amazed by it’s simplicity and performance, so i decided to give it a try.\nThe limitations of the free plan are accettable for my use case and the idea of using Wireguard as the underlying protocol is very appealing, i like the idea to create a mesh network in my homelab, in case one of my devices goes down, i can still access my services from other devices.\u003C/p>\n\u003Ch2 id=\"setup\">Setup\u003C/h2>\n\u003Cp>In my homelab i have multiple physical servers (all old x64 machine and a jetson nano) running AlmaLinux with my docker instances, services, ecc.\nI’ve installed Tailscale via the official repo, following the \u003Ca href=\"https://tailscale.com/kb/1347/installation\" target=\"_blank\" rel=\"noopener\">official guide\u003C/a> then setup my tailnet making my servers “expose” my lan address to be able to access all my hosts and services from outside my home.\u003C/p>\n\u003Cp>I also installed Tailscale on a raspberry pi zero w (DietPI) to have a “deployable” node to use when i’m out, this way i can expose the lan were the pi is attached to my tailnet, this way i can use it like a phisical bridge to access resource were i need.\u003C/p>\n\u003Ch2 id=\"really-cool-feature\">Really cool feature\u003C/h2>\n\u003Cp>After a couple of days of trying things, my eyes fell for a really interesting feature, the “funnel” option, this allow you to expose a local service to the internet via a Tailscale node, this is really useful when you want to expose a service that is behind a NAT or firewall, without the need to open ports or configure your router, or if you have a heavy NATted network like myself.\nThis is super cool then you want to share files, projects or beta with friends or collegues (with the full control, the services have a manual trigger to start the funnel, so you can decide when to expose the service).\u003C/p>",{"headings":1246,"localImagePaths":1256,"remoteImagePaths":1257,"frontmatter":1258,"imagePaths":1261},[1247,1250,1253],{"depth":59,"slug":1248,"text":1249},"tailscale--homelab","Tailscale + Homelab?",{"depth":59,"slug":1251,"text":1252},"setup","Setup",{"depth":59,"slug":1254,"text":1255},"really-cool-feature","Really cool feature",[],[],{"title":1232,"slug":1229,"description":1233,"pubDate":1259,"tags":1260,"coverImage":1241},"Nov 11 2025",[272,1236],[],"39/index.md","endeavouros-after-3-years",{"id":1263,"data":1265,"body":1271,"filePath":1272,"assetImports":1273,"digest":1275,"rendered":1276,"legacyId":1299},{"title":1266,"description":1267,"pubDate":1268,"tags":1269,"coverImage":1270},"EnadeavourOS | After 3 years","Opinions, merits and demerits of a distro that I have been using stably on all my devices for 3 years now",["Date","2023-01-18T00:00:00.000Z"],[43],"__ASTRO_IMAGE_./post_img4.jpg","# Endeavour OS | After 3 years as a daily driver\n\nOpinions, merits and demerits of a distro that I have been using stably on all my devices for 3 years now\n## Strengths and weaknesses\nEndeavourOS is an Arch-based distro that replaces what was Antergos, now deprecated.\n\nThis is one of the distros that stands as a \"simplified\" version of vanilla Arch, the installer is graphical, we find preinstalled all the necessary packages, from a DE to the browser.\n\nDespite this, under the hood we always find the usual arch package manager and a lot of very nice things.\n\nEndeavour in the installer offers the option for an installation on BTRFS file system (I will do a post later on this topic) which offers several advantages for the user.\n\nAn interesting treat we find is a kernel manager with a GUI that allows you to install zen, hardened, LTS and RT kernels.\n\nTalking about the downsides I haven't found any yet, except endeavouros repos that are not always up-to-date and few other things like some stutter during boot and not-so-fast boot times.\n \n## Personal opinions\nAfter 3 years of use this distro proved to be extremely customizable and perfectly suited to the use I had to (and must) make of it, having said that it is definitely not a distro that is good to start with, in fact it requires some experience on the part of the user to function at its best.\n\nI think the Endeavour team has done a great job with this project, making Arch more accessible to the public by having all the common issues with that particular distro avoided.\n\nHaving said that I can only give a more than positive opinion and recommend it to any of you who want to try a bleeding-edge distro without necessarily having to deal with long and complicated installations or too difficult configurations\n\n## Links\nIf you would like to learn more about this project, I leave some useful links:\n- [EndeavourOs website](https://endeavouros.com/)\n- [Dedicated subreddit](https://www.reddit.com/r/EndeavourOS/)\n- [A review by someone more professional](https://www.youtube.com/watch?v=gGeA7QQIfp4)\n#### Authors\n\n- [@filippo-ferrando](https://www.github.com/filippo-ferrando)","src/content/blog/4/index.md",[1274],"./post_img4.jpg","9522e3c3a25e0d14",{"html":1277,"metadata":1278},"\u003Ch1 id=\"endeavour-os--after-3-years-as-a-daily-driver\">Endeavour OS | After 3 years as a daily driver\u003C/h1>\n\u003Cp>Opinions, merits and demerits of a distro that I have been using stably on all my devices for 3 years now\u003C/p>\n\u003Ch2 id=\"strengths-and-weaknesses\">Strengths and weaknesses\u003C/h2>\n\u003Cp>EndeavourOS is an Arch-based distro that replaces what was Antergos, now deprecated.\u003C/p>\n\u003Cp>This is one of the distros that stands as a “simplified” version of vanilla Arch, the installer is graphical, we find preinstalled all the necessary packages, from a DE to the browser.\u003C/p>\n\u003Cp>Despite this, under the hood we always find the usual arch package manager and a lot of very nice things.\u003C/p>\n\u003Cp>Endeavour in the installer offers the option for an installation on BTRFS file system (I will do a post later on this topic) which offers several advantages for the user.\u003C/p>\n\u003Cp>An interesting treat we find is a kernel manager with a GUI that allows you to install zen, hardened, LTS and RT kernels.\u003C/p>\n\u003Cp>Talking about the downsides I haven’t found any yet, except endeavouros repos that are not always up-to-date and few other things like some stutter during boot and not-so-fast boot times.\u003C/p>\n\u003Ch2 id=\"personal-opinions\">Personal opinions\u003C/h2>\n\u003Cp>After 3 years of use this distro proved to be extremely customizable and perfectly suited to the use I had to (and must) make of it, having said that it is definitely not a distro that is good to start with, in fact it requires some experience on the part of the user to function at its best.\u003C/p>\n\u003Cp>I think the Endeavour team has done a great job with this project, making Arch more accessible to the public by having all the common issues with that particular distro avoided.\u003C/p>\n\u003Cp>Having said that I can only give a more than positive opinion and recommend it to any of you who want to try a bleeding-edge distro without necessarily having to deal with long and complicated installations or too difficult configurations\u003C/p>\n\u003Ch2 id=\"links\">Links\u003C/h2>\n\u003Cp>If you would like to learn more about this project, I leave some useful links:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://endeavouros.com/\" target=\"_blank\" rel=\"noopener\">EndeavourOs website\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://www.reddit.com/r/EndeavourOS/\" target=\"_blank\" rel=\"noopener\">Dedicated subreddit\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://www.youtube.com/watch?v=gGeA7QQIfp4\" target=\"_blank\" rel=\"noopener\">A review by someone more professional\u003C/a>\u003C/li>\n\u003C/ul>\n\u003Ch4 id=\"authors\">Authors\u003C/h4>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://www.github.com/filippo-ferrando\" target=\"_blank\" rel=\"noopener\">@filippo-ferrando\u003C/a>\u003C/li>\n\u003C/ul>",{"headings":1279,"localImagePaths":1293,"remoteImagePaths":1294,"frontmatter":1295,"imagePaths":1298},[1280,1283,1286,1289,1292],{"depth":55,"slug":1281,"text":1282},"endeavour-os--after-3-years-as-a-daily-driver","Endeavour OS | After 3 years as a daily driver",{"depth":59,"slug":1284,"text":1285},"strengths-and-weaknesses","Strengths and weaknesses",{"depth":59,"slug":1287,"text":1288},"personal-opinions","Personal opinions",{"depth":59,"slug":1290,"text":1291},"links","Links",{"depth":94,"slug":95,"text":96},[],[],{"title":1266,"slug":1263,"description":1267,"tags":1296,"pubDate":1297,"coverImage":1274},[43],"Jan 18 2023",[],"4/index.md","selfhosting-2",{"id":1300,"data":1302,"body":1308,"filePath":1309,"assetImports":1310,"digest":1312,"rendered":1313,"legacyId":1339},{"title":1303,"description":1304,"pubDate":1305,"tags":1306,"coverImage":1307},"Selfhosting #2","Series of n articles in which I will show my homelab, from the hardware to the software.",["Date","2023-01-23T00:00:00.000Z"],[1236,821],"__ASTRO_IMAGE_./post_img6.png","# Selfhosting | My experience pt.2\n\nSeries of n articles in which I will show my homelab, from the hardware to the software.\n\n## Software\nAs i mentioned in the previos article, i have two machines in my homelab, a proper pc and a Raspberry pi (which we'll going to see in the next article).\n\nI wnat to show you the various container i use on my system and the proper OS i've instlled on this particular machine\n\n### OMV\nOpenMediaVault is the distro that perfectly suits my need as it has a great disk management UI that permit to create create, delete, mount and dismount all the file system present, it can also create and manage RAIDS very well and a lot of other things (you can find out on they're [website](link)).\n\nFor OMV exist a package called [omv-extras](link) that enable on-UI containerizing feature like a docker installer, a portainer installer to get started with cointainers in a matter of few minutes.\n\nWhy i opted for this distro? I finded it more confortable for managing all the disks i have putted inside this machines, also it is a straight debian-based distro, so it should be more stable than other like Ubuntu or other alternative.\n\n### Containers\nThis machines serves 2 different scopes:\n1. NAS\n2. Homelab\n\nGiven that i don't like to fuck up things directly on the base system, i use containers to do experiments os host web-app as they don't require to install nothing on the base os except for docker.\n\nSo witch conteiners i'm running right now? I'll make a list with links and an a brief explanation for you:\n- [Heimdall](https://heimdall.site/) My favourite dashboard\n- [Emby](https://emby.media/) Media server A.K.A selfhosted netflix\n- [code-server](https://code.visualstudio.com/docs/remote/vscode-server) Instance of a containerized Vs-code accesible from the web\n- [uptime-kuma](https://uptime.kuma.pet/) Monitoring tool\n- [portainer](https://www.portainer.io/) Container manager\n- [yatch](https://yacht.sh/) Another (faster, but with less feature) container manager \n- [fresh-rss](https://freshrss.org/) rss news aggregator\n- [paperless-ng](https://github.com/jonaswinkler/paperless-ng)\n- [taisun](https://www.taisun.io/) Docker server management\n- [kasm](https://www.kasmweb.com/) Streaming containerized apps and desktops to end-users\n\n### Coming up next\n\nIn the next part we will explore what's on my Raspberry Pi\n\n#### Authors\n\n- [@filippo-ferrando](https://www.github.com/filippo-ferrando)","src/content/blog/6/index.md",[1311],"./post_img6.png","1ebb3eba30cf3731",{"html":1314,"metadata":1315},"\u003Ch1 id=\"selfhosting--my-experience-pt2\">Selfhosting | My experience pt.2\u003C/h1>\n\u003Cp>Series of n articles in which I will show my homelab, from the hardware to the software.\u003C/p>\n\u003Ch2 id=\"software\">Software\u003C/h2>\n\u003Cp>As i mentioned in the previos article, i have two machines in my homelab, a proper pc and a Raspberry pi (which we’ll going to see in the next article).\u003C/p>\n\u003Cp>I wnat to show you the various container i use on my system and the proper OS i’ve instlled on this particular machine\u003C/p>\n\u003Ch3 id=\"omv\">OMV\u003C/h3>\n\u003Cp>OpenMediaVault is the distro that perfectly suits my need as it has a great disk management UI that permit to create create, delete, mount and dismount all the file system present, it can also create and manage RAIDS very well and a lot of other things (you can find out on they’re \u003Ca href=\"link\">website\u003C/a>).\u003C/p>\n\u003Cp>For OMV exist a package called \u003Ca href=\"link\">omv-extras\u003C/a> that enable on-UI containerizing feature like a docker installer, a portainer installer to get started with cointainers in a matter of few minutes.\u003C/p>\n\u003Cp>Why i opted for this distro? I finded it more confortable for managing all the disks i have putted inside this machines, also it is a straight debian-based distro, so it should be more stable than other like Ubuntu or other alternative.\u003C/p>\n\u003Ch3 id=\"containers\">Containers\u003C/h3>\n\u003Cp>This machines serves 2 different scopes:\u003C/p>\n\u003Col>\n\u003Cli>NAS\u003C/li>\n\u003Cli>Homelab\u003C/li>\n\u003C/ol>\n\u003Cp>Given that i don’t like to fuck up things directly on the base system, i use containers to do experiments os host web-app as they don’t require to install nothing on the base os except for docker.\u003C/p>\n\u003Cp>So witch conteiners i’m running right now? I’ll make a list with links and an a brief explanation for you:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://heimdall.site/\" target=\"_blank\" rel=\"noopener\">Heimdall\u003C/a> My favourite dashboard\u003C/li>\n\u003Cli>\u003Ca href=\"https://emby.media/\" target=\"_blank\" rel=\"noopener\">Emby\u003C/a> Media server A.K.A selfhosted netflix\u003C/li>\n\u003Cli>\u003Ca href=\"https://code.visualstudio.com/docs/remote/vscode-server\" target=\"_blank\" rel=\"noopener\">code-server\u003C/a> Instance of a containerized Vs-code accesible from the web\u003C/li>\n\u003Cli>\u003Ca href=\"https://uptime.kuma.pet/\" target=\"_blank\" rel=\"noopener\">uptime-kuma\u003C/a> Monitoring tool\u003C/li>\n\u003Cli>\u003Ca href=\"https://www.portainer.io/\" target=\"_blank\" rel=\"noopener\">portainer\u003C/a> Container manager\u003C/li>\n\u003Cli>\u003Ca href=\"https://yacht.sh/\" target=\"_blank\" rel=\"noopener\">yatch\u003C/a> Another (faster, but with less feature) container manager\u003C/li>\n\u003Cli>\u003Ca href=\"https://freshrss.org/\" target=\"_blank\" rel=\"noopener\">fresh-rss\u003C/a> rss news aggregator\u003C/li>\n\u003Cli>\u003Ca href=\"https://github.com/jonaswinkler/paperless-ng\" target=\"_blank\" rel=\"noopener\">paperless-ng\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://www.taisun.io/\" target=\"_blank\" rel=\"noopener\">taisun\u003C/a> Docker server management\u003C/li>\n\u003Cli>\u003Ca href=\"https://www.kasmweb.com/\" target=\"_blank\" rel=\"noopener\">kasm\u003C/a> Streaming containerized apps and desktops to end-users\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"coming-up-next\">Coming up next\u003C/h3>\n\u003Cp>In the next part we will explore what’s on my Raspberry Pi\u003C/p>\n\u003Ch4 id=\"authors\">Authors\u003C/h4>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://www.github.com/filippo-ferrando\" target=\"_blank\" rel=\"noopener\">@filippo-ferrando\u003C/a>\u003C/li>\n\u003C/ul>",{"headings":1316,"localImagePaths":1333,"remoteImagePaths":1334,"frontmatter":1335,"imagePaths":1338},[1317,1320,1323,1326,1329,1332],{"depth":55,"slug":1318,"text":1319},"selfhosting--my-experience-pt2","Selfhosting | My experience pt.2",{"depth":59,"slug":1321,"text":1322},"software","Software",{"depth":63,"slug":1324,"text":1325},"omv","OMV",{"depth":63,"slug":1327,"text":1328},"containers","Containers",{"depth":63,"slug":1330,"text":1331},"coming-up-next","Coming up next",{"depth":94,"slug":95,"text":96},[],[],{"title":1303,"slug":1300,"description":1304,"tags":1336,"pubDate":1337,"coverImage":1311},[1236,821],"Jan 23 2023",[],"6/index.md","selfhosting-1",{"id":1340,"data":1342,"body":1347,"filePath":1348,"assetImports":1349,"digest":1351,"rendered":1352,"legacyId":1369},{"title":1343,"description":1304,"pubDate":1344,"tags":1345,"coverImage":1346},"Selfhosting #1",["Date","2023-01-22T00:00:00.000Z"],[1236,1090],"__ASTRO_IMAGE_./post_img5.jpg","# Selfhosting | My experience pt.1\n\nSeries of n articles in which I will show my homelab, from the hardware to the software.\n\n## Hardware\n\nWhen I started this project, so almost 5 years ago, I had almost no components at my disposal, so my rack consisted of an aluminum cabinet from IKEA where I had housed a very old desktop computer (with an Intel Celeron and 2gb RAM) and a 10/100 switch salvaged from a landfill.\n\nOver time I have salvaged here and there almost everything I have now, starting with the rack cabinet that was salvaged thanks to a small company in the place where I live that was getting rid of it, an HP gigabit switch that was given up for broken (actually it was just to be cleaned), a desktop computer with an i7-4790 and 16gb of RAM that may not sound like much, but for now it performs excellently everything I need to do with relatively low power consumption.\n\nTo complete the setup I included a Raspberry Pi that acts as a sd-blocker with Pi-Hole and could not miss a Mikrotik hAp to generate a secondary wifi network starting from the main one at my home\n\nIn short the components are these:\n - Raspberry pi 3b\n - Mikrotik hAp\n - Server (Intel i7 4790, 16gbram, 4tb main raid, 1tb second raid)\n - Hp ProCruve 2510 gigabit switch\n - 3com 10/100 switch\n - tp-link 5 port gigabit switch\n - 24 port patch panel\n - 2x 200 Aerocool fan\n - Aliexpress 12v fan controller\n\n\nI leave some pictures of how it came to the whole\n\n![front](https://gist.githubusercontent.com/filippo-ferrando/aa4de30d03f228cd72f3f62ac6a56abf/raw/492866425e8da4c0833e9df5a30797803ebd391c/front.jpg)\n\n![rear](https://gist.githubusercontent.com/filippo-ferrando/aa4de30d03f228cd72f3f62ac6a56abf/raw/492866425e8da4c0833e9df5a30797803ebd391c/rear.jpg)\n\n![inside-1](https://gist.githubusercontent.com/filippo-ferrando/aa4de30d03f228cd72f3f62ac6a56abf/raw/492866425e8da4c0833e9df5a30797803ebd391c/inside-1.jpg)\n\n![inside-2](https://gist.githubusercontent.com/filippo-ferrando/aa4de30d03f228cd72f3f62ac6a56abf/raw/492866425e8da4c0833e9df5a30797803ebd391c/inside-2.jpg)\n\nI forgot to mention the 2 cheapest 200 fans on amazon and a fan-controller picked up from aliexpress to provide some air recirculation (especially for the switches)\n\n\n### Coming up next\n\nIn the next part I will appronfond everything that is installed on my main server\n\n#### Authors\n\n- [@filippo-ferrando](https://www.github.com/filippo-ferrando)","src/content/blog/5/index.md",[1350],"./post_img5.jpg","873de1fe938f12fb",{"html":1353,"metadata":1354},"\u003Ch1 id=\"selfhosting--my-experience-pt1\">Selfhosting | My experience pt.1\u003C/h1>\n\u003Cp>Series of n articles in which I will show my homelab, from the hardware to the software.\u003C/p>\n\u003Ch2 id=\"hardware\">Hardware\u003C/h2>\n\u003Cp>When I started this project, so almost 5 years ago, I had almost no components at my disposal, so my rack consisted of an aluminum cabinet from IKEA where I had housed a very old desktop computer (with an Intel Celeron and 2gb RAM) and a 10/100 switch salvaged from a landfill.\u003C/p>\n\u003Cp>Over time I have salvaged here and there almost everything I have now, starting with the rack cabinet that was salvaged thanks to a small company in the place where I live that was getting rid of it, an HP gigabit switch that was given up for broken (actually it was just to be cleaned), a desktop computer with an i7-4790 and 16gb of RAM that may not sound like much, but for now it performs excellently everything I need to do with relatively low power consumption.\u003C/p>\n\u003Cp>To complete the setup I included a Raspberry Pi that acts as a sd-blocker with Pi-Hole and could not miss a Mikrotik hAp to generate a secondary wifi network starting from the main one at my home\u003C/p>\n\u003Cp>In short the components are these:\u003C/p>\n\u003Cul>\n\u003Cli>Raspberry pi 3b\u003C/li>\n\u003Cli>Mikrotik hAp\u003C/li>\n\u003Cli>Server (Intel i7 4790, 16gbram, 4tb main raid, 1tb second raid)\u003C/li>\n\u003Cli>Hp ProCruve 2510 gigabit switch\u003C/li>\n\u003Cli>3com 10/100 switch\u003C/li>\n\u003Cli>tp-link 5 port gigabit switch\u003C/li>\n\u003Cli>24 port patch panel\u003C/li>\n\u003Cli>2x 200 Aerocool fan\u003C/li>\n\u003Cli>Aliexpress 12v fan controller\u003C/li>\n\u003C/ul>\n\u003Cp>I leave some pictures of how it came to the whole\u003C/p>\n\u003Cp>\u003Cimg src=\"https://gist.githubusercontent.com/filippo-ferrando/aa4de30d03f228cd72f3f62ac6a56abf/raw/492866425e8da4c0833e9df5a30797803ebd391c/front.jpg\" alt=\"front\">\u003C/p>\n\u003Cp>\u003Cimg src=\"https://gist.githubusercontent.com/filippo-ferrando/aa4de30d03f228cd72f3f62ac6a56abf/raw/492866425e8da4c0833e9df5a30797803ebd391c/rear.jpg\" alt=\"rear\">\u003C/p>\n\u003Cp>\u003Cimg src=\"https://gist.githubusercontent.com/filippo-ferrando/aa4de30d03f228cd72f3f62ac6a56abf/raw/492866425e8da4c0833e9df5a30797803ebd391c/inside-1.jpg\" alt=\"inside-1\">\u003C/p>\n\u003Cp>\u003Cimg src=\"https://gist.githubusercontent.com/filippo-ferrando/aa4de30d03f228cd72f3f62ac6a56abf/raw/492866425e8da4c0833e9df5a30797803ebd391c/inside-2.jpg\" alt=\"inside-2\">\u003C/p>\n\u003Cp>I forgot to mention the 2 cheapest 200 fans on amazon and a fan-controller picked up from aliexpress to provide some air recirculation (especially for the switches)\u003C/p>\n\u003Ch3 id=\"coming-up-next\">Coming up next\u003C/h3>\n\u003Cp>In the next part I will appronfond everything that is installed on my main server\u003C/p>\n\u003Ch4 id=\"authors\">Authors\u003C/h4>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://www.github.com/filippo-ferrando\" target=\"_blank\" rel=\"noopener\">@filippo-ferrando\u003C/a>\u003C/li>\n\u003C/ul>",{"headings":1355,"localImagePaths":1363,"remoteImagePaths":1364,"frontmatter":1365,"imagePaths":1368},[1356,1359,1361,1362],{"depth":55,"slug":1357,"text":1358},"selfhosting--my-experience-pt1","Selfhosting | My experience pt.1",{"depth":59,"slug":1360,"text":1090},"hardware",{"depth":63,"slug":1330,"text":1331},{"depth":94,"slug":95,"text":96},[],[],{"title":1343,"slug":1340,"description":1304,"tags":1366,"pubDate":1367,"coverImage":1350},[1236,1090],"Jan 22 2023",[],"5/index.md","selfhosting-3",{"id":1370,"data":1372,"body":1377,"filePath":1378,"assetImports":1379,"digest":1381,"rendered":1382,"legacyId":1403},{"title":1373,"description":1304,"pubDate":1374,"tags":1375,"coverImage":1376},"Selfhosting #3",["Date","2023-01-24T00:00:00.000Z"],[1236,856],"__ASTRO_IMAGE_./post_img7.png","# Selfhosting | My experience pt.3\n\nSeries of n articles in which I will show my homelab, from the hardware to the software.\n\n## Raspberry Pi\nMy raspberry is my always-on machine, it host a Pi-hole instance and a Pi.alert.\n\nBut why i use a secondary pc for this service? It's really simple, consumptions!\n\nThat's right, my server isn't powered on all the time due to it's electrical consumption, it is programmed to turn itself on at 10 Am and poweroff at 1 Am, mainly beacuse i don't really need it in that moment of the day.\nBut some service requires to stay active 24/7, like the DNS (pi-hole) and the network scanner (pi alert), so i had to use a separate machine that only consumes few watts of power.\n\n### Services\n[Pi-hole](https://pi-hole.net/): network-wide ad-blocker\n[Pi-alert](https://github.com/pucherot/Pi.Alert): network devices scanner\n\n\n### Coming up next\n\nIn the next episode i will explain how i use my homelab services from outside my home-network\n\n#### Authors\n\n- [@filippo-ferrando](https://www.github.com/filippo-ferrando)","src/content/blog/7/index.md",[1380],"./post_img7.png","b0eedc49fab655b8",{"html":1383,"metadata":1384},"\u003Ch1 id=\"selfhosting--my-experience-pt3\">Selfhosting | My experience pt.3\u003C/h1>\n\u003Cp>Series of n articles in which I will show my homelab, from the hardware to the software.\u003C/p>\n\u003Ch2 id=\"raspberry-pi\">Raspberry Pi\u003C/h2>\n\u003Cp>My raspberry is my always-on machine, it host a Pi-hole instance and a Pi.alert.\u003C/p>\n\u003Cp>But why i use a secondary pc for this service? It’s really simple, consumptions!\u003C/p>\n\u003Cp>That’s right, my server isn’t powered on all the time due to it’s electrical consumption, it is programmed to turn itself on at 10 Am and poweroff at 1 Am, mainly beacuse i don’t really need it in that moment of the day.\nBut some service requires to stay active 24/7, like the DNS (pi-hole) and the network scanner (pi alert), so i had to use a separate machine that only consumes few watts of power.\u003C/p>\n\u003Ch3 id=\"services\">Services\u003C/h3>\n\u003Cp>\u003Ca href=\"https://pi-hole.net/\" target=\"_blank\" rel=\"noopener\">Pi-hole\u003C/a>: network-wide ad-blocker\n\u003Ca href=\"https://github.com/pucherot/Pi.Alert\" target=\"_blank\" rel=\"noopener\">Pi-alert\u003C/a>: network devices scanner\u003C/p>\n\u003Ch3 id=\"coming-up-next\">Coming up next\u003C/h3>\n\u003Cp>In the next episode i will explain how i use my homelab services from outside my home-network\u003C/p>\n\u003Ch4 id=\"authors\">Authors\u003C/h4>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://www.github.com/filippo-ferrando\" target=\"_blank\" rel=\"noopener\">@filippo-ferrando\u003C/a>\u003C/li>\n\u003C/ul>",{"headings":1385,"localImagePaths":1397,"remoteImagePaths":1398,"frontmatter":1399,"imagePaths":1402},[1386,1389,1392,1395,1396],{"depth":55,"slug":1387,"text":1388},"selfhosting--my-experience-pt3","Selfhosting | My experience pt.3",{"depth":59,"slug":1390,"text":1391},"raspberry-pi","Raspberry Pi",{"depth":63,"slug":1393,"text":1394},"services","Services",{"depth":63,"slug":1330,"text":1331},{"depth":94,"slug":95,"text":96},[],[],{"title":1373,"slug":1370,"description":1304,"tags":1400,"pubDate":1401,"coverImage":1380},[1236,856],"Jan 24 2023",[],"7/index.md","cfs-vs-sched-deadline",{"id":1404,"data":1406,"body":1412,"filePath":1413,"assetImports":1414,"digest":1416,"rendered":1417,"legacyId":1468},{"title":1407,"description":1408,"pubDate":1409,"tags":1410,"coverImage":1411},"CFS vs SCHED_DEADLINE","Technical overview over linux scheduler",["Date","2024-11-26T00:00:00.000Z"],[43],"__ASTRO_IMAGE_./post_img33.webp","## CFS\n\nWhen we talk about schedulers, the default choice for non-real-time processes in most Linux distributions is the CFS (Completely Fair Scheduler). \nIts operations are based on the concept of fairness, aiming to allocate CPU time to each process proportionally to their priorities (nice value).\n\n### Operating Principles\n\n- **Balancing CPU Usage**: CFS maintains an \"ideal view\" where each process receives its fair share of CPU time. It uses a red-black tree (an ordered data structure) to organize processes based on their accumulated virtual runtime.\n- **Virtual Runtime**: This time accounts for priorities; a process with higher priority increments its virtual runtime more slowly, thereby receiving more CPU time.\n- **Continuous Scheduling**: CFS avoids rigid interruptions, distributing available time among processes without fixed time slices, instead aiming for theoretical fairness.\n\n#### Best Cases\n\n- **Balanced Workloads**:\n    - When all processes have similar priorities and moderate tasks, CFS performs optimally.\n    - Example: Five processes with identical priorities performing lightweight operations like file read/write. CFS distributes CPU time fairly and smoothly.\n\n- **Interactivity**:\n    - For interactive applications like a text editor or a web browser, CFS excels by prioritizing short processes waiting for user input, ensuring a quick response.\n\n#### Worst Cases\n\n- **Processes with Large Priority Disparities**:\n    - Example: A low-priority process (nice +19) running alongside a high-priority process (nice -20). The low-priority process may suffer from starvation, receiving minimal CPU time.\n    - Applications: A background backup process might take much longer than expected if a high-priority process monopolizes the CPU.\n\n- **Real-Time Workloads**:\n    - CFS is not designed to handle strict deadlines.\n    - Example: An industrial robot control system. Even with high-priority processes, CFS cannot guarantee task completion within precise deadlines.\n\n- **High Process Count**:\n    - With thousands of active processes, managing the red-black tree can introduce overhead, reducing overall efficiency.\n\n### Nicing\n\nNicing is the mechanism Linux uses to influence a process's priority in the operating system. It is based on a value called the nice value, which represents how \"kind\" a process is in sharing CPU resources with others.\n\n#### How It Works\n\n- **Nice Value Range**:\n    - The nice value ranges from -20 (highest priority) to +19 (lowest priority).\n    - By default, a process starts with a nice value of 0, indicating normal priority.\n\n- **Impact of Nice Value**:\n    - Processes with a lower nice value get more CPU time.\n    - Processes with a higher nice value receive less CPU time, yielding resources to higher-priority processes.\n\n- **Effect on Virtual Runtime**:\n    - CFS uses the nice value to adjust the rate of virtual runtime progression: processes with low nice values increment virtual runtime more slowly, gaining more CPU time, and vice versa.\n\n#### Interacting with the Nice Value\n\n- **Initial Assignment**: You can set a process's nice value when launching it with the command:\n\n    ```bash\n    nice -n \u003Cnice_value> \u003Ccommand>\n    ```\n\n    Example: `nice -n 10 ./my_program`\n\n- **Dynamic Modification**: To change the nice value of an already running process, use:\n\n    ```bash\n    renice \u003Cnice_value> -p \u003CPID>\n    ```\n\n    Example: renice -5 -p 1234 (decreases the nice value, increasing priority).\n\n- **Practical Examples**\n\n    - Background Backups: Assign a high nice value (e.g., +19) to a backup process to ensure it has low priority and does not interfere with other activities.\n    - Critical Applications: Assign a low nice value (e.g., -10) to important processes like a web server to improve their responsiveness.\n\nnicing is a simple yet powerful system for controlling how processes share CPU time, adapting to the priority needs of various applications.\n\n### In Summary\n\nCFS is excellent for general usage scenarios that require high interactivity and fairness among processes. It is well-suited for daily use but exhibits clear limitations when handling a large number of processes or processes with significant priority disparities.\n\n## SCHED_DEADLINE\n\nSCHED_DEADLINE is a scheduler designed for time-critical tasks. Based on the EDF (Earliest Deadline First) model, it provides precise temporal management, ensuring processes meet fixed deadlines.\n\n### Operating Principles\n\n- **Key Attributes**: Each process scheduled with SCHED_DEADLINE requires three main parameters:\n    - Runtime (R): The maximum CPU time the process can use within a period.\n    - Deadline (D): The time by which the process must complete its execution.\n    - Period (T): The interval within which the process is repeatedly activated (usually, D ≤ T).\n\n    In short: the process requests R units of CPU time within a window of length D, repeating every T.\n\n- **Dynamic Priority**:\n    - Processes are scheduled based on the nearest deadline. The process with the most imminent deadline has the highest priority.\n    - The algorithm ensures deadlines are met, provided the CPU is not overloaded (i.e., the sum of R/T for all processes is ≤ 1).\n\n-** Preemption**:\n    - SCHED_DEADLINE is preemptive: a process with a closer deadline can interrupt the currently running process.\n\n#### Best Cases\n\n- **Real-Time Applications with Predictable Deadlines**:\n    - Example: An industrial robot control system requiring motion sequences every 10 ms.\n    - Why It’s Optimal: SCHED_DEADLINE ensures each movement is completed within the specified deadline, meeting critical timing requirements.\n\n- **Multimedia and Streaming**:\n    - Example: Video or audio playback requiring periodic decoding (e.g., every 20 ms).\n    - Advantage: Ensures frames or audio packets are processed on time, avoiding interruptions or latency.\n\n#### Worst Cases\n\n- **CPU Overload**:\n    - Example: Three processes each request 50% of the CPU (sum = 150%). In this case, it is impossible to satisfy all requests, and some processes will miss their deadlines.\n    - Impact: When the system is overloaded, there is no guarantee that processes will meet their timing constraints.\n\n- **Unpredictable or Bursty Processes**:\n    - Example: A web server handling sporadic requests where defining fixed runtime and period is difficult.\n    - Problem: SCHED_DEADLINE is not designed for non-deterministic workloads; such processes may be better handled by CFS.\n\n- **Competition with Non-SCHED_DEADLINE Processes**:\n    - Example: A backup process (handled by CFS) attempting intensive operations while SCHED_DEADLINE processes are active.\n    - Impact: SCHED_DEADLINE processes monopolize the CPU to meet deadlines, causing starvation in non-real-time processes.\n\n### Parameterization\n\nIn SCHED_DEADLINE, there is no tool like nice, but runtime, deadline, and period parameters can be adjusted for each process.\nConfiguring SCHED_DEADLINE\n\nThese parameters can be configured using:\n\n- **`chrt`: Command-Line Tool**:\n\n    chrt is used to manage and set scheduling policies for processes in Linux, including SCHED_DEADLINE.\n\n    **Example**:\n\n    To set a process with SCHED_DEADLINE:\n\n    ```bash\n    sudo chrt --deadline \u003Cperiod> --runtime \u003Cruntime> \u003Ccommand>\n    ```\n\n    Parameters:\n\n        --deadline: Specifies the deadline (e.g., 10 ms).\n        --runtime: Specifies the maximum CPU time (e.g., 2 ms).\n        \u003Ccommand>: The process to run.\n\n    **Full Example**:\n\n    ```bash\n    sudo chrt --deadline 10000000 --runtime 2000000 ./my_program\n    ```\n\n    This assigns 2 ms of CPU time within a 10 ms deadline.\n\n    **Limitations of `chrt`**: chrt is useful for configuring new processes but cannot dynamically modify parameters of an already running process.\n\n- **`sched_setattr` (Linux API)**:\n\n    For greater flexibility, use the sched_setattr API in C. This allows configuration and modification of SCHED_DEADLINE parameters for a running process.\n\n    Example:\n    ```c\n    #include \u003Clinux/sched.h>\n    #include \u003Csys/syscall.h>\n    #include \u003Cunistd.h>\n\n    struct sched_attr {\n        uint32_t size;\n        uint32_t sched_policy;\n        uint64_t sched_flags;\n        uint64_t sched_runtime;\n        uint64_t sched_deadline;\n        uint64_t sched_period;\n    };\n\n    int set_sched_deadline(pid_t pid, uint64_t runtime, uint64_t deadline, uint64_t period) {\n        struct sched_attr attr = {0};\n        attr.size = sizeof(attr);\n        attr.sched_policy = SCHED_DEADLINE;\n        attr.sched_runtime = runtime;\n        attr.sched_deadline = deadline;\n        attr.sched_period = period;\n\n        return syscall(SYS_sched_setattr, pid, &attr, 0);\n    }\n    ```\n\n    With this C function, you can modify the parameters of a specific process programmatically.\n\n- **Advanced Tools**:\n\n    Some Linux distributions offer tools (e.g., via `sysfs` or `cgroups`) to configure SCHED_DEADLINE in more complex environments, such as containers or embedded systems.\n\n#### Comparison with Nice\n\nUnlike nice, which simply modifies a numeric value to influence relative priority, SCHED_DEADLINE requires explicit configuration of temporal parameters, as it is designed for applications with precise real-time constraints. While less straightforward, tools like chrt and the sched_setattr API provide powerful and detailed control.\n\n### In Summary\n\nSCHED_DEADLINE excels in real-time applications with predictable and strictly defined deadlines. However, its performance deteriorates under CPU overload or workloads that are hard to model with fixed timing constraints. For such cases, schedulers like CFS are typically more appropriate.","src/content/blog/33/index.md",[1415],"./post_img33.webp","5eb617a557faf812",{"html":1418,"metadata":1419},"\u003Ch2 id=\"cfs\">CFS\u003C/h2>\n\u003Cp>When we talk about schedulers, the default choice for non-real-time processes in most Linux distributions is the CFS (Completely Fair Scheduler).\nIts operations are based on the concept of fairness, aiming to allocate CPU time to each process proportionally to their priorities (nice value).\u003C/p>\n\u003Ch3 id=\"operating-principles\">Operating Principles\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Cstrong>Balancing CPU Usage\u003C/strong>: CFS maintains an “ideal view” where each process receives its fair share of CPU time. It uses a red-black tree (an ordered data structure) to organize processes based on their accumulated virtual runtime.\u003C/li>\n\u003Cli>\u003Cstrong>Virtual Runtime\u003C/strong>: This time accounts for priorities; a process with higher priority increments its virtual runtime more slowly, thereby receiving more CPU time.\u003C/li>\n\u003Cli>\u003Cstrong>Continuous Scheduling\u003C/strong>: CFS avoids rigid interruptions, distributing available time among processes without fixed time slices, instead aiming for theoretical fairness.\u003C/li>\n\u003C/ul>\n\u003Ch4 id=\"best-cases\">Best Cases\u003C/h4>\n\u003Cul>\n\u003Cli>\n\u003Cp>\u003Cstrong>Balanced Workloads\u003C/strong>:\u003C/p>\n\u003Cul>\n\u003Cli>When all processes have similar priorities and moderate tasks, CFS performs optimally.\u003C/li>\n\u003Cli>Example: Five processes with identical priorities performing lightweight operations like file read/write. CFS distributes CPU time fairly and smoothly.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Interactivity\u003C/strong>:\u003C/p>\n\u003Cul>\n\u003Cli>For interactive applications like a text editor or a web browser, CFS excels by prioritizing short processes waiting for user input, ensuring a quick response.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ul>\n\u003Ch4 id=\"worst-cases\">Worst Cases\u003C/h4>\n\u003Cul>\n\u003Cli>\n\u003Cp>\u003Cstrong>Processes with Large Priority Disparities\u003C/strong>:\u003C/p>\n\u003Cul>\n\u003Cli>Example: A low-priority process (nice +19) running alongside a high-priority process (nice -20). The low-priority process may suffer from starvation, receiving minimal CPU time.\u003C/li>\n\u003Cli>Applications: A background backup process might take much longer than expected if a high-priority process monopolizes the CPU.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Real-Time Workloads\u003C/strong>:\u003C/p>\n\u003Cul>\n\u003Cli>CFS is not designed to handle strict deadlines.\u003C/li>\n\u003Cli>Example: An industrial robot control system. Even with high-priority processes, CFS cannot guarantee task completion within precise deadlines.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>High Process Count\u003C/strong>:\u003C/p>\n\u003Cul>\n\u003Cli>With thousands of active processes, managing the red-black tree can introduce overhead, reducing overall efficiency.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"nicing\">Nicing\u003C/h3>\n\u003Cp>Nicing is the mechanism Linux uses to influence a process’s priority in the operating system. It is based on a value called the nice value, which represents how “kind” a process is in sharing CPU resources with others.\u003C/p>\n\u003Ch4 id=\"how-it-works\">How It Works\u003C/h4>\n\u003Cul>\n\u003Cli>\n\u003Cp>\u003Cstrong>Nice Value Range\u003C/strong>:\u003C/p>\n\u003Cul>\n\u003Cli>The nice value ranges from -20 (highest priority) to +19 (lowest priority).\u003C/li>\n\u003Cli>By default, a process starts with a nice value of 0, indicating normal priority.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Impact of Nice Value\u003C/strong>:\u003C/p>\n\u003Cul>\n\u003Cli>Processes with a lower nice value get more CPU time.\u003C/li>\n\u003Cli>Processes with a higher nice value receive less CPU time, yielding resources to higher-priority processes.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Effect on Virtual Runtime\u003C/strong>:\u003C/p>\n\u003Cul>\n\u003Cli>CFS uses the nice value to adjust the rate of virtual runtime progression: processes with low nice values increment virtual runtime more slowly, gaining more CPU time, and vice versa.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ul>\n\u003Ch4 id=\"interacting-with-the-nice-value\">Interacting with the Nice Value\u003C/h4>\n\u003Cul>\n\u003Cli>\n\u003Cp>\u003Cstrong>Initial Assignment\u003C/strong>: You can set a process’s nice value when launching it with the command:\u003C/p>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#A6E22E\">nice\u003C/span>\u003Cspan style=\"color:#AE81FF\"> -n\u003C/span>\u003Cspan style=\"color:#F92672\"> &#x3C;\u003C/span>\u003Cspan style=\"color:#E6DB74\">nice_valu\u003C/span>\u003Cspan style=\"color:#F8F8F2\">e\u003C/span>\u003Cspan style=\"color:#F92672\">>\u003C/span>\u003Cspan style=\"color:#F92672\"> &#x3C;\u003C/span>\u003Cspan style=\"color:#E6DB74\">comman\u003C/span>\u003Cspan style=\"color:#F8F8F2\">d\u003C/span>\u003Cspan style=\"color:#F92672\">>\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Example: \u003Ccode>nice -n 10 ./my_program\u003C/code>\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Dynamic Modification\u003C/strong>: To change the nice value of an already running process, use:\u003C/p>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#A6E22E\">renice\u003C/span>\u003Cspan style=\"color:#F92672\"> &#x3C;\u003C/span>\u003Cspan style=\"color:#E6DB74\">nice_valu\u003C/span>\u003Cspan style=\"color:#F8F8F2\">e\u003C/span>\u003Cspan style=\"color:#F92672\">>\u003C/span>\u003Cspan style=\"color:#AE81FF\"> -p\u003C/span>\u003Cspan style=\"color:#F92672\"> &#x3C;\u003C/span>\u003Cspan style=\"color:#E6DB74\">PI\u003C/span>\u003Cspan style=\"color:#F8F8F2\">D\u003C/span>\u003Cspan style=\"color:#F92672\">>\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Example: renice -5 -p 1234 (decreases the nice value, increasing priority).\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Practical Examples\u003C/strong>\u003C/p>\n\u003Cul>\n\u003Cli>Background Backups: Assign a high nice value (e.g., +19) to a backup process to ensure it has low priority and does not interfere with other activities.\u003C/li>\n\u003Cli>Critical Applications: Assign a low nice value (e.g., -10) to important processes like a web server to improve their responsiveness.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ul>\n\u003Cp>nicing is a simple yet powerful system for controlling how processes share CPU time, adapting to the priority needs of various applications.\u003C/p>\n\u003Ch3 id=\"in-summary\">In Summary\u003C/h3>\n\u003Cp>CFS is excellent for general usage scenarios that require high interactivity and fairness among processes. It is well-suited for daily use but exhibits clear limitations when handling a large number of processes or processes with significant priority disparities.\u003C/p>\n\u003Ch2 id=\"sched_deadline\">SCHED_DEADLINE\u003C/h2>\n\u003Cp>SCHED_DEADLINE is a scheduler designed for time-critical tasks. Based on the EDF (Earliest Deadline First) model, it provides precise temporal management, ensuring processes meet fixed deadlines.\u003C/p>\n\u003Ch3 id=\"operating-principles-1\">Operating Principles\u003C/h3>\n\u003Cul>\n\u003Cli>\n\u003Cp>\u003Cstrong>Key Attributes\u003C/strong>: Each process scheduled with SCHED_DEADLINE requires three main parameters:\u003C/p>\n\u003Cul>\n\u003Cli>Runtime (R): The maximum CPU time the process can use within a period.\u003C/li>\n\u003Cli>Deadline (D): The time by which the process must complete its execution.\u003C/li>\n\u003Cli>Period (T): The interval within which the process is repeatedly activated (usually, D ≤ T).\u003C/li>\n\u003C/ul>\n\u003Cp>In short: the process requests R units of CPU time within a window of length D, repeating every T.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Dynamic Priority\u003C/strong>:\u003C/p>\n\u003Cul>\n\u003Cli>Processes are scheduled based on the nearest deadline. The process with the most imminent deadline has the highest priority.\u003C/li>\n\u003Cli>The algorithm ensures deadlines are met, provided the CPU is not overloaded (i.e., the sum of R/T for all processes is ≤ 1).\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ul>\n\u003Cp>-** Preemption**:\n- SCHED_DEADLINE is preemptive: a process with a closer deadline can interrupt the currently running process.\u003C/p>\n\u003Ch4 id=\"best-cases-1\">Best Cases\u003C/h4>\n\u003Cul>\n\u003Cli>\n\u003Cp>\u003Cstrong>Real-Time Applications with Predictable Deadlines\u003C/strong>:\u003C/p>\n\u003Cul>\n\u003Cli>Example: An industrial robot control system requiring motion sequences every 10 ms.\u003C/li>\n\u003Cli>Why It’s Optimal: SCHED_DEADLINE ensures each movement is completed within the specified deadline, meeting critical timing requirements.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Multimedia and Streaming\u003C/strong>:\u003C/p>\n\u003Cul>\n\u003Cli>Example: Video or audio playback requiring periodic decoding (e.g., every 20 ms).\u003C/li>\n\u003Cli>Advantage: Ensures frames or audio packets are processed on time, avoiding interruptions or latency.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ul>\n\u003Ch4 id=\"worst-cases-1\">Worst Cases\u003C/h4>\n\u003Cul>\n\u003Cli>\n\u003Cp>\u003Cstrong>CPU Overload\u003C/strong>:\u003C/p>\n\u003Cul>\n\u003Cli>Example: Three processes each request 50% of the CPU (sum = 150%). In this case, it is impossible to satisfy all requests, and some processes will miss their deadlines.\u003C/li>\n\u003Cli>Impact: When the system is overloaded, there is no guarantee that processes will meet their timing constraints.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Unpredictable or Bursty Processes\u003C/strong>:\u003C/p>\n\u003Cul>\n\u003Cli>Example: A web server handling sporadic requests where defining fixed runtime and period is difficult.\u003C/li>\n\u003Cli>Problem: SCHED_DEADLINE is not designed for non-deterministic workloads; such processes may be better handled by CFS.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Competition with Non-SCHED_DEADLINE Processes\u003C/strong>:\u003C/p>\n\u003Cul>\n\u003Cli>Example: A backup process (handled by CFS) attempting intensive operations while SCHED_DEADLINE processes are active.\u003C/li>\n\u003Cli>Impact: SCHED_DEADLINE processes monopolize the CPU to meet deadlines, causing starvation in non-real-time processes.\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"parameterization\">Parameterization\u003C/h3>\n\u003Cp>In SCHED_DEADLINE, there is no tool like nice, but runtime, deadline, and period parameters can be adjusted for each process.\nConfiguring SCHED_DEADLINE\u003C/p>\n\u003Cp>These parameters can be configured using:\u003C/p>\n\u003Cul>\n\u003Cli>\n\u003Cp>\u003Cstrong>\u003Ccode>chrt\u003C/code>: Command-Line Tool\u003C/strong>:\u003C/p>\n\u003Cp>chrt is used to manage and set scheduling policies for processes in Linux, including SCHED_DEADLINE.\u003C/p>\n\u003Cp>\u003Cstrong>Example\u003C/strong>:\u003C/p>\n\u003Cp>To set a process with SCHED_DEADLINE:\u003C/p>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#A6E22E\">sudo\u003C/span>\u003Cspan style=\"color:#E6DB74\"> chrt\u003C/span>\u003Cspan style=\"color:#AE81FF\"> --deadline\u003C/span>\u003Cspan style=\"color:#F92672\"> &#x3C;\u003C/span>\u003Cspan style=\"color:#E6DB74\">perio\u003C/span>\u003Cspan style=\"color:#F8F8F2\">d\u003C/span>\u003Cspan style=\"color:#F92672\">>\u003C/span>\u003Cspan style=\"color:#AE81FF\"> --runtime\u003C/span>\u003Cspan style=\"color:#F92672\"> &#x3C;\u003C/span>\u003Cspan style=\"color:#E6DB74\">runtim\u003C/span>\u003Cspan style=\"color:#F8F8F2\">e\u003C/span>\u003Cspan style=\"color:#F92672\">>\u003C/span>\u003Cspan style=\"color:#F92672\"> &#x3C;\u003C/span>\u003Cspan style=\"color:#E6DB74\">comman\u003C/span>\u003Cspan style=\"color:#F8F8F2\">d\u003C/span>\u003Cspan style=\"color:#F92672\">>\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Parameters:\u003C/p>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>  --deadline: Specifies the deadline (e.g., 10 ms).\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>  --runtime: Specifies the maximum CPU time (e.g., 2 ms).\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>  &#x3C;command>: The process to run.\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>\u003Cstrong>Full Example\u003C/strong>:\u003C/p>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#A6E22E\">sudo\u003C/span>\u003Cspan style=\"color:#E6DB74\"> chrt\u003C/span>\u003Cspan style=\"color:#AE81FF\"> --deadline\u003C/span>\u003Cspan style=\"color:#AE81FF\"> 10000000\u003C/span>\u003Cspan style=\"color:#AE81FF\"> --runtime\u003C/span>\u003Cspan style=\"color:#AE81FF\"> 2000000\u003C/span>\u003Cspan style=\"color:#E6DB74\"> ./my_program\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>This assigns 2 ms of CPU time within a 10 ms deadline.\u003C/p>\n\u003Cp>\u003Cstrong>Limitations of \u003Ccode>chrt\u003C/code>\u003C/strong>: chrt is useful for configuring new processes but cannot dynamically modify parameters of an already running process.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>\u003Ccode>sched_setattr\u003C/code> (Linux API)\u003C/strong>:\u003C/p>\n\u003Cp>For greater flexibility, use the sched_setattr API in C. This allows configuration and modification of SCHED_DEADLINE parameters for a running process.\u003C/p>\n\u003Cp>Example:\u003C/p>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"c\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">#include\u003C/span>\u003Cspan style=\"color:#E6DB74\"> &#x3C;linux/sched.h>\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">#include\u003C/span>\u003Cspan style=\"color:#E6DB74\"> &#x3C;sys/syscall.h>\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">#include\u003C/span>\u003Cspan style=\"color:#E6DB74\"> &#x3C;unistd.h>\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#66D9EF;font-style:italic\">struct\u003C/span>\u003Cspan style=\"color:#F8F8F2\"> sched_attr {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#66D9EF;font-style:italic\">    uint32_t\u003C/span>\u003Cspan style=\"color:#F8F8F2\"> size;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#66D9EF;font-style:italic\">    uint32_t\u003C/span>\u003Cspan style=\"color:#F8F8F2\"> sched_policy;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#66D9EF;font-style:italic\">    uint64_t\u003C/span>\u003Cspan style=\"color:#F8F8F2\"> sched_flags;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#66D9EF;font-style:italic\">    uint64_t\u003C/span>\u003Cspan style=\"color:#F8F8F2\"> sched_runtime;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#66D9EF;font-style:italic\">    uint64_t\u003C/span>\u003Cspan style=\"color:#F8F8F2\"> sched_deadline;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#66D9EF;font-style:italic\">    uint64_t\u003C/span>\u003Cspan style=\"color:#F8F8F2\"> sched_period;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F8F8F2\">};\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#66D9EF;font-style:italic\">int\u003C/span>\u003Cspan style=\"color:#A6E22E\"> set_sched_deadline\u003C/span>\u003Cspan style=\"color:#F8F8F2\">(\u003C/span>\u003Cspan style=\"color:#66D9EF;font-style:italic\">pid_t\u003C/span>\u003Cspan style=\"color:#FD971F;font-style:italic\"> pid\u003C/span>\u003Cspan style=\"color:#F8F8F2\">, \u003C/span>\u003Cspan style=\"color:#66D9EF;font-style:italic\">uint64_t\u003C/span>\u003Cspan style=\"color:#FD971F;font-style:italic\"> runtime\u003C/span>\u003Cspan style=\"color:#F8F8F2\">, \u003C/span>\u003Cspan style=\"color:#66D9EF;font-style:italic\">uint64_t\u003C/span>\u003Cspan style=\"color:#FD971F;font-style:italic\"> deadline\u003C/span>\u003Cspan style=\"color:#F8F8F2\">, \u003C/span>\u003Cspan style=\"color:#66D9EF;font-style:italic\">uint64_t\u003C/span>\u003Cspan style=\"color:#FD971F;font-style:italic\"> period\u003C/span>\u003Cspan style=\"color:#F8F8F2\">) {\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#66D9EF;font-style:italic\">    struct\u003C/span>\u003Cspan style=\"color:#F8F8F2\"> sched_attr attr \u003C/span>\u003Cspan style=\"color:#F92672\">=\u003C/span>\u003Cspan style=\"color:#F8F8F2\"> {\u003C/span>\u003Cspan style=\"color:#AE81FF\">0\u003C/span>\u003Cspan style=\"color:#F8F8F2\">};\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F8F8F2\">    attr.size \u003C/span>\u003Cspan style=\"color:#F92672\">=\u003C/span>\u003Cspan style=\"color:#F92672\"> sizeof\u003C/span>\u003Cspan style=\"color:#F8F8F2\">(attr);\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F8F8F2\">    attr.sched_policy \u003C/span>\u003Cspan style=\"color:#F92672\">=\u003C/span>\u003Cspan style=\"color:#F8F8F2\"> SCHED_DEADLINE;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F8F8F2\">    attr.sched_runtime \u003C/span>\u003Cspan style=\"color:#F92672\">=\u003C/span>\u003Cspan style=\"color:#F8F8F2\"> runtime;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F8F8F2\">    attr.sched_deadline \u003C/span>\u003Cspan style=\"color:#F92672\">=\u003C/span>\u003Cspan style=\"color:#F8F8F2\"> deadline;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F8F8F2\">    attr.sched_period \u003C/span>\u003Cspan style=\"color:#F92672\">=\u003C/span>\u003Cspan style=\"color:#F8F8F2\"> period;\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">    return\u003C/span>\u003Cspan style=\"color:#A6E22E\"> syscall\u003C/span>\u003Cspan style=\"color:#F8F8F2\">(SYS_sched_setattr, pid, \u003C/span>\u003Cspan style=\"color:#F92672\">&#x26;\u003C/span>\u003Cspan style=\"color:#F8F8F2\">attr, \u003C/span>\u003Cspan style=\"color:#AE81FF\">0\u003C/span>\u003Cspan style=\"color:#F8F8F2\">);\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F8F8F2\">}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>With this C function, you can modify the parameters of a specific process programmatically.\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Advanced Tools\u003C/strong>:\u003C/p>\n\u003Cp>Some Linux distributions offer tools (e.g., via \u003Ccode>sysfs\u003C/code> or \u003Ccode>cgroups\u003C/code>) to configure SCHED_DEADLINE in more complex environments, such as containers or embedded systems.\u003C/p>\n\u003C/li>\n\u003C/ul>\n\u003Ch4 id=\"comparison-with-nice\">Comparison with Nice\u003C/h4>\n\u003Cp>Unlike nice, which simply modifies a numeric value to influence relative priority, SCHED_DEADLINE requires explicit configuration of temporal parameters, as it is designed for applications with precise real-time constraints. While less straightforward, tools like chrt and the sched_setattr API provide powerful and detailed control.\u003C/p>\n\u003Ch3 id=\"in-summary-1\">In Summary\u003C/h3>\n\u003Cp>SCHED_DEADLINE excels in real-time applications with predictable and strictly defined deadlines. However, its performance deteriorates under CPU overload or workloads that are hard to model with fixed timing constraints. For such cases, schedulers like CFS are typically more appropriate.\u003C/p>",{"headings":1420,"localImagePaths":1462,"remoteImagePaths":1463,"frontmatter":1464,"imagePaths":1467},[1421,1424,1427,1430,1433,1436,1439,1442,1445,1448,1450,1452,1454,1457,1460],{"depth":59,"slug":1422,"text":1423},"cfs","CFS",{"depth":63,"slug":1425,"text":1426},"operating-principles","Operating Principles",{"depth":94,"slug":1428,"text":1429},"best-cases","Best Cases",{"depth":94,"slug":1431,"text":1432},"worst-cases","Worst Cases",{"depth":63,"slug":1434,"text":1435},"nicing","Nicing",{"depth":94,"slug":1437,"text":1438},"how-it-works","How It Works",{"depth":94,"slug":1440,"text":1441},"interacting-with-the-nice-value","Interacting with the Nice Value",{"depth":63,"slug":1443,"text":1444},"in-summary","In Summary",{"depth":59,"slug":1446,"text":1447},"sched_deadline","SCHED_DEADLINE",{"depth":63,"slug":1449,"text":1426},"operating-principles-1",{"depth":94,"slug":1451,"text":1429},"best-cases-1",{"depth":94,"slug":1453,"text":1432},"worst-cases-1",{"depth":63,"slug":1455,"text":1456},"parameterization","Parameterization",{"depth":94,"slug":1458,"text":1459},"comparison-with-nice","Comparison with Nice",{"depth":63,"slug":1461,"text":1444},"in-summary-1",[],[],{"title":1407,"slug":1404,"description":1408,"pubDate":1465,"tags":1466,"coverImage":1415},"Nov 26 2024",[43],[],"33/index.md","selfhosting-4",{"id":1469,"data":1471,"body":1477,"filePath":1478,"assetImports":1479,"digest":1481,"rendered":1482,"legacyId":1500},{"title":1472,"description":1304,"pubDate":1473,"tags":1474,"coverImage":1476},"Selfhosting #4",["Date","2023-01-29T00:00:00.000Z"],[1236,1090,1475],"VPN","__ASTRO_IMAGE_./post_img8.webp","# Selfhosting | My experience pt.4\n\nSeries of n articles in which I will show my homelab, from the hardware to the software.\n\n## How i connect from external networks\n\nToday we talk about Twingate!\nTwingate is my service of choice for connection remotly to my homelab.\n\nThere are many services to be able to connect remotely, from nginx reverse proxy to wireguard, all of these in one way or another can be used to connect remotely to your LAN.\n\n## Why?\n\nThis question is very easily answered by putting it the other way around, why shouldn't I be able to access my containers remotely? These are private, used only by me and hardly down, so I see no reason not to use these services outside the home as well.\n\n[This](https://www.twingate.com/) is the twingate site where you can find installation and operation information, i'll give you also a list of alternatives to this:\n- [NGINX](https://docs.nginx.com/nginx/admin-guide/web-server/reverse-proxy/)\n- [Wireguard](https://www.wireguard.com/)\n- [OpenVPN](https://openvpn.net/)\n- [Tor Hidden Services](https://community.torproject.org/onion-services/overview/)\n\n#### Authors\n\n- [@filippo-ferrando](https://www.github.com/filippo-ferrando)","src/content/blog/8/index.md",[1480],"./post_img8.webp","544ba827edb0b32d",{"html":1483,"metadata":1484},"\u003Ch1 id=\"selfhosting--my-experience-pt4\">Selfhosting | My experience pt.4\u003C/h1>\n\u003Cp>Series of n articles in which I will show my homelab, from the hardware to the software.\u003C/p>\n\u003Ch2 id=\"how-i-connect-from-external-networks\">How i connect from external networks\u003C/h2>\n\u003Cp>Today we talk about Twingate!\nTwingate is my service of choice for connection remotly to my homelab.\u003C/p>\n\u003Cp>There are many services to be able to connect remotely, from nginx reverse proxy to wireguard, all of these in one way or another can be used to connect remotely to your LAN.\u003C/p>\n\u003Ch2 id=\"why\">Why?\u003C/h2>\n\u003Cp>This question is very easily answered by putting it the other way around, why shouldn’t I be able to access my containers remotely? These are private, used only by me and hardly down, so I see no reason not to use these services outside the home as well.\u003C/p>\n\u003Cp>\u003Ca href=\"https://www.twingate.com/\" target=\"_blank\" rel=\"noopener\">This\u003C/a> is the twingate site where you can find installation and operation information, i’ll give you also a list of alternatives to this:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://docs.nginx.com/nginx/admin-guide/web-server/reverse-proxy/\" target=\"_blank\" rel=\"noopener\">NGINX\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://www.wireguard.com/\" target=\"_blank\" rel=\"noopener\">Wireguard\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://openvpn.net/\" target=\"_blank\" rel=\"noopener\">OpenVPN\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://community.torproject.org/onion-services/overview/\" target=\"_blank\" rel=\"noopener\">Tor Hidden Services\u003C/a>\u003C/li>\n\u003C/ul>\n\u003Ch4 id=\"authors\">Authors\u003C/h4>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://www.github.com/filippo-ferrando\" target=\"_blank\" rel=\"noopener\">@filippo-ferrando\u003C/a>\u003C/li>\n\u003C/ul>",{"headings":1485,"localImagePaths":1494,"remoteImagePaths":1495,"frontmatter":1496,"imagePaths":1499},[1486,1489,1492,1493],{"depth":55,"slug":1487,"text":1488},"selfhosting--my-experience-pt4","Selfhosting | My experience pt.4",{"depth":59,"slug":1490,"text":1491},"how-i-connect-from-external-networks","How i connect from external networks",{"depth":59,"slug":1220,"text":1221},{"depth":94,"slug":95,"text":96},[],[],{"title":1472,"slug":1469,"description":1304,"tags":1497,"pubDate":1498,"coverImage":1480},[1236,1090,1475],"Jan 29 2023",[],"8/index.md","docker-devops-cicd",{"id":1501,"data":1503,"body":1509,"filePath":1510,"assetImports":1511,"digest":1513,"rendered":1514,"legacyId":1546},{"title":1504,"description":1505,"pubDate":1506,"tags":1507,"coverImage":1508},"CI/CD and Container: a DevOps story","Tales of a DevOps",["Date","2025-09-30T00:00:00.000Z"],[822,910],"__ASTRO_IMAGE_./post_img38.png","## The Beginning\n\nA few months ago, I found myself embarking on a new CI/CD adventure: building C++ code with GitHub Actions. The challenge wasn’t compiling the code itself, but rather figuring out all the dependencies the project needed. Unsurprisingly, the `ubuntu:latest` image provided by GitHub was missing many of them. Some dependencies weren’t even available in the official repositories, which meant I had to manually compile them during the workflow—a true nightmare.\n\n## First Solution (A Giant Mess)\n\nMy initial thought was, “No big deal, I’ll just install the libraries I need before compiling, and voilà, job done.” Well, that wasn’t such a brilliant idea. I had to manually compile `libboost`, which is notoriously time-consuming. The workflow ended up taking between one and two hours to complete—far too long and costly to be sustainable.\n\n## Second Approach (The Successful One)\n\n### A Bit of Context\n\nIn my previous post, I introduced **Docker**, docker-compose, and Dockerfiles—now industry standards for deploying services without the headaches of manual OS configuration. But containerization is much more than just a deployment tool. Containers have become essential for developers, providing consistent, isolated environments throughout the entire software development lifecycle.\n\nWith this in mind, I looked for a way to use pre-configured containers in my workflow. Thankfully, GitHub Actions supports running workflows inside custom containers, allowing us to define the exact environment our code needs. This not only saves time but also reduces costs, since we can reuse prebuilt Docker images with all the necessary tools already installed.\n\nBy leveraging containers, teams can ensure that code behaves identically on every machine—from a developer’s laptop to a production server. This consistency eliminates the infamous “it works on my machine” problem and makes onboarding new team members much smoother. Containers can also be versioned and shared, making it easy to roll back to a previous environment if something goes wrong or a dependency update causes issues.\n\n### Why `ubuntu:latest` Isn’t Always the Best Choice\n\nThe code I was trying to ship required, as mentioned earlier, `libboost`—a heavy library to compile. However, that same library is natively shipped with AlmaLinux, the distribution I often use for my experiments. This led me to create a custom Dockerfile based on AlmaLinux to build the project.\n\nChoosing the right base image for your container is crucial. While `ubuntu:latest` is a safe default, it may not always be the most efficient or secure option for your specific needs. Custom images let you tailor the environment to your project’s requirements, including only the necessary dependencies and tools. This not only speeds up your CI/CD pipelines but also reduces the attack surface by minimizing unnecessary packages.\n\n## Using a Custom Container in GitHub Actions\n\nIf you want to leverage a custom container in your GitHub Actions workflow, you only need to add a few parameters to your workflow YAML file:\n\n```yml\ncontainer:\n    image: ${{ secrets.IMAGE_NAME }}\n    credentials:\n        username: ${{ github.actor }}\n        password: ${{ secrets.PASSWORD }}\n```\n\nWith this configuration, your workflow steps will execute inside the specified container. This approach ensures a consistent, reproducible environment for your builds and can save you significant time and effort in maintaining your CI/CD pipelines.\n\nYou can build and push your custom Docker image to a container registry such as Docker Hub or GitHub Container Registry. By referencing the image in your workflow, you guarantee that every run uses the exact same environment. This is especially useful for complex projects with many dependencies or for teams that need to enforce strict version control over their build tools.\n\nAdditionally, using containers in CI/CD workflows can help with security and compliance. You can scan your images for vulnerabilities before deploying them, and you can ensure that sensitive credentials are managed securely through GitHub Secrets.\n\n## My Perspective and Experience\n\nLooking back, adopting containers for CI/CD has been transformative. Early on, I struggled with excessive runtimes and costs, not to mention the complexity of the workflow itself. With the custom container I created, build times were cut by more than half, and the overall complexity decreased significantly. Now, I only need to maintain a single workflow and Dockerfile for any updates or changes.\n\n## Conclusion\n\nContainerization is a powerful tool not just for deployment, but also for building and testing your applications. By using custom Docker images in your CI/CD workflows, you can avoid repetitive setup tasks, ensure consistency across builds, and focus on delivering value through your code. If you haven’t tried this approach yet, I highly recommend giving it a shot in your next automation project!\n\nEmbracing containers in your development and CI/CD processes can lead to faster builds, fewer errors, and a more reliable software delivery pipeline. As the DevOps landscape continues to evolve, mastering containerization will become an increasingly valuable skill for developers and operations teams alike. In my experience, it’s an investment that pays off quickly—and one I wish I’d made even sooner.","src/content/blog/38/index.md",[1512],"./post_img38.png","8470d2236e99e902",{"html":1515,"metadata":1516},"\u003Ch2 id=\"the-beginning\">The Beginning\u003C/h2>\n\u003Cp>A few months ago, I found myself embarking on a new CI/CD adventure: building C++ code with GitHub Actions. The challenge wasn’t compiling the code itself, but rather figuring out all the dependencies the project needed. Unsurprisingly, the \u003Ccode>ubuntu:latest\u003C/code> image provided by GitHub was missing many of them. Some dependencies weren’t even available in the official repositories, which meant I had to manually compile them during the workflow—a true nightmare.\u003C/p>\n\u003Ch2 id=\"first-solution-a-giant-mess\">First Solution (A Giant Mess)\u003C/h2>\n\u003Cp>My initial thought was, “No big deal, I’ll just install the libraries I need before compiling, and voilà, job done.” Well, that wasn’t such a brilliant idea. I had to manually compile \u003Ccode>libboost\u003C/code>, which is notoriously time-consuming. The workflow ended up taking between one and two hours to complete—far too long and costly to be sustainable.\u003C/p>\n\u003Ch2 id=\"second-approach-the-successful-one\">Second Approach (The Successful One)\u003C/h2>\n\u003Ch3 id=\"a-bit-of-context\">A Bit of Context\u003C/h3>\n\u003Cp>In my previous post, I introduced \u003Cstrong>Docker\u003C/strong>, docker-compose, and Dockerfiles—now industry standards for deploying services without the headaches of manual OS configuration. But containerization is much more than just a deployment tool. Containers have become essential for developers, providing consistent, isolated environments throughout the entire software development lifecycle.\u003C/p>\n\u003Cp>With this in mind, I looked for a way to use pre-configured containers in my workflow. Thankfully, GitHub Actions supports running workflows inside custom containers, allowing us to define the exact environment our code needs. This not only saves time but also reduces costs, since we can reuse prebuilt Docker images with all the necessary tools already installed.\u003C/p>\n\u003Cp>By leveraging containers, teams can ensure that code behaves identically on every machine—from a developer’s laptop to a production server. This consistency eliminates the infamous “it works on my machine” problem and makes onboarding new team members much smoother. Containers can also be versioned and shared, making it easy to roll back to a previous environment if something goes wrong or a dependency update causes issues.\u003C/p>\n\u003Ch3 id=\"why-ubuntulatest-isnt-always-the-best-choice\">Why \u003Ccode>ubuntu:latest\u003C/code> Isn’t Always the Best Choice\u003C/h3>\n\u003Cp>The code I was trying to ship required, as mentioned earlier, \u003Ccode>libboost\u003C/code>—a heavy library to compile. However, that same library is natively shipped with AlmaLinux, the distribution I often use for my experiments. This led me to create a custom Dockerfile based on AlmaLinux to build the project.\u003C/p>\n\u003Cp>Choosing the right base image for your container is crucial. While \u003Ccode>ubuntu:latest\u003C/code> is a safe default, it may not always be the most efficient or secure option for your specific needs. Custom images let you tailor the environment to your project’s requirements, including only the necessary dependencies and tools. This not only speeds up your CI/CD pipelines but also reduces the attack surface by minimizing unnecessary packages.\u003C/p>\n\u003Ch2 id=\"using-a-custom-container-in-github-actions\">Using a Custom Container in GitHub Actions\u003C/h2>\n\u003Cp>If you want to leverage a custom container in your GitHub Actions workflow, you only need to add a few parameters to your workflow YAML file:\u003C/p>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"yml\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">container\u003C/span>\u003Cspan style=\"color:#F8F8F2\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">    image\u003C/span>\u003Cspan style=\"color:#F8F8F2\">: \u003C/span>\u003Cspan style=\"color:#E6DB74\">${{ secrets.IMAGE_NAME }}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">    credentials\u003C/span>\u003Cspan style=\"color:#F8F8F2\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">        username\u003C/span>\u003Cspan style=\"color:#F8F8F2\">: \u003C/span>\u003Cspan style=\"color:#E6DB74\">${{ github.actor }}\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">        password\u003C/span>\u003Cspan style=\"color:#F8F8F2\">: \u003C/span>\u003Cspan style=\"color:#E6DB74\">${{ secrets.PASSWORD }}\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>With this configuration, your workflow steps will execute inside the specified container. This approach ensures a consistent, reproducible environment for your builds and can save you significant time and effort in maintaining your CI/CD pipelines.\u003C/p>\n\u003Cp>You can build and push your custom Docker image to a container registry such as Docker Hub or GitHub Container Registry. By referencing the image in your workflow, you guarantee that every run uses the exact same environment. This is especially useful for complex projects with many dependencies or for teams that need to enforce strict version control over their build tools.\u003C/p>\n\u003Cp>Additionally, using containers in CI/CD workflows can help with security and compliance. You can scan your images for vulnerabilities before deploying them, and you can ensure that sensitive credentials are managed securely through GitHub Secrets.\u003C/p>\n\u003Ch2 id=\"my-perspective-and-experience\">My Perspective and Experience\u003C/h2>\n\u003Cp>Looking back, adopting containers for CI/CD has been transformative. Early on, I struggled with excessive runtimes and costs, not to mention the complexity of the workflow itself. With the custom container I created, build times were cut by more than half, and the overall complexity decreased significantly. Now, I only need to maintain a single workflow and Dockerfile for any updates or changes.\u003C/p>\n\u003Ch2 id=\"conclusion\">Conclusion\u003C/h2>\n\u003Cp>Containerization is a powerful tool not just for deployment, but also for building and testing your applications. By using custom Docker images in your CI/CD workflows, you can avoid repetitive setup tasks, ensure consistency across builds, and focus on delivering value through your code. If you haven’t tried this approach yet, I highly recommend giving it a shot in your next automation project!\u003C/p>\n\u003Cp>Embracing containers in your development and CI/CD processes can lead to faster builds, fewer errors, and a more reliable software delivery pipeline. As the DevOps landscape continues to evolve, mastering containerization will become an increasingly valuable skill for developers and operations teams alike. In my experience, it’s an investment that pays off quickly—and one I wish I’d made even sooner.\u003C/p>",{"headings":1517,"localImagePaths":1540,"remoteImagePaths":1541,"frontmatter":1542,"imagePaths":1545},[1518,1521,1524,1527,1530,1533,1536,1539],{"depth":59,"slug":1519,"text":1520},"the-beginning","The Beginning",{"depth":59,"slug":1522,"text":1523},"first-solution-a-giant-mess","First Solution (A Giant Mess)",{"depth":59,"slug":1525,"text":1526},"second-approach-the-successful-one","Second Approach (The Successful One)",{"depth":63,"slug":1528,"text":1529},"a-bit-of-context","A Bit of Context",{"depth":63,"slug":1531,"text":1532},"why-ubuntulatest-isnt-always-the-best-choice","Why ubuntu:latest Isn’t Always the Best Choice",{"depth":59,"slug":1534,"text":1535},"using-a-custom-container-in-github-actions","Using a Custom Container in GitHub Actions",{"depth":59,"slug":1537,"text":1538},"my-perspective-and-experience","My Perspective and Experience",{"depth":59,"slug":91,"text":92},[],[],{"title":1504,"slug":1501,"description":1505,"pubDate":1543,"tags":1544,"coverImage":1512},"Sep 30 2025",[822,910],[],"38/index.md","overview-on-docker",{"id":1547,"data":1549,"body":1555,"filePath":1556,"assetImports":1557,"digest":1559,"rendered":1560,"legacyId":1618},{"title":1550,"description":1551,"pubDate":1552,"tags":1553,"coverImage":1554},"Containerization: a Docker overview","Docker, Dockerfile and Docker-Compose",["Date","2025-03-26T00:00:00.000Z"],[822],"__ASTRO_IMAGE_./post_img37.webp","# 🚢 Docker, Dockerfile, and Docker-Compose: A Comprehensive Guide\n\nIn the world of software development, containerization has become a game-changer. **Docker**, in particular, has revolutionized the way we *develop*, *ship*, and *run* applications. Whether you're a computer science student or a seasoned professional, understanding **Docker**, **Dockerfile**, and **Docker-Compose** is crucial. This guide will walk you through the basics and help you get started with these powerful tools.\n\n## 🐳 What is Docker?\n\nDocker is an open-source platform designed to automate the deployment, scaling, and management of applications. It achieves this through containerization, which allows developers to package applications and their dependencies into a single, portable container. This ensures that the application runs consistently across different environments.\n\n### 🔥 Why Use Docker?\n\n - **Consistency**: Docker ensures that your application runs the same way in development, testing, and production environments.\n - **Isolation**: Containers provide a lightweight, isolated environment for running applications, reducing conflicts between dependencies.\n - **Scalability**: Docker makes it easy to scale applications by running multiple instances of a container.\n - **Portability**: Containers can be run on any system that supports Docker, making it easy to move applications between different environments.\n\n## 📜 Understanding Dockerfile\n\nA Dockerfile is a script containing a series of instructions on how to build a Docker image. Think of it as a blueprint for your application's environment. Each instruction in a Dockerfile creates a layer in the image, which can be cached and reused to speed up the build process.\n\n### 🏗️ Basic Structure of a Dockerfile\n\n - `FROM`: Specifies the base image to use for the build. For example, `FROM` ubuntu:20.04 starts with an Ubuntu base image.\n - `RUN`: Executes commands in the container. For example, `RUN` apt-get update updates the package list in the container.\n - `COPY`: Copies files from the host machine to the container. For example, `COPY` . /app copies the current directory to the /app directory in the container.\n - `WORKDIR`: Sets the working directory for subsequent instructions. For example, `WORKDIR` /app sets the working directory to /app.\n - `EXPOSE`: Informs Docker that the container listens on the specified network ports at runtime. For example, `EXPOSE` 8080 exposes port 8080.\n - `CMD`: Provides defaults for an executing container. For example, `CMD` [\"python\", \"app.py\"] runs the app.py script using Python.\n\n### 📝 Example Dockerfile\n\n```dockerfile\n# Use an official Python runtime as a parent image\nFROM python:3.8-slim\n\n# Set the working directory in the container\nWORKDIR /app\n\n# Copy the current directory contents into the container at /app\nCOPY . /app\n\n# Install any needed packages specified in requirements.txt\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Make port 80 available to the world outside this container\nEXPOSE 80\n\n# Define environment variable\nENV NAME World\n\n# Run app.py when the container launches\nCMD [\"python\", \"app.py\"]\n```\n\n## ⚙️ Docker-Compose: Simplifying Multi-Container Applications\n\nDocker-Compose is a tool for defining and running multi-container Docker applications. With Docker-Compose, you can use a YAML file to configure your application's services. This makes it easier to manage and run complex applications that require multiple containers.\n\n### 🤔 Why Use Docker-Compose?\n\n - **Simplicity**: Docker-Compose simplifies the process of running multi-container applications with a single command.\n - **Consistency**: Ensures that your application's services are configured and run consistently across different environments.\n - **Scalability**: Allows you to scale services up or down with a single command.\n\n### 📂 Basic Structure of a docker-compose.yml File\n\n - `version`: Specifies the version of the Docker-Compose file format.\n - `services`: Defines the services that make up your application. Each service corresponds to a container.\n - `networks`: Defines custom networks for your application.\n - `volumes`: Defines named volumes for persistent data storage.\n\n### 🛠️ Example docker-compose.yml File\n\n```yml\nversion: '3'\nservices:\n  web:\n    image: python:3.8-slim\n    build: .\n    ports:\n      - \"5000:5000\"\n    volumes:\n      - .:/code\n    environment:\n      FLASK_ENV: development\n  redis:\n    image: \"redis:alpine\"\n```\n\nIn this example, the docker-compose.yml file defines two services: web and redis. The web service uses a Python image and maps port 5000 on the host to port 5000 in the container. The redis service uses the official Redis image.\n\n## 🚀 Getting Started with Docker, Dockerfile, and Docker-Compose\n\nTo get started with Docker, you'll need to install Docker and Docker-Compose on your machine. Once installed, you can create a Dockerfile for your application and use Docker-Compose to manage multi-container applications.\n(The walk-through will be for Arch-based distro, but the packages are similar for other distros)\n\n### 📥 Installing Docker and Docker-Compose\n\nUsing your favorite AUR package manager run:\n```bash\nparu -S docker docker-compose\n```\n\nThis will install all you need to get started with docker!\n\n\n### 🔨 Building and Running a Docker Container\n\nBuild the Docker image: Navigate to the directory containing your Dockerfile and run:\n\n```bash\ndocker build -t my-python-app .\n```\n\nRun the Docker container: Start a container using the image you just built:\n\n```bash\ndocker run -p 4000:80 my-python-app\n```\n\n### `:>` Running a Docker-Compose Application\n\nNavigate to the directory containing your docker-compose.yml file, to start the application, run the following command to start your multi-container application:\n\n```bash\ndocker compose up\n```\nor if you want to run it in **detached** mode you can run `docker compose up -d`\n\n## 🎯 Conclusion\n\nDocker, Dockerfile, and Docker-Compose are powerful tools that simplify the development, deployment, and management of applications. By understanding and leveraging these tools, you can create consistent, isolated, and scalable environments for your applications. Whether you're a computer science student or a professional in the field, mastering Docker will undoubtedly enhance your development workflow and make you more efficient.\n\n#### Happy coding! 🚀","src/content/blog/37/index.md",[1558],"./post_img37.webp","3869e2ad65cde482",{"html":1561,"metadata":1562},"\u003Ch1 id=\"-docker-dockerfile-and-docker-compose-a-comprehensive-guide\">🚢 Docker, Dockerfile, and Docker-Compose: A Comprehensive Guide\u003C/h1>\n\u003Cp>In the world of software development, containerization has become a game-changer. \u003Cstrong>Docker\u003C/strong>, in particular, has revolutionized the way we \u003Cem>develop\u003C/em>, \u003Cem>ship\u003C/em>, and \u003Cem>run\u003C/em> applications. Whether you’re a computer science student or a seasoned professional, understanding \u003Cstrong>Docker\u003C/strong>, \u003Cstrong>Dockerfile\u003C/strong>, and \u003Cstrong>Docker-Compose\u003C/strong> is crucial. This guide will walk you through the basics and help you get started with these powerful tools.\u003C/p>\n\u003Ch2 id=\"-what-is-docker\">🐳 What is Docker?\u003C/h2>\n\u003Cp>Docker is an open-source platform designed to automate the deployment, scaling, and management of applications. It achieves this through containerization, which allows developers to package applications and their dependencies into a single, portable container. This ensures that the application runs consistently across different environments.\u003C/p>\n\u003Ch3 id=\"-why-use-docker\">🔥 Why Use Docker?\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Cstrong>Consistency\u003C/strong>: Docker ensures that your application runs the same way in development, testing, and production environments.\u003C/li>\n\u003Cli>\u003Cstrong>Isolation\u003C/strong>: Containers provide a lightweight, isolated environment for running applications, reducing conflicts between dependencies.\u003C/li>\n\u003Cli>\u003Cstrong>Scalability\u003C/strong>: Docker makes it easy to scale applications by running multiple instances of a container.\u003C/li>\n\u003Cli>\u003Cstrong>Portability\u003C/strong>: Containers can be run on any system that supports Docker, making it easy to move applications between different environments.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"-understanding-dockerfile\">📜 Understanding Dockerfile\u003C/h2>\n\u003Cp>A Dockerfile is a script containing a series of instructions on how to build a Docker image. Think of it as a blueprint for your application’s environment. Each instruction in a Dockerfile creates a layer in the image, which can be cached and reused to speed up the build process.\u003C/p>\n\u003Ch3 id=\"️-basic-structure-of-a-dockerfile\">🏗️ Basic Structure of a Dockerfile\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Ccode>FROM\u003C/code>: Specifies the base image to use for the build. For example, \u003Ccode>FROM\u003C/code> ubuntu:20.04 starts with an Ubuntu base image.\u003C/li>\n\u003Cli>\u003Ccode>RUN\u003C/code>: Executes commands in the container. For example, \u003Ccode>RUN\u003C/code> apt-get update updates the package list in the container.\u003C/li>\n\u003Cli>\u003Ccode>COPY\u003C/code>: Copies files from the host machine to the container. For example, \u003Ccode>COPY\u003C/code> . /app copies the current directory to the /app directory in the container.\u003C/li>\n\u003Cli>\u003Ccode>WORKDIR\u003C/code>: Sets the working directory for subsequent instructions. For example, \u003Ccode>WORKDIR\u003C/code> /app sets the working directory to /app.\u003C/li>\n\u003Cli>\u003Ccode>EXPOSE\u003C/code>: Informs Docker that the container listens on the specified network ports at runtime. For example, \u003Ccode>EXPOSE\u003C/code> 8080 exposes port 8080.\u003C/li>\n\u003Cli>\u003Ccode>CMD\u003C/code>: Provides defaults for an executing container. For example, \u003Ccode>CMD\u003C/code> [“python”, “app.py”] runs the app.py script using Python.\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"-example-dockerfile\">📝 Example Dockerfile\u003C/h3>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"dockerfile\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#88846F\"># Use an official Python runtime as a parent image\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">FROM\u003C/span>\u003Cspan style=\"color:#F8F8F2\"> python:3.8-slim\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#88846F\"># Set the working directory in the container\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">WORKDIR\u003C/span>\u003Cspan style=\"color:#F8F8F2\"> /app\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#88846F\"># Copy the current directory contents into the container at /app\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">COPY\u003C/span>\u003Cspan style=\"color:#F8F8F2\"> . /app\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#88846F\"># Install any needed packages specified in requirements.txt\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">RUN\u003C/span>\u003Cspan style=\"color:#F8F8F2\"> pip install --no-cache-dir -r requirements.txt\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#88846F\"># Make port 80 available to the world outside this container\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">EXPOSE\u003C/span>\u003Cspan style=\"color:#F8F8F2\"> 80\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#88846F\"># Define environment variable\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">ENV\u003C/span>\u003Cspan style=\"color:#F8F8F2\"> NAME World\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#88846F\"># Run app.py when the container launches\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">CMD\u003C/span>\u003Cspan style=\"color:#F8F8F2\"> [\u003C/span>\u003Cspan style=\"color:#E6DB74\">\"python\"\u003C/span>\u003Cspan style=\"color:#F8F8F2\">, \u003C/span>\u003Cspan style=\"color:#E6DB74\">\"app.py\"\u003C/span>\u003Cspan style=\"color:#F8F8F2\">]\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch2 id=\"️-docker-compose-simplifying-multi-container-applications\">⚙️ Docker-Compose: Simplifying Multi-Container Applications\u003C/h2>\n\u003Cp>Docker-Compose is a tool for defining and running multi-container Docker applications. With Docker-Compose, you can use a YAML file to configure your application’s services. This makes it easier to manage and run complex applications that require multiple containers.\u003C/p>\n\u003Ch3 id=\"-why-use-docker-compose\">🤔 Why Use Docker-Compose?\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Cstrong>Simplicity\u003C/strong>: Docker-Compose simplifies the process of running multi-container applications with a single command.\u003C/li>\n\u003Cli>\u003Cstrong>Consistency\u003C/strong>: Ensures that your application’s services are configured and run consistently across different environments.\u003C/li>\n\u003Cli>\u003Cstrong>Scalability\u003C/strong>: Allows you to scale services up or down with a single command.\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"-basic-structure-of-a-docker-composeyml-file\">📂 Basic Structure of a docker-compose.yml File\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Ccode>version\u003C/code>: Specifies the version of the Docker-Compose file format.\u003C/li>\n\u003Cli>\u003Ccode>services\u003C/code>: Defines the services that make up your application. Each service corresponds to a container.\u003C/li>\n\u003Cli>\u003Ccode>networks\u003C/code>: Defines custom networks for your application.\u003C/li>\n\u003Cli>\u003Ccode>volumes\u003C/code>: Defines named volumes for persistent data storage.\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"️-example-docker-composeyml-file\">🛠️ Example docker-compose.yml File\u003C/h3>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"yml\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">version\u003C/span>\u003Cspan style=\"color:#F8F8F2\">: \u003C/span>\u003Cspan style=\"color:#E6DB74\">'3'\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">services\u003C/span>\u003Cspan style=\"color:#F8F8F2\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">  web\u003C/span>\u003Cspan style=\"color:#F8F8F2\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">    image\u003C/span>\u003Cspan style=\"color:#F8F8F2\">: \u003C/span>\u003Cspan style=\"color:#E6DB74\">python:3.8-slim\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">    build\u003C/span>\u003Cspan style=\"color:#F8F8F2\">: \u003C/span>\u003Cspan style=\"color:#AE81FF\">.\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">    ports\u003C/span>\u003Cspan style=\"color:#F8F8F2\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F8F8F2\">      - \u003C/span>\u003Cspan style=\"color:#E6DB74\">\"5000:5000\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">    volumes\u003C/span>\u003Cspan style=\"color:#F8F8F2\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F8F8F2\">      - \u003C/span>\u003Cspan style=\"color:#E6DB74\">.:/code\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">    environment\u003C/span>\u003Cspan style=\"color:#F8F8F2\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">      FLASK_ENV\u003C/span>\u003Cspan style=\"color:#F8F8F2\">: \u003C/span>\u003Cspan style=\"color:#E6DB74\">development\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">  redis\u003C/span>\u003Cspan style=\"color:#F8F8F2\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">    image\u003C/span>\u003Cspan style=\"color:#F8F8F2\">: \u003C/span>\u003Cspan style=\"color:#E6DB74\">\"redis:alpine\"\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>In this example, the docker-compose.yml file defines two services: web and redis. The web service uses a Python image and maps port 5000 on the host to port 5000 in the container. The redis service uses the official Redis image.\u003C/p>\n\u003Ch2 id=\"-getting-started-with-docker-dockerfile-and-docker-compose\">🚀 Getting Started with Docker, Dockerfile, and Docker-Compose\u003C/h2>\n\u003Cp>To get started with Docker, you’ll need to install Docker and Docker-Compose on your machine. Once installed, you can create a Dockerfile for your application and use Docker-Compose to manage multi-container applications.\n(The walk-through will be for Arch-based distro, but the packages are similar for other distros)\u003C/p>\n\u003Ch3 id=\"-installing-docker-and-docker-compose\">📥 Installing Docker and Docker-Compose\u003C/h3>\n\u003Cp>Using your favorite AUR package manager run:\u003C/p>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#A6E22E\">paru\u003C/span>\u003Cspan style=\"color:#AE81FF\"> -S\u003C/span>\u003Cspan style=\"color:#E6DB74\"> docker\u003C/span>\u003Cspan style=\"color:#E6DB74\"> docker-compose\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>This will install all you need to get started with docker!\u003C/p>\n\u003Ch3 id=\"-building-and-running-a-docker-container\">🔨 Building and Running a Docker Container\u003C/h3>\n\u003Cp>Build the Docker image: Navigate to the directory containing your Dockerfile and run:\u003C/p>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#A6E22E\">docker\u003C/span>\u003Cspan style=\"color:#E6DB74\"> build\u003C/span>\u003Cspan style=\"color:#AE81FF\"> -t\u003C/span>\u003Cspan style=\"color:#E6DB74\"> my-python-app\u003C/span>\u003Cspan style=\"color:#E6DB74\"> .\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Run the Docker container: Start a container using the image you just built:\u003C/p>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#A6E22E\">docker\u003C/span>\u003Cspan style=\"color:#E6DB74\"> run\u003C/span>\u003Cspan style=\"color:#AE81FF\"> -p\u003C/span>\u003Cspan style=\"color:#E6DB74\"> 4000:80\u003C/span>\u003Cspan style=\"color:#E6DB74\"> my-python-app\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch3 id=\"-running-a-docker-compose-application\">\u003Ccode>:>\u003C/code> Running a Docker-Compose Application\u003C/h3>\n\u003Cp>Navigate to the directory containing your docker-compose.yml file, to start the application, run the following command to start your multi-container application:\u003C/p>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#A6E22E\">docker\u003C/span>\u003Cspan style=\"color:#E6DB74\"> compose\u003C/span>\u003Cspan style=\"color:#E6DB74\"> up\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>or if you want to run it in \u003Cstrong>detached\u003C/strong> mode you can run \u003Ccode>docker compose up -d\u003C/code>\u003C/p>\n\u003Ch2 id=\"-conclusion\">🎯 Conclusion\u003C/h2>\n\u003Cp>Docker, Dockerfile, and Docker-Compose are powerful tools that simplify the development, deployment, and management of applications. By understanding and leveraging these tools, you can create consistent, isolated, and scalable environments for your applications. Whether you’re a computer science student or a professional in the field, mastering Docker will undoubtedly enhance your development workflow and make you more efficient.\u003C/p>\n\u003Ch4 id=\"happy-coding\">Happy coding! 🚀\u003C/h4>",{"headings":1563,"localImagePaths":1612,"remoteImagePaths":1613,"frontmatter":1614,"imagePaths":1617},[1564,1567,1570,1573,1576,1579,1582,1585,1588,1591,1594,1597,1600,1603,1606,1609],{"depth":55,"slug":1565,"text":1566},"-docker-dockerfile-and-docker-compose-a-comprehensive-guide","🚢 Docker, Dockerfile, and Docker-Compose: A Comprehensive Guide",{"depth":59,"slug":1568,"text":1569},"-what-is-docker","🐳 What is Docker?",{"depth":63,"slug":1571,"text":1572},"-why-use-docker","🔥 Why Use Docker?",{"depth":59,"slug":1574,"text":1575},"-understanding-dockerfile","📜 Understanding Dockerfile",{"depth":63,"slug":1577,"text":1578},"️-basic-structure-of-a-dockerfile","🏗️ Basic Structure of a Dockerfile",{"depth":63,"slug":1580,"text":1581},"-example-dockerfile","📝 Example Dockerfile",{"depth":59,"slug":1583,"text":1584},"️-docker-compose-simplifying-multi-container-applications","⚙️ Docker-Compose: Simplifying Multi-Container Applications",{"depth":63,"slug":1586,"text":1587},"-why-use-docker-compose","🤔 Why Use Docker-Compose?",{"depth":63,"slug":1589,"text":1590},"-basic-structure-of-a-docker-composeyml-file","📂 Basic Structure of a docker-compose.yml File",{"depth":63,"slug":1592,"text":1593},"️-example-docker-composeyml-file","🛠️ Example docker-compose.yml File",{"depth":59,"slug":1595,"text":1596},"-getting-started-with-docker-dockerfile-and-docker-compose","🚀 Getting Started with Docker, Dockerfile, and Docker-Compose",{"depth":63,"slug":1598,"text":1599},"-installing-docker-and-docker-compose","📥 Installing Docker and Docker-Compose",{"depth":63,"slug":1601,"text":1602},"-building-and-running-a-docker-container","🔨 Building and Running a Docker Container",{"depth":63,"slug":1604,"text":1605},"-running-a-docker-compose-application",":> Running a Docker-Compose Application",{"depth":59,"slug":1607,"text":1608},"-conclusion","🎯 Conclusion",{"depth":94,"slug":1610,"text":1611},"happy-coding","Happy coding! 🚀",[],[],{"title":1550,"slug":1547,"description":1551,"pubDate":1615,"tags":1616,"coverImage":1558},"Mar 26 2025",[822],[],"37/index.md","funny-sorting-algorithm",{"id":1619,"data":1621,"body":1629,"filePath":1630,"assetImports":1631,"digest":1633,"rendered":1634,"legacyId":1662},{"title":1622,"description":1623,"pubDate":1624,"tags":1625,"coverImage":1628},"Funny sorting algorithm","Stupid post for a stupid algorithm",["Date","2023-02-12T00:00:00.000Z"],[1626,1627],"Python","Algo","__ASTRO_IMAGE_./post_img9.png","# Bogosort\n## The most useless and stupid sorting algorithm\n\n### How it works?\n\nWell, let's say that the basic operation of this algorithm is really very simple, this checks an array asking each time if it is sorted, if it is not, the array is randomized and the cycle repeats.\n\nUnlike all the other algorithms this one doesn't make any sense.\n\n### Time of execution\n\nThis sorting algorithm is probabilistic in nature. If all elements to be sorted are different, the complexity is: ```O(n × n!)```\n\nThe reason why the algorithm almost certainly arrives at a conclusion is explained by the [tireless monkey theorem](https://en.wikipedia.org/wiki/Infinite_monkey_theorem): with each attempt there is a probability of getting the sort right, so given an unlimited number of attempts, it should eventually succeed.\n\nAlmost certainly, here, it is a precise mathematical term.\n\n### Implementation\n\nThis is a simple Python3 implement of this kind of algo\n\n```python\nimport random\n\ndef bogo_sort(a):\n    i = 0\n    while not is_sorted(a):\n        print(f\"Iteration {i}: {a}\")\n        random.shuffle(a)\n        i += 1\n    return a\n\ndef is_sorted(a):\n    for i in range(len(a) - 1):\n        if a[i] > a[i + 1]:\n            return False\n    return True\n\ndef main():\n    a = []\n    l = input(\"Insert array length: \")\n    for i in range(int(l)):\n        a.append(random.randint(0, 1000))\n    bogo_sort(a)\n    print(f\"Final iteration: {a}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### Visualize Execution\n\n\u003Ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/DaPJkYo2quc\" title=\"Bogo Sort algo\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen>\u003C/iframe>\n\n#### Authors\n\n- [@filippo-ferrando](https://www.github.com/filippo-ferrando)","src/content/blog/9/index.md",[1632],"./post_img9.png","3321b71e9b1ffe21",{"html":1635,"metadata":1636},"\u003Ch1 id=\"bogosort\">Bogosort\u003C/h1>\n\u003Ch2 id=\"the-most-useless-and-stupid-sorting-algorithm\">The most useless and stupid sorting algorithm\u003C/h2>\n\u003Ch3 id=\"how-it-works\">How it works?\u003C/h3>\n\u003Cp>Well, let’s say that the basic operation of this algorithm is really very simple, this checks an array asking each time if it is sorted, if it is not, the array is randomized and the cycle repeats.\u003C/p>\n\u003Cp>Unlike all the other algorithms this one doesn’t make any sense.\u003C/p>\n\u003Ch3 id=\"time-of-execution\">Time of execution\u003C/h3>\n\u003Cp>This sorting algorithm is probabilistic in nature. If all elements to be sorted are different, the complexity is: \u003Ccode>O(n × n!)\u003C/code>\u003C/p>\n\u003Cp>The reason why the algorithm almost certainly arrives at a conclusion is explained by the \u003Ca href=\"https://en.wikipedia.org/wiki/Infinite_monkey_theorem\" target=\"_blank\" rel=\"noopener\">tireless monkey theorem\u003C/a>: with each attempt there is a probability of getting the sort right, so given an unlimited number of attempts, it should eventually succeed.\u003C/p>\n\u003Cp>Almost certainly, here, it is a precise mathematical term.\u003C/p>\n\u003Ch3 id=\"implementation\">Implementation\u003C/h3>\n\u003Cp>This is a simple Python3 implement of this kind of algo\u003C/p>\n\u003Cpre class=\"astro-code monokai\" style=\"background-color:#272822;color:#F8F8F2; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">import\u003C/span>\u003Cspan style=\"color:#F8F8F2\"> random\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#66D9EF;font-style:italic\">def\u003C/span>\u003Cspan style=\"color:#A6E22E\"> bogo_sort\u003C/span>\u003Cspan style=\"color:#F8F8F2\">(\u003C/span>\u003Cspan style=\"color:#FD971F;font-style:italic\">a\u003C/span>\u003Cspan style=\"color:#F8F8F2\">):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F8F8F2\">    i \u003C/span>\u003Cspan style=\"color:#F92672\">=\u003C/span>\u003Cspan style=\"color:#AE81FF\"> 0\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">    while\u003C/span>\u003Cspan style=\"color:#F92672\"> not\u003C/span>\u003Cspan style=\"color:#F8F8F2\"> is_sorted(a):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#66D9EF\">        print\u003C/span>\u003Cspan style=\"color:#F8F8F2\">(\u003C/span>\u003Cspan style=\"color:#66D9EF;font-style:italic\">f\u003C/span>\u003Cspan style=\"color:#E6DB74\">\"Iteration \u003C/span>\u003Cspan style=\"color:#AE81FF\">{\u003C/span>\u003Cspan style=\"color:#F8F8F2\">i\u003C/span>\u003Cspan style=\"color:#AE81FF\">}\u003C/span>\u003Cspan style=\"color:#E6DB74\">: \u003C/span>\u003Cspan style=\"color:#AE81FF\">{\u003C/span>\u003Cspan style=\"color:#F8F8F2\">a\u003C/span>\u003Cspan style=\"color:#AE81FF\">}\u003C/span>\u003Cspan style=\"color:#E6DB74\">\"\u003C/span>\u003Cspan style=\"color:#F8F8F2\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F8F8F2\">        random.shuffle(a)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F8F8F2\">        i \u003C/span>\u003Cspan style=\"color:#F92672\">+=\u003C/span>\u003Cspan style=\"color:#AE81FF\"> 1\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">    return\u003C/span>\u003Cspan style=\"color:#F8F8F2\"> a\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#66D9EF;font-style:italic\">def\u003C/span>\u003Cspan style=\"color:#A6E22E\"> is_sorted\u003C/span>\u003Cspan style=\"color:#F8F8F2\">(\u003C/span>\u003Cspan style=\"color:#FD971F;font-style:italic\">a\u003C/span>\u003Cspan style=\"color:#F8F8F2\">):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">    for\u003C/span>\u003Cspan style=\"color:#F8F8F2\"> i \u003C/span>\u003Cspan style=\"color:#F92672\">in\u003C/span>\u003Cspan style=\"color:#66D9EF\"> range\u003C/span>\u003Cspan style=\"color:#F8F8F2\">(\u003C/span>\u003Cspan style=\"color:#66D9EF\">len\u003C/span>\u003Cspan style=\"color:#F8F8F2\">(a) \u003C/span>\u003Cspan style=\"color:#F92672\">-\u003C/span>\u003Cspan style=\"color:#AE81FF\"> 1\u003C/span>\u003Cspan style=\"color:#F8F8F2\">):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">        if\u003C/span>\u003Cspan style=\"color:#F8F8F2\"> a[i] \u003C/span>\u003Cspan style=\"color:#F92672\">>\u003C/span>\u003Cspan style=\"color:#F8F8F2\"> a[i \u003C/span>\u003Cspan style=\"color:#F92672\">+\u003C/span>\u003Cspan style=\"color:#AE81FF\"> 1\u003C/span>\u003Cspan style=\"color:#F8F8F2\">]:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">            return\u003C/span>\u003Cspan style=\"color:#AE81FF\"> False\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">    return\u003C/span>\u003Cspan style=\"color:#AE81FF\"> True\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#66D9EF;font-style:italic\">def\u003C/span>\u003Cspan style=\"color:#A6E22E\"> main\u003C/span>\u003Cspan style=\"color:#F8F8F2\">():\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F8F8F2\">    a \u003C/span>\u003Cspan style=\"color:#F92672\">=\u003C/span>\u003Cspan style=\"color:#F8F8F2\"> []\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F8F8F2\">    l \u003C/span>\u003Cspan style=\"color:#F92672\">=\u003C/span>\u003Cspan style=\"color:#66D9EF\"> input\u003C/span>\u003Cspan style=\"color:#F8F8F2\">(\u003C/span>\u003Cspan style=\"color:#E6DB74\">\"Insert array length: \"\u003C/span>\u003Cspan style=\"color:#F8F8F2\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">    for\u003C/span>\u003Cspan style=\"color:#F8F8F2\"> i \u003C/span>\u003Cspan style=\"color:#F92672\">in\u003C/span>\u003Cspan style=\"color:#66D9EF\"> range\u003C/span>\u003Cspan style=\"color:#F8F8F2\">(\u003C/span>\u003Cspan style=\"color:#66D9EF;font-style:italic\">int\u003C/span>\u003Cspan style=\"color:#F8F8F2\">(l)):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F8F8F2\">        a.append(random.randint(\u003C/span>\u003Cspan style=\"color:#AE81FF\">0\u003C/span>\u003Cspan style=\"color:#F8F8F2\">, \u003C/span>\u003Cspan style=\"color:#AE81FF\">1000\u003C/span>\u003Cspan style=\"color:#F8F8F2\">))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F8F8F2\">    bogo_sort(a)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#66D9EF\">    print\u003C/span>\u003Cspan style=\"color:#F8F8F2\">(\u003C/span>\u003Cspan style=\"color:#66D9EF;font-style:italic\">f\u003C/span>\u003Cspan style=\"color:#E6DB74\">\"Final iteration: \u003C/span>\u003Cspan style=\"color:#AE81FF\">{\u003C/span>\u003Cspan style=\"color:#F8F8F2\">a\u003C/span>\u003Cspan style=\"color:#AE81FF\">}\u003C/span>\u003Cspan style=\"color:#E6DB74\">\"\u003C/span>\u003Cspan style=\"color:#F8F8F2\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F92672\">if\u003C/span>\u003Cspan style=\"color:#F8F8F2\"> __name__ \u003C/span>\u003Cspan style=\"color:#F92672\">==\u003C/span>\u003Cspan style=\"color:#E6DB74\"> \"__main__\"\u003C/span>\u003Cspan style=\"color:#F8F8F2\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F8F8F2\">    main()\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch3 id=\"visualize-execution\">Visualize Execution\u003C/h3>\n\u003Ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/DaPJkYo2quc\" title=\"Bogo Sort algo\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen>\u003C/iframe>\n\u003Ch4 id=\"authors\">Authors\u003C/h4>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://www.github.com/filippo-ferrando\" target=\"_blank\" rel=\"noopener\">@filippo-ferrando\u003C/a>\u003C/li>\n\u003C/ul>",{"headings":1637,"localImagePaths":1656,"remoteImagePaths":1657,"frontmatter":1658,"imagePaths":1661},[1638,1641,1644,1646,1649,1652,1655],{"depth":55,"slug":1639,"text":1640},"bogosort","Bogosort",{"depth":59,"slug":1642,"text":1643},"the-most-useless-and-stupid-sorting-algorithm","The most useless and stupid sorting algorithm",{"depth":63,"slug":1437,"text":1645},"How it works?",{"depth":63,"slug":1647,"text":1648},"time-of-execution","Time of execution",{"depth":63,"slug":1650,"text":1651},"implementation","Implementation",{"depth":63,"slug":1653,"text":1654},"visualize-execution","Visualize Execution",{"depth":94,"slug":95,"text":96},[],[],{"title":1622,"slug":1619,"description":1623,"pubDate":1659,"tags":1660,"coverImage":1632},"Feb 12 2023",[1626,1627],[],"9/index.md"]